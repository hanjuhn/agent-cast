import os
import json
from datetime import datetime
from typing import List, Dict, Any
from openai import OpenAI
from dotenv import load_dotenv
from rouge_score import rouge_scorer
from bert_score import score as bert_score

try:
    from .base_agent import BaseAgent
    from state import WorkflowState
except ImportError:
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from .base_agent import BaseAgent
    from state import WorkflowState

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ

class ResearchCriticAgent:
    """
    ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ê³  ê°œì„ ì„ ìœ„í•œ ê²½ìŸì  í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ ë¹„í‰ê°€ ì—ì´ì „íŠ¸.
    LEGO í”„ë ˆì„ì›Œí¬ì˜ Critic-Explainer ê²½ìŸ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ ë¦¬ì„œì¹˜ í’ˆì§ˆ í–¥ìƒì„ ë„ëª¨í•©ë‹ˆë‹¤.
    """
    def __init__(self, model="gpt-4o"):
        """
        ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = model
        
        # Critic ì—­í•  í”„ë¡¬í”„íŠ¸
        self.role_prompt = """
        ë‹¹ì‹ ì€ ResearchCriticì…ë‹ˆë‹¤. ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ê³  
        ê°œì„ ì„ ìœ„í•œ ë‹¤ê°ì ì´ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ë‹¹ì‹ ì˜ ì—­í• :
        1. ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ ê°ê´€ì ì´ê³  ì—„ê²©í•˜ê²Œ í‰ê°€
        2. Factual Feedback: ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ì¶œì²˜ ì‹ ë¢°ì„± ê²€í† 
        3. Logical Feedback: ë…¼ë¦¬ì  ì™„ê²°ì„±ê³¼ êµ¬ì¡°ì  ì¼ê´€ì„± ê²€í† 
        4. Relevance Feedback: ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ì˜ ê´€ë ¨ì„± ê²€í† 
        5. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì œì•ˆ ì œê³µ
        6. í—ˆìœ„ ì •ë³´ë‚˜ ë¶€ì •í™•í•œ ì¸ìš© ì‹ë³„ ë° ì§€ì 

        í‰ê°€ ê¸°ì¤€:
        - ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ì¶œì²˜ ì‹ ë¢°ì„±
        - ë…¼ë¦¬ì  ì™„ê²°ì„±ê³¼ êµ¬ì¡°ì  ì¼ê´€ì„±
        - ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ì˜ ê´€ë ¨ì„±
        - ì •ë³´ì˜ ê¹Šì´ì™€ í¬ê´„ì„±
        - ì¸ìš©ê³¼ ì°¸ì¡°ì˜ ì ì ˆì„±
        """

    def _calculate_quantitative_metrics(self, generated_text: str, reference_texts: list[str]) -> dict:
        """
        [TOOL] ì •ëŸ‰ì  í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ë‚´ë¶€ ë©”ì†Œë“œ.
        """
        print("---  cuantitativa de la evaluaciÃ³n se ha iniciado ---")
        
        # BERTScore ê³„ì‚°
        P, R, F1 = bert_score([generated_text], reference_texts, lang="ko", model_type="bert-base-multilingual-cased")
        bertscore_f1 = F1.mean().item()
        print(f"BERTScore F1: {bertscore_f1:.4f}")

        # ROUGE Score ê³„ì‚°
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        # ì—¬ëŸ¬ ì°¸ì¡° ë¬¸ì„œì— ëŒ€í•œ í‰ê·  ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” ì²« ë²ˆì§¸ ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        scores = scorer.score(reference_texts[0], generated_text)
        rougeL_fmeasure = scores['rougeL'].fmeasure
        print(f"ROUGE-L F-measure: {rougeL_fmeasure:.4f}")
        
        print("--- EvaluaciÃ³n Cuantitativa Completada ---")
        return {
            "bert_score_f1": round(bertscore_f1, 4),
            "rougeL_fmeasure": round(rougeL_fmeasure, 4)
        }

    def evaluate_research_output(self, research_output: str, source_documents: list[str], 
                               user_profile: str) -> dict:
        """
        ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ê³  í”¼ë“œë°± ìƒì„± (LEGO í”„ë ˆì„ì›Œí¬ ìŠ¤íƒ€ì¼)
        
        Args:
            research_output (str): í‰ê°€í•  ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼
            source_documents (list[str]): ì°¸ì¡° ë¬¸ì„œë“¤
            user_profile (str): ì‚¬ìš©ì í”„ë¡œí•„
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼ì™€ í”¼ë“œë°±
        """
        print("--- ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ í‰ê°€ ì‹œì‘ ---")
        
        # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì—¬ í† í° ì œí•œ ë°©ì§€
        def truncate_text(text: str, max_chars: int = 3000) -> str:
            """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê¸¸ì´ë¡œ ìë¥´ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
            if len(text) <= max_chars:
                return text
            
            # ì²« ë¶€ë¶„ê³¼ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ìœ ì§€í•˜ê³  ì¤‘ê°„ì„ ìš”ì•½
            first_part = text[:max_chars//3]
            last_part = text[-(max_chars//3):]
            middle_summary = f"...[ì¤‘ê°„ ë‚´ìš© ìš”ì•½: {len(text) - (max_chars//3)*2}ì ìƒëµ]..."
            
            return first_part + middle_summary + last_part
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ìš”ì•½
        truncated_research = truncate_text(research_output, 3000)
        truncated_profile = truncate_text(user_profile, 500)
        
        prompt = f"""
{self.role_prompt}

**ì‚¬ìš©ì í”„ë¡œí•„ (ìš”ì•½):**
{truncated_profile}

**í‰ê°€í•  ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ (ìš”ì•½):**
{truncated_research}

**ì°¸ì¡° ë¬¸ì„œ ìˆ˜:**
{len(source_documents)}ê°œ

**ì‘ì—…:**
ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ ë‹¤ê°ì ìœ¼ë¡œ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
    "overall_score": 0.85,
    "evaluation_criteria": {{
        "factual_accuracy": {{
            "score": 0.9,
            "feedback": "ì‚¬ì‹¤ì  ì •í™•ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤."
        }},
        "logical_consistency": {{
            "score": 0.8,
            "feedback": "ë…¼ë¦¬ì  ì¼ê´€ì„±ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤."
        }},
        "relevance": {{
            "score": 0.85,
            "feedback": "ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        }},
        "completeness": {{
            "score": 0.75,
            "feedback": "ëŒ€ë¶€ë¶„ì˜ ë‚´ìš©ì„ ë‹¤ë£¨ì§€ë§Œ, ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        }},
        "clarity": {{
            "score": 0.9,
            "feedback": "ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        }}
    }},
    "detailed_feedback": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ë¦¬ì„œì¹˜ ê²°ê³¼ì…ë‹ˆë‹¤.",
    "improvement_suggestions": [
        "ì™„ì„±ë„ í–¥ìƒì„ ìœ„í•´ ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ì„ ì¶”ê°€í•˜ì„¸ìš”.",
        "ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ë”ìš± ê°•í™”í•˜ì„¸ìš”."
    ],
    "critical_issues": [],
    "recommendations": "ì´ ë¦¬ì„œì¹˜ ê²°ê³¼ëŠ” ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."
}}
```

**ì¤‘ìš”:** JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¦¬ì„œì¹˜ ë¹„í‰ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°ê´€ì ì¸ í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            
            evaluation_text = response.choices[0].message.content.strip()
            
            # í† í° ì‚¬ìš©ëŸ‰ í™•ì¸
            if hasattr(response, 'usage'):
                usage = response.usage
                print(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: ì…ë ¥ {usage.prompt_tokens}, ì¶œë ¥ {usage.completion_tokens}, ì´ {usage.total_tokens}")
            
            # ë””ë²„ê¹…: GPT ì‘ë‹µ ë‚´ìš© í™•ì¸
            print(f"ğŸ” GPT ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {evaluation_text[:200]}...")
            
            # JSON íŒŒì‹±
            try:
                evaluation_result = json.loads(evaluation_text)
                print("âœ… JSON íŒŒì‹± ì„±ê³µ")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                print(f"ğŸ“„ ì „ì²´ ì‘ë‹µ: {evaluation_text}")
                
                # JSON ì¶”ì¶œ ì‹œë„
                try:
                    # JSON ë¸”ë¡ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
                    import re
                    
                    # íŒ¨í„´ 1: ```json ... ``` ë¸”ë¡
                    json_match = re.search(r'```json\s*(.*?)\s*```', evaluation_text, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1)
                        evaluation_result = json.loads(json_content)
                        print("âœ… JSON ë¸”ë¡ì—ì„œ ì¶”ì¶œ ì„±ê³µ")
                        return evaluation_result
                    
                    # íŒ¨í„´ 2: ``` ... ``` ë¸”ë¡ (json íƒœê·¸ ì—†ìŒ)
                    json_match = re.search(r'```\s*(.*?)\s*```', evaluation_text, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(1)
                        try:
                            evaluation_result = json.loads(json_content)
                            print("âœ… ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ì„±ê³µ")
                            return evaluation_result
                        except:
                            pass
                    
                    # íŒ¨í„´ 3: { ... } JSON ê°ì²´ ì§ì ‘ ì°¾ê¸°
                    json_match = re.search(r'\{.*\}', evaluation_text, re.DOTALL)
                    if json_match:
                        json_content = json_match.group(0)
                        try:
                            evaluation_result = json.loads(json_content)
                            print("âœ… JSON ê°ì²´ ì§ì ‘ ì¶”ì¶œ ì„±ê³µ")
                            return evaluation_result
                        except:
                            pass
                    
                    # íŒ¨í„´ 4: GPT ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
                    print("ğŸ”„ GPT ì‘ë‹µì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜ ì‹œë„...")
                    evaluation_result = self._parse_unstructured_response(evaluation_text)
                    if evaluation_result:
                        print("âœ… êµ¬ì¡°í™”ëœ ì‘ë‹µ íŒŒì‹± ì„±ê³µ")
                        return evaluation_result
                    
                    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                    print("âš ï¸ ëª¨ë“  JSON ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨, ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ì‚¬ìš©")
                    evaluation_result = self._get_default_evaluation()
                    
                except Exception as extract_error:
                    print(f"âš ï¸ JSON ì¶”ì¶œ ì‹¤íŒ¨: {extract_error}, ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ì‚¬ìš©")
                    evaluation_result = self._get_default_evaluation()
            
            # ì •ëŸ‰ì  ì§€í‘œ ê³„ì‚° (ì„ íƒì )
            try:
                quantitative_metrics = self._calculate_quantitative_metrics(research_output, source_documents)
                evaluation_result["quantitative_metrics"] = quantitative_metrics
            except Exception as e:
                print(f"âš ï¸ ì •ëŸ‰ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
                evaluation_result["quantitative_metrics"] = {"error": "ê³„ì‚° ì‹¤íŒ¨"}
            
            print("--- ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ í‰ê°€ ì™„ë£Œ ---")
            return evaluation_result
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._get_default_evaluation()

    def _get_default_evaluation(self) -> dict:
        """ê¸°ë³¸ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "overall_score": 0.7,
            "evaluation_criteria": {
                "factual_accuracy": {
                    "score": 0.7,
                    "feedback": "í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                },
                "logical_consistency": {
                    "score": 0.7,
                    "feedback": "í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                },
                "relevance": {
                    "score": 0.7,
                    "feedback": "í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                },
                "completeness": {
                    "score": 0.7,
                    "feedback": "í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                },
                "clarity": {
                    "score": 0.7,
                    "feedback": "í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                }
            },
            "detailed_feedback": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê¸°ë³¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
            "improvement_suggestions": ["í‰ê°€ë¥¼ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."],
            "critical_issues": ["í‰ê°€ ì‹œìŠ¤í…œ ì˜¤ë¥˜"],
            "recommendations": "í‰ê°€ë¥¼ ë‹¤ì‹œ ìˆ˜í–‰í•˜ì„¸ìš”."
        }
    
    def _parse_unstructured_response(self, response_text: str) -> dict:
        """êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ GPT ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‰ê°€ ê²°ê³¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            import re
            
            # ì ìˆ˜ ì¶”ì¶œ (0.0 ~ 1.0 ë²”ìœ„)
            score_match = re.search(r'(\d+\.?\d*)\s*ì |score[:\s]*(\d+\.?\d*)|(\d+\.?\d*)', response_text, re.IGNORECASE)
            overall_score = 0.7  # ê¸°ë³¸ê°’
            if score_match:
                for group in score_match.groups():
                    if group:
                        try:
                            score = float(group)
                            if 0.0 <= score <= 1.0:
                                overall_score = score
                                break
                        except ValueError:
                            continue
            
            # í”¼ë“œë°± í‚¤ì›Œë“œ ì¶”ì¶œ
            feedback_keywords = {
                "factual_accuracy": ["ì‚¬ì‹¤", "ì •í™•", "ì¶œì²˜", "ì¸ìš©"],
                "logical_consistency": ["ë…¼ë¦¬", "ì¼ê´€", "êµ¬ì¡°", "íë¦„"],
                "relevance": ["ê´€ë ¨", "ì ì ˆ", "ìš”êµ¬ì‚¬í•­"],
                "completeness": ["ì™„ì„±", "í¬ê´„", "ì„¸ë¶€", "ë¶€ì¡±"],
                "clarity": ["ëª…í™•", "ì´í•´", "í‘œí˜„", "ê°€ë…"]
            }
            
            evaluation_criteria = {}
            for criterion, keywords in feedback_keywords.items():
                score = 0.7  # ê¸°ë³¸ê°’
                feedback = "í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
                for keyword in keywords:
                    if keyword in response_text:
                        score = min(0.9, score + 0.1)  # í‚¤ì›Œë“œ ë°œê²¬ ì‹œ ì ìˆ˜ ìƒìŠ¹
                        feedback = f"{keyword} ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                        break
                
                evaluation_criteria[criterion] = {
                    "score": round(score, 2),
                    "feedback": feedback
                }
            
            # ê°œì„  ì œì•ˆ ì¶”ì¶œ
            improvement_suggestions = []
            if "ê°œì„ " in response_text or "ì œì•ˆ" in response_text or "suggestion" in response_text.lower():
                improvement_suggestions = ["í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."]
            else:
                improvement_suggestions = ["í‰ê°€ë¥¼ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."]
            
            return {
                "overall_score": round(overall_score, 2),
                "evaluation_criteria": evaluation_criteria,
                "detailed_feedback": "êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‰ê°€ ê²°ê³¼ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
                "improvement_suggestions": improvement_suggestions,
                "critical_issues": [],
                "recommendations": "ì´ í‰ê°€ ê²°ê³¼ëŠ” íŒŒì‹±ëœ ê²°ê³¼ì…ë‹ˆë‹¤."
            }
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡°í™”ëœ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

def save_evaluation_results(data, filename=None):
    """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_results_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"âœ… í‰ê°€ ê²°ê³¼ê°€ '{filename}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

class CriticAgent(BaseAgent):
    """ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  í’ˆì§ˆì„ ê²€í† í•˜ëŠ” ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="critic",
            description="ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  í’ˆì§ˆì„ ê²€í† í•˜ëŠ” ì—ì´ì „íŠ¸"
        )
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        self.client = OpenAI(api_key=api_key)
        self.model = "gpt-4"
        
        # Critic ì—­í•  í”„ë¡¬í”„íŠ¸
        self.role_prompt = """
        ë‹¹ì‹ ì€ ResearchCriticì…ë‹ˆë‹¤. ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ í‰ê°€í•˜ê³  
        ê°œì„ ì„ ìœ„í•œ ë‹¤ê°ì ì´ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ë‹¹ì‹ ì˜ ì—­í• :
        1. ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ ê°ê´€ì ì´ê³  ì—„ê²©í•˜ê²Œ í‰ê°€
        2. Factual Feedback: ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ì¶œì²˜ ì‹ ë¢°ì„± ê²€í† 
        3. Logical Feedback: ë…¼ë¦¬ì  ì™„ê²°ì„±ê³¼ êµ¬ì¡°ì  ì¼ê´€ì„± ê²€í† 
        4. Relevance Feedback: ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ì˜ ê´€ë ¨ì„± ê²€í† 
        5. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì œì•ˆ ì œê³µ
        6. í—ˆìœ„ ì •ë³´ë‚˜ ë¶€ì •í™•í•œ ì¸ìš© ì‹ë³„ ë° ì§€ì 

        í‰ê°€ ê¸°ì¤€:
        - ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ì¶œì²˜ ì‹ ë¢°ì„±
        - ë…¼ë¦¬ì  ì™„ê²°ì„±ê³¼ êµ¬ì¡°ì  ì¼ê´€ì„±
        - ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ì˜ ê´€ë ¨ì„±
        - ì •ë³´ì˜ ê¹Šì´ì™€ í¬ê´„ì„±
        - ì¸ìš©ê³¼ ì°¸ì¡°ì˜ ì ì ˆì„±
        """
    
    def _create_source_documents_from_state(self, state: WorkflowState) -> List[str]:
        """WorkflowStateì—ì„œ ì†ŒìŠ¤ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        source_documents = []
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì†ŒìŠ¤ ë¬¸ì„œ ìƒì„±
        search_results = getattr(state, 'search_results', [])
        for result in search_results:
            content = f"{result.get('title', '')} {result.get('content', '')}"
            if content.strip():
                source_documents.append(content)
        
        # ê°œì¸í™” ì •ë³´ì—ì„œ ì†ŒìŠ¤ ë¬¸ì„œ ìƒì„±
        personal_info = getattr(state, 'personal_info', {})
        if personal_info:
            for key, value in personal_info.items():
                if isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict):
                            content = " ".join([f"{k}: {v}" for k, v in item.items() if v])
                        else:
                            content = str(item)
                        
                        if content and len(content) > 10:
                            source_documents.append(content)
        
        # ì—°êµ¬ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì†ŒìŠ¤ ë¬¸ì„œ ìƒì„±
        research_context = getattr(state, 'research_context', {})
        if research_context:
            for key, value in research_context.items():
                if isinstance(value, list):
                    for item in value:
                        if isinstance(item, dict):
                            content = " ".join([f"{k}: {v}" for k, v in item.items() if v])
                        else:
                            content = str(item)
                        
                        if content and len(content) > 10:
                            source_documents.append(content)
        
        # ê¸°ë³¸ ì†ŒìŠ¤ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì œê³µ
        if not source_documents:
            source_documents = [
                "AI ê¸°ìˆ  ë™í–¥ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì •ë³´ì™€ ì—°êµ¬ ìë£Œ",
                "LLM ìµœì í™” ë° MoE ì•„í‚¤í…ì²˜ ê´€ë ¨ ê¸°ìˆ  ë¬¸ì„œ",
                "AI ì—°êµ¬ ë° ê°œë°œ í”„ë¡œì íŠ¸ ê´€ë ¨ ìë£Œ"
            ]
        
        return source_documents
    
    def _create_user_profile_from_state(self, state: WorkflowState) -> str:
        """WorkflowStateì—ì„œ ì‚¬ìš©ì í”„ë¡œí•„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        personal_info = getattr(state, 'personal_info', {})
        research_context = getattr(state, 'research_context', {})
        
        profile_parts = []
        
        # ì—°êµ¬ í‚¤ì›Œë“œ
        research_keywords = personal_info.get('research_keywords', [])
        if research_keywords:
            profile_parts.append(f"ì—°êµ¬ ê´€ì‹¬ì‚¬: {', '.join(research_keywords)}")
        
        # í˜„ì¬ í”„ë¡œì íŠ¸
        current_projects = research_context.get('current_projects', [])
        if current_projects:
            profile_parts.append(f"í˜„ì¬ í”„ë¡œì íŠ¸: {', '.join(current_projects)}")
        
        # ì—°êµ¬ ë°©í–¥
        research_direction = research_context.get('research_direction', '')
        if research_direction:
            profile_parts.append(f"ì—°êµ¬ ë°©í–¥: {research_direction}")
        
        if profile_parts:
            return " | ".join(profile_parts)
        else:
            return "AI ê¸°ìˆ ì— ê´€ì‹¬ì´ ìˆëŠ” ì—°êµ¬ì"
    
    def evaluate_research_output(self, research_output: str, source_documents: List[str], 
                               user_profile: str) -> Dict[str, Any]:
        """ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ê³  í”¼ë“œë°± ìƒì„±"""
        print("--- ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ í‰ê°€ ì‹œì‘ ---")
        
        prompt = f"""
{self.role_prompt}

**ì‚¬ìš©ì í”„ë¡œí•„:**
{user_profile}

**í‰ê°€í•  ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼:**
{research_output}

**ì°¸ì¡° ë¬¸ì„œë“¤:**
{chr(10).join([f"- {doc[:200]}..." for doc in source_documents])}

**ì‘ì—…:**
ë¦¬ì„œì¹˜ ê²°ê³¼ë¬¼ì„ ë‹¤ê°ì ìœ¼ë¡œ í‰ê°€í•˜ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
    "overall_score": 0.85,
    "evaluation_criteria": {{
        "factual_accuracy": {{
            "score": 0.9,
            "feedback": "ì‚¬ì‹¤ì  ì •í™•ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ì¶œì²˜ì™€ ì¸ìš©ì´ ì ì ˆí•˜ê²Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        }},
        "logical_consistency": {{
            "score": 0.8,
            "feedback": "ë…¼ë¦¬ì  ì¼ê´€ì„±ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. êµ¬ì¡°ì™€ íë¦„ì´ ëª…í™•í•©ë‹ˆë‹¤."
        }},
        "relevance": {{
            "score": 0.85,
            "feedback": "ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        }},
        "completeness": {{
            "score": 0.75,
            "feedback": "ëŒ€ë¶€ë¶„ì˜ ë‚´ìš©ì„ ë‹¤ë£¨ì§€ë§Œ, ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        }},
        "clarity": {{
            "score": 0.9,
            "feedback": "ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        }}
    }},
    "detailed_feedback": "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í’ˆì§ˆì˜ ë¦¬ì„œì¹˜ ê²°ê³¼ì…ë‹ˆë‹¤. íŠ¹íˆ ì‚¬ì‹¤ì  ì •í™•ì„±ê³¼ ëª…í™•ì„±ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.",
    "improvement_suggestions": [
        "ì™„ì„±ë„ í–¥ìƒì„ ìœ„í•´ ì¼ë¶€ ì„¸ë¶€ì‚¬í•­ì„ ì¶”ê°€í•˜ì„¸ìš”.",
        "ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ë”ìš± ê°•í™”í•˜ì„¸ìš”."
    ],
    "critical_issues": [],
    "recommendations": "ì´ ë¦¬ì„œì¹˜ ê²°ê³¼ëŠ” ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."
}}
```

**ì¤‘ìš”:** JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¦¬ì„œì¹˜ ë¹„í‰ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°ê´€ì ì¸ í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            evaluation_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹±
            try:
                evaluation_result = json.loads(evaluation_text)
            except json.JSONDecodeError:
                print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ì‚¬ìš©")
                evaluation_result = self._get_default_evaluation()
            
            return evaluation_result
            
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}, ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ì‚¬ìš©")
            return self._get_default_evaluation()
    
    def _get_default_evaluation(self) -> Dict[str, Any]:
        """ê¸°ë³¸ í‰ê°€ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "overall_score": 0.75,
            "evaluation_criteria": {
                "factual_accuracy": {
                    "score": 0.8,
                    "feedback": "ê¸°ë³¸ì ì¸ ì‚¬ì‹¤ì  ì •í™•ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤."
                },
                "logical_consistency": {
                    "score": 0.7,
                    "feedback": "ë…¼ë¦¬ì  êµ¬ì¡°ê°€ ëŒ€ì²´ë¡œ ì¼ê´€ë©ë‹ˆë‹¤."
                },
                "relevance": {
                    "score": 0.8,
                    "feedback": "ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ ê´€ë ¨ì„±ì´ ìˆìŠµë‹ˆë‹¤."
                },
                "completeness": {
                    "score": 0.7,
                    "feedback": "ê¸°ë³¸ì ì¸ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤."
                },
                "clarity": {
                    "score": 0.8,
                    "feedback": "ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                }
            },
            "detailed_feedback": "ê¸°ë³¸ì ì¸ í’ˆì§ˆì˜ ë¦¬ì„œì¹˜ ê²°ê³¼ì…ë‹ˆë‹¤. ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.",
            "improvement_suggestions": [
                "ë” êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.",
                "ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°•í™”í•˜ì„¸ìš”."
            ],
            "critical_issues": [],
            "recommendations": "ê¸°ë³¸ì ì¸ ê²€í†  í›„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        }
    
    async def process(self, state: WorkflowState) -> WorkflowState:
        """ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        self.log_execution("ë¦¬ì„œì¹˜ ê²°ê³¼ í‰ê°€ ì‹œì‘")
        
        try:
            # ìƒíƒœì—ì„œ ë°ì´í„° ì¶”ì¶œ
            research_result = getattr(state, 'research_result', '')
            source_documents = self._create_source_documents_from_state(state)
            user_profile = self._create_user_profile_from_state(state)
            
            if not research_result:
                self.log_execution("í‰ê°€í•  ë¦¬ì„œì¹˜ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤")
                research_result = "AI ê¸°ìˆ  ë™í–¥ì— ëŒ€í•œ ê¸°ë³¸ ë¶„ì„ ë³´ê³ ì„œ"
            
            # í‰ê°€ ìˆ˜í–‰
            evaluation_results = self.evaluate_research_output(
                research_result, source_documents, user_profile
            )
            
            # ê²°ê³¼ ì €ì¥
            output_filename = f"output/critic/evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs(os.path.dirname(output_filename), exist_ok=True)
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            state_dict = {k: v for k, v in state.__dict__.items()}
            if 'evaluation_results' in state_dict:
                del state_dict['evaluation_results']
            if 'critic_feedback' in state_dict:
                del state_dict['critic_feedback']
            if 'quality_score' in state_dict:
                del state_dict['quality_score']
            
            new_state = WorkflowState(
                **state_dict,
                evaluation_results=evaluation_results,
                critic_feedback=evaluation_results.get('detailed_feedback', ''),
                quality_score=evaluation_results.get('overall_score', 0.0)
            )
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            new_state = self.update_workflow_status(new_state, "critic_completed")
            
            self.log_execution(f"ë¦¬ì„œì¹˜ ê²°ê³¼ í‰ê°€ ì™„ë£Œ: ì ìˆ˜ {evaluation_results.get('overall_score', 0.0)}")
            return new_state
            
        except Exception as e:
            self.log_execution(f"ë¦¬ì„œì¹˜ ê²°ê³¼ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            raise

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸš€ ë¦¬ì„œì¹˜ ë¹„í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. ResearchCriticAgent ì´ˆê¸°í™”
    print("\n1ï¸âƒ£ ResearchCriticAgent ì´ˆê¸°í™” ì¤‘...")
    critic = ResearchCriticAgent()
    
    # 2. ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„
    print("\n2ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    sample_research_output = """
    ìµœì‹  AI íŠ¸ë Œë“œ ë¶„ì„
    
    ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ëŠ” íŠ¸ë Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    
    1. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ (LLM)ì˜ ë°œì „
    - GPT-4, Claude ë“±ì˜ ëª¨ë¸ì´ ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë˜ê³  ìˆìŠµë‹ˆë‹¤.
    - ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì´ ê°•í™”ë˜ì–´ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì„±ì„ í†µí•© ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    2. ìƒì„±í˜• AIì˜ í™•ì‚°
    - DALL-E, Midjourney ë“±ì˜ ì´ë¯¸ì§€ ìƒì„± AIê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.
    - ë¹„ì¦ˆë‹ˆìŠ¤ ì‘ìš© ì‚¬ë¡€ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    
    3. AI ê·œì œ ë° ìœ¤ë¦¬
    - AIì˜ ì•ˆì „ì„±ê³¼ ìœ¤ë¦¬ì— ëŒ€í•œ ë…¼ì˜ê°€ í™œë°œí•©ë‹ˆë‹¤.
    - ê°êµ­ì—ì„œ AI ê·œì œ ë²•ì•ˆì„ ë„ì…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    """
    
    sample_source_documents = [
        "OpenAIì˜ ìµœì‹  ì—°êµ¬ ë³´ê³ ì„œì— ë”°ë¥´ë©´ GPT-4ëŠ” ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
        "Googleì˜ ì—°êµ¬íŒ€ì€ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ì˜ ë°œì „ì— ëŒ€í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.",
        "EUì˜ AI ê·œì œ ë²•ì•ˆì€ AI ì‹œìŠ¤í…œì˜ íˆ¬ëª…ì„±ê³¼ ì±…ì„ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤."
    ]
    
    sample_user_profile = "AI ê¸°ìˆ ì— ê´€ì‹¬ì´ ìˆëŠ” ì¼ë°˜ ì‚¬ìš©ì"
    
    # 3. í‰ê°€ ìˆ˜í–‰
    print("\n3ï¸âƒ£ ë¦¬ì„œì¹˜ ê²°ê³¼ í‰ê°€ ì¤‘...")
    evaluation_result = critic.evaluate_research_output(
        sample_research_output, sample_source_documents, sample_user_profile
    )
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n4ï¸âƒ£ ê²°ê³¼ ì €ì¥ ì¤‘...")
    saved_filename = save_evaluation_results(evaluation_result)
    
    if saved_filename:
        print(f"\nâœ… ë¦¬ì„œì¹˜ ë¹„í‰ê°€ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ì „ì²´ ì ìˆ˜: {evaluation_result.get('overall_score', 0.0)}")
        print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {saved_filename}")
        
        # í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“‹ í‰ê°€ ê²°ê³¼ ìš”ì•½:")
        criteria = evaluation_result.get('evaluation_criteria', {})
        for criterion, result in criteria.items():
            print(f"   - {criterion}: {result.get('score', 0.0)}")
        
        print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        suggestions = evaluation_result.get('improvement_suggestions', [])
        for i, suggestion in enumerate(suggestions, 1):
            print(f"   {i}. {suggestion}")
    else:
        print("âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()

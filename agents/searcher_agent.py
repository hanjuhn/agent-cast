"""Searcher Agent for web crawling and information collection."""

import time
import json
import re
import requests
import os
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin
from dotenv import load_dotenv

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains

from bs4 import BeautifulSoup

try:
    from .base_agent import BaseAgent
    from state import WorkflowState
except ImportError:
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from .base_agent import BaseAgent
    from state import WorkflowState

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ

class WebSearcher:
    def __init__(self, perplexity_api_key: str = None):
        self.driver = None
        # API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
        self.perplexity_api_key = perplexity_api_key or os.environ.get('PERPLEXITY_API_KEY')
        if not self.perplexity_api_key:
            print("âš ï¸ PERPLEXITY_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.setup_driver()
    
    def setup_driver(self):
        """WebDriver ì„¤ì •"""
        print("WebDriver ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        chrome_options = Options()
        # chrome_options.add_argument("--headless")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        print("WebDriver ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("WebDriverë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    def crawl_pytorch_kr(self):
        """íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§"""
        print("\n=== íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        URL = "https://discuss.pytorch.kr/c/news"
        self.driver.get(URL)
        print(f"'{URL}' í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        time.sleep(3)

        one_week_ago = datetime.now() - timedelta(days=7)
        post_info = {}

        print("\nìµœì‹  ê²Œì‹œê¸€ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ìŠ¤í¬ë¡¤ ë‹¤ìš´)...")
        while True:
            topic_list_items = self.driver.find_elements(By.CSS_SELECTOR, "tbody.topic-list-body tr.topic-list-item")
            
            if not topic_list_items:
                print("ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            last_post_date = None
            
            for item in topic_list_items:
                try:
                    date_span = item.find_element(By.CSS_SELECTOR, "span.relative-date")
                    post_timestamp_ms = int(date_span.get_attribute("data-time"))
                    post_date = datetime.fromtimestamp(post_timestamp_ms / 1000)
                    
                    last_post_date = post_date
                    
                    if post_date >= one_week_ago:
                        link_element = item.find_element(By.CSS_SELECTOR, "a.title")
                        link = link_element.get_attribute("href")
                        if link not in post_info:
                            post_info[link] = post_date
                    
                except (StaleElementReferenceException, NoSuchElementException):
                    continue
            
            # ë§ˆì§€ë§‰ ê²Œì‹œê¸€ì´ 1ì£¼ì¼ ì´ì „ì´ë©´ ì¤‘ë‹¨
            if last_post_date and last_post_date < one_week_ago:
                print(f"ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ë‚ ì§œ: {last_post_date.strftime('%Y-%m-%d')} - 1ì£¼ì¼ ì´ì „ì´ë¯€ë¡œ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            # í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                WebDriverWait(self.driver, 10).until(
                    lambda driver: len(driver.find_elements(By.CSS_SELECTOR, "tbody.topic-list-body tr.topic-list-item")) > len(topic_list_items)
                )
            except TimeoutException:
                print("ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ìŠ¤í¬ë¡¤ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break

        print(f"ì´ {len(post_info)}ê°œì˜ ìµœì‹  ê²Œì‹œê¸€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ê²Œì‹œê¸€ ë‚´ìš© ìˆ˜ì§‘
        posts_data = []
        for link, post_date in post_info.items():
            try:
                self.driver.get(link)
                time.sleep(2)
                
                # ì œëª© ì¶”ì¶œ - ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                title = "Unknown"
                try:
                    title_element = self.driver.find_element(By.CSS_SELECTOR, "h1.fancy-title")
                    title = title_element.text.strip()
                except NoSuchElementException:
                    try:
                        title_element = self.driver.find_element(By.CSS_SELECTOR, "h1")
                        title = title_element.text.strip()
                    except NoSuchElementException:
                        try:
                            title_element = self.driver.find_element(By.CSS_SELECTOR, ".topic-title")
                            title = title_element.text.strip()
                        except NoSuchElementException:
                            title = f"PyTorch ê²Œì‹œê¸€ {len(posts_data) + 1}"
                
                # ë‚´ìš© ì¶”ì¶œ - ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                content = "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                try:
                    content_element = self.driver.find_element(By.CSS_SELECTOR, "div.cooked")
                    content = content_element.text.strip()
                except NoSuchElementException:
                    try:
                        content_element = self.driver.find_element(By.CSS_SELECTOR, ".topic-body")
                        content = content_element.text.strip()
                    except NoSuchElementException:
                        try:
                            content_element = self.driver.find_element(By.CSS_SELECTOR, ".post-content")
                            content = content_element.text.strip()
                        except NoSuchElementException:
                            content = f"PyTorch í•œêµ­ ì‚¬ìš©ì ëª¨ì„ ê²Œì‹œê¸€ì…ë‹ˆë‹¤. ì œëª©: {title}"
                
                # ì‘ì„±ì ì¶”ì¶œ
                try:
                    author_element = self.driver.find_element(By.CSS_SELECTOR, "span.username")
                    author = author_element.text.strip()
                except NoSuchElementException:
                    author = "Unknown"
                
                post_data = {
                    "title": title,
                    "content": content,
                    "author": author,
                    "url": link,
                    "date": post_date.isoformat(),
                    "source": "pytorch_kr"
                }
                posts_data.append(post_data)
                print(f"âœ… '{title}' ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        return posts_data

    def crawl_aitimes_kr(self):
        """AIíƒ€ì„ìŠ¤ í¬ë¡¤ë§"""
        print("\n=== AIíƒ€ì„ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ ===")
        
        URL = "https://www.aitimes.kr/news/articleList.html?page=1&total=0&box_idxno=&view_type=sm"
        self.driver.get(URL)
        print(f"'{URL}' í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        time.sleep(3)

        one_week_ago = datetime.now() - timedelta(days=7)
        posts_data = []

        # ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘
        article_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.list-titles")
        
        for article in article_elements[:20]:  # ìµœì‹  20ê°œ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
            try:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_element = article.find_element(By.CSS_SELECTOR, "a")
                title = title_element.text.strip()
                link = title_element.get_attribute("href")
                
                # ê¸°ì‚¬ í˜ì´ì§€ë¡œ ì´ë™
                self.driver.get(link)
                time.sleep(2)
                
                # ë‚ ì§œ ì¶”ì¶œ
                try:
                    date_element = self.driver.find_element(By.CSS_SELECTOR, "div.view-date")
                    date_text = date_element.text.strip()
                    # ë‚ ì§œ íŒŒì‹± (ì˜ˆ: "2024.01.15 14:30")
                    post_date = datetime.strptime(date_text, "%Y.%m.%d %H:%M")
                except:
                    post_date = datetime.now()
                
                # 1ì£¼ì¼ ì´ì „ ê¸°ì‚¬ëŠ” ê±´ë„ˆë›°ê¸°
                if post_date < one_week_ago:
                    continue
                
                # ë‚´ìš© ì¶”ì¶œ
                try:
                    content_element = self.driver.find_element(By.CSS_SELECTOR, "div.article-content")
                    content = content_element.text.strip()
                except NoSuchElementException:
                    content = "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                post_data = {
                    "title": title,
                    "content": content,
                    "author": "AIíƒ€ì„ìŠ¤",
                    "url": link,
                    "date": post_date.isoformat(),
                    "source": "aitimes_kr"
                }
                posts_data.append(post_data)
                print(f"âœ… '{title}' ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        
        return posts_data

    def search_perplexity(self, query: str, max_results: int = 10):
        """Perplexity APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰"""
        if not self.perplexity_api_key:
            print("âŒ Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"\n=== Perplexity ê²€ìƒ‰ ì‹œì‘: '{query}' ===")
        
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.perplexity_api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "llama-3.1-sonar-small-128k-online",
            "messages": [
                {
                    "role": "user",
                    "content": f"Search for recent information about: {query}. Provide detailed, factual information about current trends and developments."
                }
            ],
            "max_tokens": 800,
            "temperature": 0.1,
            "search_recency_filter": "month"
        }
        
        try:
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
            search_result = {
                "title": f"Perplexity ê²€ìƒ‰ ê²°ê³¼: {query}",
                "content": content,
                "author": "Perplexity AI",
                "url": "https://www.perplexity.ai",
                "date": datetime.now().isoformat(),
                "source": "perplexity"
            }
            
            print("âœ… Perplexity ê²€ìƒ‰ ì™„ë£Œ")
            return [search_result]
            
        except Exception as e:
            print(f"âŒ Perplexity ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ğŸ”„ GPT fallbackìœ¼ë¡œ ì „í™˜ ì¤‘...")
            
            # GPT fallback ì‹œë„
            try:
                fallback_result = self._search_with_gpt_fallback(query)
                if fallback_result:
                    print("âœ… GPT fallback ê²€ìƒ‰ ì„±ê³µ")
                    return fallback_result
                else:
                    print("âš ï¸ GPT fallbackë„ ì‹¤íŒ¨")
                    return []
            except Exception as fallback_error:
                print(f"âŒ GPT fallback ì‹¤íŒ¨: {fallback_error}")
                return []
    
    def _search_with_gpt_fallback(self, query: str):
        """GPTë¥¼ ì‚¬ìš©í•œ fallback ê²€ìƒ‰"""
        try:
            import os
            from dotenv import load_dotenv
            load_dotenv()
            
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            
            import openai
            client = openai.OpenAI(api_key=openai_api_key)
            
            # GPT-4ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì²˜ë¦¬
            prompt = f"""
            ìµœì‹  AI ì—°êµ¬ ë™í–¥ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ìµœì‹ ì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
            
            ì£¼ì œ: {query}
            
            ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
            1. í˜„ì¬ ì£¼ìš” AI ì—°êµ¬ ë¶„ì•¼
            2. ìµœê·¼ ë°œí‘œëœ ì¤‘ìš”í•œ ë…¼ë¬¸ì´ë‚˜ ê¸°ìˆ 
            3. ì‚°ì—…ê³„ ë™í–¥
            4. í–¥í›„ ì „ë§
            
            í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            """
            
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ AI ì—°êµ¬ ë™í–¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            content = response.choices[0].message.content
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
            search_result = {
                "title": f"GPT Fallback ê²€ìƒ‰ ê²°ê³¼: {query}",
                "content": content,
                "author": "OpenAI GPT-4",
                "url": "https://openai.com",
                "date": datetime.now().isoformat(),
                "source": "gpt_fallback"
            }
            
            return [search_result]
            
        except Exception as e:
            print(f"âŒ GPT fallback ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

def save_search_results(data, filename=None):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"search_results_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ '{filename}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

class SearcherAgent(BaseAgent):
    """ì›¹ í¬ë¡¤ë§ ë° ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""
    
    def __init__(self, perplexity_api_key: str = None):
        super().__init__(
            name="searcher",
            description="ì›¹ í¬ë¡¤ë§ì„ í†µí•´ ìµœì‹  AI ì—°êµ¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì—ì´ì „íŠ¸"
        )
        self.required_inputs = ["search_query"]
        self.output_keys = ["search_results", "search_metadata"]
        self.web_searcher = WebSearcher(perplexity_api_key)
    
    async def process(self, state: WorkflowState) -> WorkflowState:
        """ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ì •ë³´ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        self.log_execution("ì›¹ í¬ë¡¤ë§ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not self.validate_inputs(state):
                raise ValueError("í•„ìˆ˜ ì…ë ¥ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸° - ì‚¬ìš©ì ì¿¼ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
            user_query = getattr(state, 'user_query', '')
            if user_query:
                # ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê°„ì†Œí™”)
                if "AI ì—°êµ¬ ë™í–¥" in user_query or "íŒŸìºìŠ¤íŠ¸" in user_query:
                    search_query = "AI research trends 2024"
                elif "LLM" in user_query or "ëª¨ë¸" in user_query:
                    search_query = "LLM research developments"
                elif "ê¸°ê³„í•™ìŠµ" in user_query or "ë¨¸ì‹ ëŸ¬ë‹" in user_query:
                    search_query = "machine learning advances"
                else:
                    search_query = "AI technology trends"
            else:
                search_query = "AI research trends"
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            state_dict = {k: v for k, v in state.__dict__.items()}
            if 'search_query' in state_dict:
                del state_dict['search_query']
            
            # ì›¹ í¬ë¡¤ë§ ìˆ˜í–‰
            pytorch_posts = self.web_searcher.crawl_pytorch_kr()
            aitimes_posts = self.web_searcher.crawl_aitimes_kr()
            perplexity_results = self.web_searcher.search_perplexity(search_query)
            
            # ëª¨ë“  ê²°ê³¼ í•©ì¹˜ê¸°
            all_results = pytorch_posts + aitimes_posts + perplexity_results
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©
            if not all_results:
                print("âš ï¸ ì›¹ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                try:
                    import json
                    existing_data_path = "output/combined_search_results.json"
                    if os.path.exists(existing_data_path):
                        with open(existing_data_path, 'r', encoding='utf-8') as f:
                            existing_data = json.load(f)
                        
                        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê²°ê³¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        for i, item in enumerate(existing_data[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì‚¬ìš©
                            if 'content' in item:
                                search_item = {
                                    "title": item.get('title', f'AI Research Item {i+1}'),
                                    "content": item['content'],
                                    "author": item.get('author', 'AI Research'),
                                    "url": item.get('url', 'https://ai-research.com'),
                                    "date": item.get('date', datetime.now().isoformat()),
                                    "source": "existing_data"
                                }
                                all_results.append(search_item)
                        
                        print(f"âœ… ê¸°ì¡´ ë°ì´í„°ì—ì„œ {len(all_results)}ê°œ í•­ëª©ì„ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        print("âŒ ê¸°ì¡´ ë°ì´í„° íŒŒì¼ë„ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"âŒ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê²°ê³¼ ì €ì¥
            output_filename = f"output/searcher/search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            save_search_results(all_results, output_filename)
            
            # ì¤‘ë³µë  ìˆ˜ ìˆëŠ” í‚¤ë“¤ ì œê±°
            if 'search_results' in state_dict:
                del state_dict['search_results']
            if 'metadata' in state_dict:
                del state_dict['metadata']
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            search_metadata = {
                "total_results": len(all_results),
                "pytorch_posts": len(pytorch_posts),
                "aitimes_posts": len(aitimes_posts),
                "perplexity_results": len(perplexity_results),
                "output_file": output_filename,
                "search_query": search_query
            }
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©
            merged_metadata = state_dict.get('metadata', {}).copy()
            merged_metadata.update({"search": search_metadata})
            
            new_state = WorkflowState(
                **state_dict,
                search_query=search_query,
                search_results=all_results,
                metadata=merged_metadata
            )
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            new_state = self.update_workflow_status(new_state, "searcher_completed")
            
            self.log_execution(f"ì›¹ í¬ë¡¤ë§ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_results)}ê°œ ê²°ê³¼ (ê²€ìƒ‰ ì¿¼ë¦¬: {search_query})")
            return new_state
            
        except Exception as e:
            self.log_execution(f"ì›¹ í¬ë¡¤ë§ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            raise
        finally:
            # WebDriver ì¢…ë£Œ
            self.web_searcher.close_driver()

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸš€ ì›¹ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. WebSearcher ì´ˆê¸°í™”
    print("\n1ï¸âƒ£ WebSearcher ì´ˆê¸°í™” ì¤‘...")
    searcher = WebSearcher()
    
    try:
        # 2. íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§
        print("\n2ï¸âƒ£ íŒŒì´í† ì¹˜ í•œêµ­ ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§ ì¤‘...")
        pytorch_posts = searcher.crawl_pytorch_kr()
        
        # 3. AIíƒ€ì„ìŠ¤ í¬ë¡¤ë§
        print("\n3ï¸âƒ£ AIíƒ€ì„ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
        aitimes_posts = searcher.crawl_aitimes_kr()
        
        # 4. Perplexity ê²€ìƒ‰
        print("\n4ï¸âƒ£ Perplexity ê²€ìƒ‰ ì¤‘...")
        perplexity_results = searcher.search_perplexity("ìµœì‹  AI íŠ¸ë Œë“œ")
        
        # 5. ê²°ê³¼ í•©ì¹˜ê¸°
        print("\n5ï¸âƒ£ ê²°ê³¼ í•©ì¹˜ê¸° ì¤‘...")
        all_results = pytorch_posts + aitimes_posts + perplexity_results
        
        # 6. ê²°ê³¼ ì €ì¥
        print("\n6ï¸âƒ£ ê²°ê³¼ ì €ì¥ ì¤‘...")
        saved_filename = save_search_results(all_results)
        
        if saved_filename:
            print(f"\nâœ… ì›¹ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ê²°ê³¼: {len(all_results)}ê°œ")
            print(f"   - íŒŒì´í† ì¹˜: {len(pytorch_posts)}ê°œ")
            print(f"   - AIíƒ€ì„ìŠ¤: {len(aitimes_posts)}ê°œ")
            print(f"   - Perplexity: {len(perplexity_results)}ê°œ")
            print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {saved_filename}")
            
            # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
            if all_results:
                print(f"\nğŸ“‹ ìƒ˜í”Œ ê²°ê³¼ (ì²« ë²ˆì§¸ í•­ëª©):")
                sample = all_results[0]
                print(f"ì œëª©: {sample.get('title', 'N/A')}")
                print(f"ì¶œì²˜: {sample.get('source', 'N/A')}")
                print(f"ë‚ ì§œ: {sample.get('date', 'N/A')}")
                print(f"ë‚´ìš© ê¸¸ì´: {len(sample.get('content', ''))}ì")
        else:
            print("âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ì›¹ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        # WebDriver ì¢…ë£Œ
        searcher.close_driver()

if __name__ == "__main__":
    main()

"""Summarizer Agent for creating summaries of collected information."""

import json
import torch
import numpy as np
from tqdm import tqdm
from datetime import datetime

# Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

from .base_agent import BaseAgent
try:
    from state import WorkflowState
except ImportError:
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from state import WorkflowState

# --- GPU ì„¤ì • ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {DEVICE}")

class KoT5Summarizer:
    """
    KoT5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” ì—ì´ì „íŠ¸
    """
    
    def __init__(self, model_name="psyche/KoT5-summarization"):
        """
        KoT5 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            model_name (str): ì‚¬ìš©í•  KoT5 ëª¨ë¸ëª…
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """KoT5 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"ğŸ“¥ KoT5 ëª¨ë¸('{self.model_name}') ë¡œë“œ ì¤‘...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(DEVICE)
            print("âœ… KoT5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def summarize_text(self, text, max_length=200, min_length=30):
        """
        í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
        
        Args:
            text (str): ìš”ì•½í•  í…ìŠ¤íŠ¸
            max_length (int): ìµœëŒ€ ìš”ì•½ ê¸¸ì´
            min_length (int): ìµœì†Œ ìš”ì•½ ê¸¸ì´
            
        Returns:
            str: ìš”ì•½ëœ í…ìŠ¤íŠ¸
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ëƒ„)
            if len(text) > 4000:
                text = text[:4000] + "..."
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                max_length=1024, 
                truncation=True
            ).to(DEVICE)
            
            # ìš”ì•½ ìƒì„±
            summary_ids = self.model.generate(
                inputs['input_ids'],
                max_length=max_length,
                min_length=min_length,
                num_beams=4,
                length_penalty=2.0,
                early_stopping=True
            )
            
            # ë””ì½”ë”©
            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            return summary.strip()
            
        except Exception as e:
            print(f"âš ï¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def load_search_results(filename="combined_search_results.json"):
    """
    ê²€ìƒ‰ ê²°ê³¼ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        filename (str): ë¡œë“œí•  JSON íŒŒì¼ëª…
        
    Returns:
        list: ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°
    """
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… '{filename}' íŒŒì¼ì—ì„œ {len(data)}ê°œì˜ ìƒ˜í”Œì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return data
    except FileNotFoundError:
        print(f"ğŸš¨ ì—ëŸ¬: '{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except json.JSONDecodeError:
        print(f"ğŸš¨ ì—ëŸ¬: '{filename}' íŒŒì¼ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

def save_summarized_results(data, filename=None):
    """
    ìš”ì•½ì´ ì¶”ê°€ëœ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        data (list): ìš”ì•½ì´ ì¶”ê°€ëœ ë°ì´í„°
        filename (str): ì €ì¥í•  íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
    """
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"summarized_search_results_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"âœ… ìš”ì•½ ê²°ê³¼ê°€ '{filename}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def process_search_results(data, summarizer):
    """
    ê²€ìƒ‰ ê²°ê³¼ì— ìš”ì•½ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        data (list): ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°
        summarizer (KoT5Summarizer): ìš”ì•½ ëª¨ë¸
        
    Returns:
        list: ìš”ì•½ì´ ì¶”ê°€ëœ ë°ì´í„°
    """
    print(f"ğŸ“ ì´ {len(data)}ê°œ ìƒ˜í”Œì— ëŒ€í•œ ìš”ì•½ ìƒì„± ì‹œì‘")
    
    processed_data = []
    
    for i, sample in enumerate(tqdm(data, desc="ìš”ì•½ ìƒì„± ì¤‘")):
        # ê¸°ì¡´ ë°ì´í„° ë³µì‚¬
        processed_sample = sample.copy()
        
        # contentê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìš”ì•½ ìƒì„±
        if 'content' in sample and sample['content'].strip():
            try:
                summary = summarizer.summarize_text(sample['content'])
                processed_sample['summary'] = summary
            except Exception as e:
                print(f"âš ï¸ ìƒ˜í”Œ {i+1} ìš”ì•½ ì‹¤íŒ¨: {e}")
                processed_sample['summary'] = "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        else:
            processed_sample['summary'] = "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        processed_data.append(processed_sample)
    
    return processed_data

class SummarizerAgent(BaseAgent):
    """í…ìŠ¤íŠ¸ ìš”ì•½ ì—ì´ì „íŠ¸"""
    
    def __init__(self, model_name="psyche/KoT5-summarization"):
        super().__init__(
            name="summarizer",
            description="í…ìŠ¤íŠ¸ ìš”ì•½ ì—ì´ì „íŠ¸"
        )
        self.required_inputs = ["search_results"]
        self.output_keys = ["summarized_results", "summarization_metadata"]
        self.summarizer = KoT5Summarizer(model_name)
    
    async def process(self, state: WorkflowState) -> WorkflowState:
        """ê²€ìƒ‰ ê²°ê³¼ì— ìš”ì•½ì„ ì¶”ê°€í•©ë‹ˆë‹¤."""
        self.log_execution("í…ìŠ¤íŠ¸ ìš”ì•½ ì‹œì‘")
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not self.validate_inputs(state):
                raise ValueError("í•„ìˆ˜ ì…ë ¥ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            search_results = getattr(state, 'search_results', [])
            if not search_results:
                raise ValueError("ìš”ì•½í•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìš”ì•½ ìƒì„±
            summarized_results = process_search_results(search_results, self.summarizer)
            
            # ê²°ê³¼ ì €ì¥
            output_filename = f"AgentCast/output/summarizer/summarized_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            save_summarized_results(summarized_results, output_filename)
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            new_state = WorkflowState(
                **{k: v for k, v in state.__dict__.items()},
                summarized_results=summarized_results,
                summarization_metadata={
                    "total_items": len(summarized_results),
                    "successful_summaries": sum(1 for item in summarized_results if item.get('summary') != "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."),
                    "output_file": output_filename
                }
            )
            
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸
            new_state = self.update_workflow_status(new_state, "summarizer_completed")
            
            self.log_execution(f"í…ìŠ¤íŠ¸ ìš”ì•½ ì™„ë£Œ: {len(summarized_results)}ê°œ í•­ëª© ì²˜ë¦¬")
            return new_state
            
        except Exception as e:
            self.log_execution(f"í…ìŠ¤íŠ¸ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            raise

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸš€ KoT5 ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 50)
    
    # 1. ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ
    print("\n1ï¸âƒ£ ê²€ìƒ‰ ê²°ê³¼ ë¡œë“œ ì¤‘...")
    data = load_search_results()
    
    if not data:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. KoT5 ëª¨ë¸ ì´ˆê¸°í™”
    print("\n2ï¸âƒ£ KoT5 ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    try:
        summarizer = KoT5Summarizer()
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # 3. ìš”ì•½ ìƒì„±
    print("\n3ï¸âƒ£ ìš”ì•½ ìƒì„± ì¤‘...")
    processed_data = process_search_results(data, summarizer)
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n4ï¸âƒ£ ê²°ê³¼ ì €ì¥ ì¤‘...")
    saved_filename = save_summarized_results(processed_data)
    
    if saved_filename:
        print(f"\nâœ… ìš”ì•½ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜: {len(processed_data)}")
        print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {saved_filename}")
        
        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
        if processed_data:
            print(f"\nğŸ“‹ ìƒ˜í”Œ ê²°ê³¼ (ì²« ë²ˆì§¸ í•­ëª©):")
            sample = processed_data[0]
            print(f"ì œëª©: {sample.get('title', 'N/A')}")
            print(f"ì›ë³¸ ê¸¸ì´: {len(sample.get('content', ''))}ì")
            print(f"ìš”ì•½ ê¸¸ì´: {len(sample.get('summary', ''))}ì")
            print(f"ìš”ì•½: {sample.get('summary', 'N/A')}")
    else:
        print("âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()

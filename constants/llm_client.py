"""LLM client utilities for OpenAI GPT-4 integration."""

import os
import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from openai import AsyncOpenAI
from .ai_models import OPENAI_MODELS


class LLMClient:
    """OpenAI GPT-4 í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤."""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or provide api_key parameter.")
        
        self.client = AsyncOpenAI(api_key=self.api_key)
        self.default_model = "gpt-4"  # GPT-4 ì‚¬ìš©
        self.max_retries = 3
        self.retry_delay = 1.0
    
    async def generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        GPT-4ë¥¼ ì‚¬ìš©í•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì˜¨ë„ ì„¤ì •
            **kwargs: ì¶”ê°€ OpenAI API íŒŒë¼ë¯¸í„°
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        model = model or self.default_model
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(self.max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    **kwargs
                )
                
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                if attempt < self.max_retries - 1:
                    print(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                else:
                    raise e
    
    async def analyze_personalized_data(
        self,
        slack_data: Dict[str, Any],
        notion_data: Dict[str, Any],
        gmail_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê°œì¸í™”ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì—°êµ¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            slack_data: Slackì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
            notion_data: Notionì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
            gmail_data: Gmailì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
            
        Returns:
            ë¶„ì„ëœ ê°œì¸í™” ì •ë³´
        """
        system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
Slack, Notion, Gmail ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì—°êµ¬ ë°©í–¥ê³¼ ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•˜ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{
    "research_interests": ["ê´€ì‹¬ ì—°êµ¬ ë¶„ì•¼ 1", "ê´€ì‹¬ ì—°êµ¬ ë¶„ì•¼ 2"],
    "current_projects": ["ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ 1", "ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ 2"],
    "collaboration_opportunities": ["í˜‘ë ¥ ê¸°íšŒ 1", "í˜‘ë ¥ ê¸°íšŒ 2"],
    "upcoming_deadlines": [
        {"task": "í•  ì¼", "deadline": "ë§ˆê°ì¼", "priority": "high/medium/low"}
    ],
    "research_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
    "preferred_topics": ["ì„ í˜¸ ì£¼ì œ 1", "ì„ í˜¸ ì£¼ì œ 2"],
    "communication_patterns": {
        "active_channels": ["ì±„ë„1", "ì±„ë„2"],
        "frequent_collaborators": ["í˜‘ë ¥ì1", "í˜‘ë ¥ì2"],
        "communication_style": "ì„¤ëª…"
    }
}

ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì—°ê²° ì‹¤íŒ¨í•œ ê²½ìš°ì—ëŠ” í•´ë‹¹ í•„ë“œë¥¼ ë¹ˆ ë°°ì—´ì´ë‚˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

=== Slack ë°ì´í„° ===
{slack_data}

=== Notion ë°ì´í„° ===
{notion_data}

=== Gmail ë°ì´í„° ===
{gmail_data}

ìœ„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì—°êµ¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

        try:
            response = await self.generate_response(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì‚¬ìš©
            )
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                return self._extract_info_from_text(response)
                
        except Exception as e:
            print(f"ê°œì¸í™” ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_analysis()
    
    async def generate_rag_queries(
        self,
        personalized_info: Dict[str, Any],
        user_query: str = ""
    ) -> Dict[str, Any]:
        """
        ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            personalized_info: ê°œì¸í™”ëœ ì •ë³´
            user_query: ì‚¬ìš©ì ì¿¼ë¦¬ (ì„ íƒì‚¬í•­)
            
        Returns:
            ìƒì„±ëœ RAG ì¿¼ë¦¬ ì •ë³´
        """
        system_prompt = """ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{
    "primary_queries": ["ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬ 1", "ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬ 2"],
    "secondary_queries": ["ë³´ì¡° ê²€ìƒ‰ ì¿¼ë¦¬ 1", "ë³´ì¡° ê²€ìƒ‰ ì¿¼ë¦¬ 2"],
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
    "search_scope": {
        "time_range": "ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„",
        "sources": ["ì†ŒìŠ¤1", "ì†ŒìŠ¤2"],
        "languages": ["ì–¸ì–´1", "ì–¸ì–´2"],
        "document_types": ["ë¬¸ì„œ íƒ€ì…1", "ë¬¸ì„œ íƒ€ì…2"]
    },
    "research_priorities": [
        {
            "topic": "ì—°êµ¬ ì£¼ì œ",
            "priority": "high/medium/low",
            "rationale": "ìš°ì„ ìˆœìœ„ ê·¼ê±°"
        }
    ],
    "expected_results": ["ì˜ˆìƒ ê²°ê³¼ íƒ€ì… 1", "ì˜ˆìƒ ê²°ê³¼ íƒ€ì… 2"]
}

ê°œì¸í™”ëœ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©ìê°€ ì •ë§ ê´€ì‹¬ìˆì–´ í•  ë§Œí•œ ìµœì‹  ì—°êµ¬ì™€ ê¸°ìˆ  ë™í–¥ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒ ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

=== ê°œì¸í™”ëœ ì •ë³´ ===
{personalized_info}

=== ì‚¬ìš©ì ì¿¼ë¦¬ ===
{user_query or "ìµœì‹  AI ì—°êµ¬ ë™í–¥ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ê³  ì‹¶ìŠµë‹ˆë‹¤."}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."""

        try:
            response = await self.generate_response(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.5
            )
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                queries = json.loads(response)
            except json.JSONDecodeError:
                queries = self._extract_queries_from_text(response, personalized_info)
            
            # ì¿¼ë¦¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            await self._save_queries_to_file(queries, personalized_info, user_query)
            
            return queries
                
        except Exception as e:
            print(f"RAG ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            fallback_queries = self._get_default_queries()
            
            # í´ë°± ì¿¼ë¦¬ë„ ì €ì¥
            await self._save_queries_to_file(fallback_queries, personalized_info, user_query, is_fallback=True)
            
            return fallback_queries
    
    def _extract_info_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
        import re
        
        keywords = re.findall(r'\b(?:AI|ë¨¸ì‹ ëŸ¬ë‹|ë”¥ëŸ¬ë‹|ìµœì í™”|ë°ì´í„°|ì—°êµ¬|ì•Œê³ ë¦¬ì¦˜|ì‹ ê²½ë§)\b', text, re.IGNORECASE)
        
        return {
            "research_interests": keywords[:3] if keywords else ["AI", "ë¨¸ì‹ ëŸ¬ë‹"],
            "current_projects": ["AI ì—°êµ¬ í”„ë¡œì íŠ¸"],
            "collaboration_opportunities": [],
            "upcoming_deadlines": [],
            "research_keywords": keywords[:5] if keywords else ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”"],
            "preferred_topics": keywords[:3] if keywords else ["AI ì—°êµ¬"],
            "communication_patterns": {
                "active_channels": [],
                "frequent_collaborators": [],
                "communication_style": "í˜‘ë ¥ì "
            }
        }
    
    def _extract_queries_from_text(self, text: str, personalized_info: Dict[str, Any]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        keywords = personalized_info.get("research_keywords", ["AI", "ë¨¸ì‹ ëŸ¬ë‹"])
        
        return {
            "primary_queries": [f"{keyword} ìµœì‹  ì—°êµ¬" for keyword in keywords[:3]],
            "secondary_queries": [f"{keyword} ì‘ìš© ì‚¬ë¡€" for keyword in keywords[:2]],
            "keywords": keywords,
            "search_scope": {
                "time_range": "2023-2024",
                "sources": ["arxiv.org", "ieee.org"],
                "languages": ["en", "ko"],
                "document_types": ["research_paper", "conference_proceedings"]
            },
            "research_priorities": [
                {
                    "topic": keywords[0] if keywords else "AI ì—°êµ¬",
                    "priority": "high",
                    "rationale": "ì‚¬ìš©ìì˜ ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼"
                }
            ],
                        "expected_results": ["ë…¼ë¬¸", "ê¸°ìˆ  ë™í–¥"]
        }
    
    async def _save_queries_to_file(
        self, 
        queries: Dict[str, Any], 
        personalized_info: Dict[str, Any], 
        user_query: str, 
        is_fallback: bool = False
    ):
        """ìƒì„±ëœ ì¿¼ë¦¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # output/queries ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = Path("output/queries")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # íŒŒì¼ëª… ìƒì„±
            prefix = "fallback_" if is_fallback else ""
            filename = f"{prefix}rag_queries_{timestamp}.json"
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            save_data = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "user_query": user_query,
                    "is_fallback": is_fallback,
                    "generation_method": "fallback" if is_fallback else "llm_generated"
                },
                "personalized_info_summary": {
                    "research_keywords": personalized_info.get("personal_info", {}).get("research_keywords", []),
                    "research_interests": personalized_info.get("research_context", {}).get("research_interests", []),
                    "current_projects": personalized_info.get("research_context", {}).get("current_projects", [])
                },
                "generated_queries": queries
            }
            
            # íŒŒì¼ ì €ì¥
            file_path = output_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ RAG ì¿¼ë¦¬ ì €ì¥: {file_path}")
            
            # ìµœì‹  ì¿¼ë¦¬ë„ ë³„ë„ ì €ì¥ (ë®ì–´ì“°ê¸°)
            latest_file = output_dir / "latest_rag_queries.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ìµœì‹  ì¿¼ë¦¬ ì €ì¥: {latest_file}")
            
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_default_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "research_interests": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”"],
            "current_projects": ["AI ì—°êµ¬ í”„ë¡œì íŠ¸"],
            "collaboration_opportunities": [],
            "upcoming_deadlines": [],
            "research_keywords": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”", "ë°ì´í„°"],
            "preferred_topics": ["AI ì—°êµ¬", "ë¨¸ì‹ ëŸ¬ë‹"],
            "communication_patterns": {
                "active_channels": [],
                "frequent_collaborators": [],
                "communication_style": "í˜‘ë ¥ì "
            }
        }
    
    def _get_default_queries(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "primary_queries": ["AI ìµœì‹  ì—°êµ¬ ë™í–¥", "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²• ë°œì „"],
            "secondary_queries": ["AI ì‘ìš© ì‚¬ë¡€", "ë°ì´í„° ìµœì í™”"],
            "keywords": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”"],
            "search_scope": {
                "time_range": "2023-2024",
                "sources": ["arxiv.org", "ieee.org"],
                "languages": ["en", "ko"],
                "document_types": ["research_paper"]
            },
            "research_priorities": [
                {
                    "topic": "AI ì—°êµ¬ ë™í–¥",
                    "priority": "high",
                    "rationale": "ì¼ë°˜ì ì¸ ê´€ì‹¬ ì£¼ì œ"
                }
            ],
            "expected_results": ["ë…¼ë¬¸", "ê¸°ìˆ  ë™í–¥"]
        }


# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_llm_client = None


def get_llm_client() -> LLMClient:
    """ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client

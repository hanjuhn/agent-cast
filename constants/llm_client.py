"""LLM client utilities for OpenAI GPT-4 integration."""

import os
import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from openai import AsyncOpenAI
from .ai_models import OPENAI_MODELS


class LLMClient:
    """OpenAI GPT-4 í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤."""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or provide api_key parameter.")
        
        self.client = AsyncOpenAI(api_key=self.api_key)
        self.default_model = "gpt-4"  # GPT-4 ì‚¬ìš©
        self.max_retries = 3
        self.retry_delay = 1.0
    
    async def generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        GPT-4ë¥¼ ì‚¬ìš©í•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)
            model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì˜¨ë„ ì„¤ì •
            **kwargs: ì¶”ê°€ OpenAI API íŒŒë¼ë¯¸í„°
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        model = model or self.default_model
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # ì¬ì‹œë„ ë¡œì§
        for attempt in range(self.max_retries):
            try:
                response = await self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    **kwargs
                )
                
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                if attempt < self.max_retries - 1:
                    print(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                else:
                    raise e
    
    async def analyze_personalized_data(
        self,
        slack_data: Dict[str, Any],
        notion_data: Dict[str, Any],
        gmail_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê°œì¸í™”ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì—°êµ¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            slack_data: Slackì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
            notion_data: Notionì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
            gmail_data: Gmailì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°
            
        Returns:
            ë¶„ì„ëœ ê°œì¸í™” ì •ë³´
        """
        system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
Slack, Notion, Gmail ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì—°êµ¬ ë°©í–¥ê³¼ ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•˜ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{
    "research_interests": ["ê´€ì‹¬ ì—°êµ¬ ë¶„ì•¼ 1", "ê´€ì‹¬ ì—°êµ¬ ë¶„ì•¼ 2"],
    "current_projects": ["ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ 1", "ì§„í–‰ ì¤‘ì¸ í”„ë¡œì íŠ¸ 2"],
    "collaboration_opportunities": ["í˜‘ë ¥ ê¸°íšŒ 1", "í˜‘ë ¥ ê¸°íšŒ 2"],
    "upcoming_deadlines": [
        {"task": "í•  ì¼", "deadline": "ë§ˆê°ì¼", "priority": "high/medium/low"}
    ],
    "research_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
    "preferred_topics": ["ì„ í˜¸ ì£¼ì œ 1", "ì„ í˜¸ ì£¼ì œ 2"],
    "communication_patterns": {
        "active_channels": ["ì±„ë„1", "ì±„ë„2"],
        "frequent_collaborators": ["í˜‘ë ¥ì1", "í˜‘ë ¥ì2"],
        "communication_style": "ì„¤ëª…"
    }
}

ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì—°ê²° ì‹¤íŒ¨í•œ ê²½ìš°ì—ëŠ” í•´ë‹¹ í•„ë“œë¥¼ ë¹ˆ ë°°ì—´ì´ë‚˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

=== Slack ë°ì´í„° ===
{slack_data}

=== Notion ë°ì´í„° ===
{notion_data}

=== Gmail ë°ì´í„° ===
{gmail_data}

ìœ„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì—°êµ¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

        try:
            response = await self.generate_response(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ì‚¬ìš©
            )
            
            # JSON íŒŒì‹± ì‹œë„
            import json
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                return self._extract_info_from_text(response)
                
        except Exception as e:
            print(f"ê°œì¸í™” ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_analysis()
    
    async def generate_rag_queries(
        self,
        personalized_info: Dict[str, Any],
        user_query: str = ""
    ) -> Any:
        """
        ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ RAG ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            personalized_info: ê°œì¸í™”ëœ ì •ë³´
            user_query: ì‚¬ìš©ì ì¿¼ë¦¬ (ì„ íƒì‚¬í•­)
            
        Returns:
            ìƒì„±ëœ RAG ë‹¨ì¼ ë¬¸ì¥ ì¿¼ë¦¬ (str)
        """
        system_prompt = """ë‹¹ì‹ ì€ ì •ë³´ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°œì¸í™”ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë°”ë¡œ ê²€ìƒ‰ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ë‹¨ì¼ ë¬¸ì¥ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ë¬¸ì¥ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì•ë’¤ ì—¬ë°± ì™¸ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸/ë”°ì˜´í‘œ/ì½”ë“œë¸”ë¡/JSONì„ ë„£ì§€ ë§ˆì„¸ìš”.
- 20~100ì ì‚¬ì´ì˜ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- í•µì‹¬ ì£¼ì œ, ì‹œì (ìµœì‹ ì„±), ë¬¸ì„œìœ í˜•(ë…¼ë¬¸/ë¦¬ë·° ë“±)ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”.
ì˜ˆ: "ë™ì  ë°°ì¹­ì„ í™œìš©í•œ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ íš¨ìœ¨ í•™ìŠµ ìµœì‹  ë…¼ë¬¸ê³¼ ë¦¬ë·°"
"""

        user_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ í•˜ë‚˜ì˜ í•œêµ­ì–´ ê²€ìƒ‰ ë¬¸ì¥ì„ ìƒì„±í•˜ì„¸ìš”:

[ê°œì¸í™”ëœ ì •ë³´]
{personalized_info}

[ì‚¬ìš©ì ìš”ì²­]
{user_query or "ìµœì‹  AI ì—°êµ¬ ë™í–¥ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ê³  ì‹¶ìŠµë‹ˆë‹¤."}
"""

        try:
            response = await self.generate_response(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.5
            )
            # ë‹¨ì¼ ë¬¸ì¥ ì¶”ì¶œ
            single_query = self._extract_single_query(response, personalized_info, user_query)

            # íŒŒì¼ ì €ì¥
            await self._save_queries_to_file(single_query, personalized_info, user_query)

            return single_query
                
        except Exception as e:
            print(f"RAG ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            fallback_query = self._get_default_single_query(personalized_info, user_query)

            # í´ë°± ì¿¼ë¦¬ë„ ì €ì¥
            await self._save_queries_to_file(fallback_query, personalized_info, user_query, is_fallback=True)

            return fallback_query
    
    def _extract_info_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
        import re
        
        keywords = re.findall(r'\b(?:AI|ë¨¸ì‹ ëŸ¬ë‹|ë”¥ëŸ¬ë‹|ìµœì í™”|ë°ì´í„°|ì—°êµ¬|ì•Œê³ ë¦¬ì¦˜|ì‹ ê²½ë§)\b', text, re.IGNORECASE)
        
        return {
            "research_interests": keywords[:3] if keywords else ["AI", "ë¨¸ì‹ ëŸ¬ë‹"],
            "current_projects": ["AI ì—°êµ¬ í”„ë¡œì íŠ¸"],
            "collaboration_opportunities": [],
            "upcoming_deadlines": [],
            "research_keywords": keywords[:5] if keywords else ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”"],
            "preferred_topics": keywords[:3] if keywords else ["AI ì—°êµ¬"],
            "communication_patterns": {
                "active_channels": [],
                "frequent_collaborators": [],
                "communication_style": "í˜‘ë ¥ì "
            }
        }
    
    def _extract_queries_from_text(self, text: str, personalized_info: Dict[str, Any]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        keywords = personalized_info.get("research_keywords", ["AI", "ë¨¸ì‹ ëŸ¬ë‹"])
        
        return {
            "primary_queries": [f"{keyword} ìµœì‹  ì—°êµ¬" for keyword in keywords[:3]],
            "secondary_queries": [f"{keyword} ì‘ìš© ì‚¬ë¡€" for keyword in keywords[:2]],
            "keywords": keywords,
            "search_scope": {
                "time_range": "2023-2024",
                "sources": ["arxiv.org", "ieee.org"],
                "languages": ["en", "ko"],
                "document_types": ["research_paper", "conference_proceedings"]
            },
            "research_priorities": [
                {
                    "topic": keywords[0] if keywords else "AI ì—°êµ¬",
                    "priority": "high",
                    "rationale": "ì‚¬ìš©ìì˜ ì£¼ìš” ê´€ì‹¬ ë¶„ì•¼"
                }
            ],
                        "expected_results": ["ë…¼ë¬¸", "ê¸°ìˆ  ë™í–¥"]
        }

    def _extract_single_query(self, text: str, personalized_info: Dict[str, Any], user_query: str) -> str:
        """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì¼ ë¬¸ì¥ ì¿¼ë¦¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. JSONì¼ ê²½ìš°ì—ë„ ì²« ì¿¼ë¦¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        try:
            data = json.loads(text)
            if isinstance(data, dict):
                # ìš°ì„ ìˆœìœ„: explicit 'query' -> primary_queries[0] -> keywords ê¸°ë°˜ ìƒì„±
                query = data.get("query")
                if not query:
                    primary = data.get("primary_queries") or []
                    if isinstance(primary, list) and primary:
                        query = str(primary[0])
                if not query:
                    kws = data.get("keywords") or []
                    if isinstance(kws, list) and kws:
                        head = " ".join([str(k) for k in kws[:2]])
                        query = f"{head} ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ ì£¼ìš” ë…¼ë¬¸"
                if query:
                    return query.strip()
        except Exception:
            pass

        # JSONì´ ì•„ë‹ˆê±°ë‚˜ ì í•©í•œ í‚¤ê°€ ì—†ì„ ë•Œ: ì‘ë‹µì—ì„œ í•œ ì¤„ë§Œ ì¶”ì¶œ
        line = text.strip().splitlines()[0].strip().strip('"').strip("'")
        if line:
            return line

        # ìµœì¢… í´ë°±
        return self._get_default_single_query(personalized_info, user_query)

    def _get_default_single_query(self, personalized_info: Dict[str, Any], user_query: str) -> str:
        """ê°œì¸í™” ì •ë³´ë¥¼ ë°˜ì˜í•œ ê¸°ë³¸ ë‹¨ì¼ ë¬¸ì¥ ì¿¼ë¦¬ ìƒì„±."""
        personal = personalized_info.get("personal_info", {}) if isinstance(personalized_info, dict) else {}
        context = personalized_info.get("research_context", {}) if isinstance(personalized_info, dict) else {}
        keywords = personal.get("research_keywords", [])
        interests = context.get("research_interests", [])
        base = (user_query or "ìµœì‹  AI ì—°êµ¬ ë™í–¥").strip()
        core = (interests[0] if interests else (keywords[0] if keywords else "AI")).strip()
        if core in base:
            topic = base
        else:
            topic = f"{core} {base}"
        return f"{topic} ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ê³¼ ë¦¬ë·°"
    
    async def _save_queries_to_file(
        self, 
        queries: Dict[str, Any], 
        personalized_info: Dict[str, Any], 
        user_query: str, 
        is_fallback: bool = False
    ):
        """ìƒì„±ëœ ì¿¼ë¦¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # output/queries ë””ë ‰í† ë¦¬ ìƒì„±
            output_dir = Path("output/queries")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # íŒŒì¼ëª… ìƒì„±
            prefix = "fallback_" if is_fallback else ""
            filename = f"{prefix}rag_queries_{timestamp}.json"
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            save_data = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "user_query": user_query,
                    "is_fallback": is_fallback,
                    "generation_method": "fallback" if is_fallback else "llm_generated"
                },
                "personalized_info_summary": {
                    "research_keywords": personalized_info.get("personal_info", {}).get("research_keywords", []),
                    "research_interests": personalized_info.get("research_context", {}).get("research_interests", []),
                    "current_projects": personalized_info.get("research_context", {}).get("current_projects", [])
                },
                "generated_queries": queries
            }
            
            # íŒŒì¼ ì €ì¥
            file_path = output_dir / filename
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ RAG ì¿¼ë¦¬ ì €ì¥: {file_path}")
            
            # ìµœì‹  ì¿¼ë¦¬ë„ ë³„ë„ ì €ì¥ (ë®ì–´ì“°ê¸°)
            latest_file = output_dir / "latest_rag_queries.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ìµœì‹  ì¿¼ë¦¬ ì €ì¥: {latest_file}")
            
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_default_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "research_interests": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”"],
            "current_projects": ["AI ì—°êµ¬ í”„ë¡œì íŠ¸"],
            "collaboration_opportunities": [],
            "upcoming_deadlines": [],
            "research_keywords": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”", "ë°ì´í„°"],
            "preferred_topics": ["AI ì—°êµ¬", "ë¨¸ì‹ ëŸ¬ë‹"],
            "communication_patterns": {
                "active_channels": [],
                "frequent_collaborators": [],
                "communication_style": "í˜‘ë ¥ì "
            }
        }
    
    def _get_default_queries(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¿¼ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "primary_queries": ["AI ìµœì‹  ì—°êµ¬ ë™í–¥", "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²• ë°œì „"],
            "secondary_queries": ["AI ì‘ìš© ì‚¬ë¡€", "ë°ì´í„° ìµœì í™”"],
            "keywords": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ìµœì í™”"],
            "search_scope": {
                "time_range": "2023-2024",
                "sources": ["arxiv.org", "ieee.org"],
                "languages": ["en", "ko"],
                "document_types": ["research_paper"]
            },
            "research_priorities": [
                {
                    "topic": "AI ì—°êµ¬ ë™í–¥",
                    "priority": "high",
                    "rationale": "ì¼ë°˜ì ì¸ ê´€ì‹¬ ì£¼ì œ"
                }
            ],
            "expected_results": ["ë…¼ë¬¸", "ê¸°ìˆ  ë™í–¥"]
        }


# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_llm_client = None


def get_llm_client() -> LLMClient:
    """ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client

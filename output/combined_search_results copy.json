[
    {
        "title": "[GN⁺] GPT-OSS vs. Qwen3 및 GPT-2 이후 LLM 아키텍처 발전 상세 비교",
        "url": "https://discuss.pytorch.kr/t/gn-gpt-oss-vs-qwen3-gpt-2-llm/7497",
        "content": "GPT-OSS vs. Qwen3 및 GPT-2 이후 LLM 아키텍처 발전 상세 비교 글 소개\nOpenAI가 gpt-oss-20b/120b 모델을 오픈 가중치로 공개함에 따라 2019년 GPT-2 이후 처음으로 OpenAI의 대형 공개 가중치 LLM이 등장함\ngpt-oss 모델은 GPT-2와 비교해 Dropout, Absolute Position Embedding, GELU 등을 효율적인 현대 기법인 RoPE, SwiGLU, RMSNorm 등으로 대체하며 발전함\nMixture-of-Experts(모듈형 전문가 구조), Sliding Window Attention, MXFP4 양자화 등의 적용으로 성능 효율뿐 아니라 단일 GPU 실행 환경을 크게 개선함\nQwen3와의 비교에서 아키텍처 깊이/넓이, 전문가 수, 주의 편향, 오픈소스 라이선스 등 다양한 차별점이 존재함을 확인함\ngpt-oss-20b는 최신 하드웨어에 맞춘 경량화와 reasoning effort 조정 기능으로 실제 활용성과 연구 확장성 모두 확보함\n개요 및 주요 혁신\nOpenAI는 gpt-oss-20b/120b를 2019년 GPT-2 이후 처음으로 오픈 가중치로 공개함\n일반 사용자 GPU(최대 16GB RAM)에서 20B, H100 80GB에서 120B를 실행 가능하게 함\nMXFP4 최적화로 단일 GPU 실행, 소비자 접근성 확대\nGPT-2 → gpt-oss 주요 아키텍처 변화\nDropout 제거\nGPT-2에는 Dropout이 포함됐으나 대량 데이터 단일 epoch 학습 환경에선 오히려 성능 저하가 확인됨\n최근 연구 결과에서도 Dropout 미적용이 LLM의 다운스트림 작업에서 더 뛰어난 성능을 보임\nRoPE(회전 위치 임베딩) 채택\n기존 절대 위치 임베딩 대신 RoPE(Rotary Position Embedding) 가 주류로 자리 잡음\nRoPE는 쿼리/키 벡터의 각도를 위치에 따라 회전시켜 더 유연하고 일반화된 위치 정보를 제공함\nSwiGLU 활성화 함수와 GLU 도입\nGEGLU/SwiGLU 등 GLU 방식 도입으로 기존 2-layer FFN보다 적은 파라미터로 더 우수한 표현 능력을 발휘함\nSwish는 연산적으로도 GELU 대비 효율적\nMixture-of-Experts(MoE) 적용\n단일 FFN 대신 다중 전문가(Expert) 네트워크를 활용해 매 토큰 생성 시 일부 전문가만 활성화\n모델 파라미터 수를 급격히 늘리면서도 추론 효율성(희소성) 유지, 학습 용량 증대\nGrouped Query Attention(GQA) 도입\n기존 Multi-Head Attention 대비 키/값 공유로 메모리 및 연산량 절감 효과\n성능 손실 없이 효율성 개선, 대규모 LLM에서 표준적 적용 추세\nSliding Window Attention 활용\n일부 레이어마다 전체 문맥 대신 최근 128토큰 한정 Sliding Window로 국소 주의 계산, 메모리 사용량 최소화\n성능 저하 없이 빠른 추론, 대규모 컨텍스트 지원용\nRMSNorm 채택\nLayerNorm 대신 RMSNorm 적용으로 연산 효율 증대\nLayerNorm의 평균/분산 계산 대신 RMS(평균제곱근)를 적용, GPU 연산 부담 감소\ngpt-oss와 Qwen3 비교\n규모/구조 차이\nQwen3은 더 깊은(48개 Transformer 블록) 구조이나, gpt-oss는 더 넓은(emb dimension, head 수 증가) 구조\n깊은 모델이 더 유연하지만 학습 어려움, 넓은 모델이 추론 병렬화에 유리(Gemma 2 논문, 9B 모델 기준 넓은 쪽이 소폭 우세)\nMoE 구조 차이\ngpt-oss-20b: 32명 대형 전문가, 4명만 활성화\nQwen3: 다수 소형 전문가, 8명 활성화\n최신 흐름은 더 많은 소형 전문가 구성이 효과적이라는 방향이나 gpt-oss는 대형-소수 구조 고수 (20B, 120B에서는 전문가 및 블록 수만 조정)\nAttention Bias와 Sinks\ngpt-oss는 attention에 bias 유닛 활용 (GPT-2 시절 이후 보기 드문 방식)\n하지만 key-proj에는 효과 미미함이 최근 연구에서 밝혀짐\n주의 sink는 시퀀스 시작위치에 항상 attend되는 특수 토큰 개념이나, gpt-oss에서는 입력 토큰에 변형 없이 Learned bias logit 형태로 각 head에 추가 적용\n라이선스 및 공개 범위\nApache 2.0 오픈소스 라이선스로 상업적 활용/파생 모델 구축 자유\n단, 진정한 의미의 오픈소스(학습 코드, 데이터 세트 공개)는 아님(‘open weight’ 모델임)\n기타 세부 사항 및 실제 운용\n훈련/최적화\ngpt-oss는 2.1M H100-hours 컴퓨팅 리소스로 훈련\n영어 중심, STEM과 코딩, 일반 지식 텍스트에 집중\n사전학습+지도 미세학습(Instruction), RL 기반 reasoning 단계 등 최신 기법 적용\nReasoning Effort 조절\nSystem prompt를 통해 reasoning effort(저/중/고)를 설정해 답변 길이·정확도를 자동 조정\n단순 작업은 저효율로 빠르게, 복잡한 reasoning이 필요하면 높게 설정 가능\nMXFP4 양자화로 단일 GPU 지원\nMXFP4 포맷 활용으로 20B도 16GB VRAM(최신 GPU 필수)에서 구동 가능\n120B는 H100 기준 80GB 메모리면 단일 GPU에서 실현 가능, 분산 처리 없고 구동 간편\n벤치마크 및 실 사용성\ngpt-oss는 학습 초점이 reasoning에 치중, 일부 범용 지식 질문에는 환각(hallucination) 경향\n사용성 면에서는 현존 오픈 모델 중 상위, tool integration과 조합 시 실용성 강화 예정\n실제 사용에서 정확도와 reasoning의 균형, 추후 타 오픈모델과의 비교 필요\nGPT-5와의 비교\ngpt-oss-120b는 OpenAI 상용 모델(GPT-5)과 벤치마크 기준 근접 성능을 보임\n현실 환경에서의 우위는 더 지켜봐야 하나, 오픈 가중치로 제공되는 최신 LLM 중 강력한 대안임\n벤치마크만으로 실전 경쟁력 완전히 설명하기엔 한계, 향후 외부 비교 및 연구에 큰 기회 제공\n요약\ngpt-oss 시리즈의 등장은 대형 오픈 가중치 LLM 분야의 새로운 기준 제시, 최신 LLM들이 도입한 혁신적 아키텍처들이 실제로 어떻게 구현·적용됐는지 상세히 비교, 분석됨\nQuen3, GPT-5 등 다른 최신 모델과의 차별점과 추세를 파악할 수 있어, 실제 적용/연구에 유용한 최신 동향 파악 가능\nHacker News 의견\nQwen3가 로컬 테스트에서 훨씬 뛰어남을 확인함. 32B 파라미터 버전에서는 프롬프트를 거의 완벽하게 지키며 결과가 자연스럽게 나옴. 반면 simplebench gpt-oss(120B)는 논리 퍼즐에서 좋지 않은 성능을 보임. 이런 차이는 트레이닝 방식, 모델 차원, 그리고 적은 수의 대형 전문가 vs 많은 수의 소형 전문가 등에서 비롯된다고 생각함\nQwen3 32B는 모든 파라미터를 항상 사용하는 덴스 모델임. GPT OSS 20B는 일부만 사용하는 스파스 MoE(Expert of Experts) 모델로, 한 번에 약 3.6B만 활용함. 이로 인해 덴스 20B 모델보다 빠르고, 3.6B 모델보다는 똑똑함. 공정한 비교라면 덴스 8B 모델과 비교해야 하고, Qwen Coder 30B A3B 같은 모델도 좋은 비교 지점임\n내 생각에 이런 차이는 모델 아키텍처보다는 데이터와 트레이닝 파이프라인 영향이 훨씬 크다고 봄. gpt-oss가 Phi 스타일의 합성 데이터셋만을 활용하고, 주로 벤치마크 게임에 집중했다는 이야기가 있는데, 그 증거가 충분히 설득력 있어 보임\nMoE의 기대 성능 공식은 sqrt(활성 헤드 수 * 전체 파라미터 수)임. 예를 들어 sqrt(120*5) ~= 24로, GPT-OSS 120B는 사실 24B 수준의 성능과 훨씬 작은 모델 수준의 속도를 제공함\nqwen3는 느린 편임. 직접 써보니 동작은 하는데 속도가 느리고, 기능이 부족한 느낌임\nSebastian Raschk의 블로그 글들이 보물 같은 정보임. get-oss와 qwen3 모델을 Ollama, LM Studio로 로컬에서 사용하고, 대형 모델은 상용 API를 씀. get-oss는 프롬프트에 많은 컨텍스트 정보를 넘기면 좋은 결과를 주고, qwen3는 그냥 훌륭함. 3년 전까지는 신경망, GAN, RNN, LSTM 등 머신러닝을 실제로 구현할 정도로 잘 이해했었는데, 요즘 LLM은 직접 개발할 정도로 쉽지 않아서 아쉬움. Sebastian Raschk의 책도 보고 있는데, 아마 끝까지 다 못 볼 듯함\n믿을 수 없을 정도로 빠르게 변화하는 분야에서 Sebastian Raschk가 항상 최신 정보를 간결하게 정리해줘서 정말 도움을 받고 있음\n로컬 3090 GPU에서 qwen3 coder instruct 30b-a3b exl3 q6 모델을 돌려서 샘플 페이지도 만들고, 서버 실행, 남아있는 서버 감지, 이를 직접 종료한 후(권한 요청까지 받음), 재실행 후 ip를 자동으로 찾아 브라우저에 띄우는 과정을 해봄. 이제는 더 이상 단순 데모가 아니라 주니어나 인턴에게도 실질적으로 유용한 수준의 도움임\n내 경험상 qwen3-coder가 월등히 뛰어남. gpt-oss:20b도 설치해봤지만, 코드 요약을 시키면 qwen3는 몇 초 만에 결과가 나오고 gpt-oss는 5분 넘게 아무 일도 하지 않아서 중단함. 그래서 그냥 qwen3만 씀. 만약 원하는 답을 못 받으면, 검색 엔진이나 Perplexity를 씀. 10GB 3080, Ryzen 3600x, 32GB RAM을 쓰고 있음. Qwen3-coder는 지금까지 써본 것 중 최고임\nQwen3 coder 480B는 Sonnet 4와 맞먹을 정도로 좋음. 이 덕분에 중국 모델이 미국 기반 모델을 조만간 앞지를 수도 있다는 실감을 처음 가짐(특히 코딩 분야에서)\ngpt-oss 20B는 10GB에 올라가지 않아서 생긴 문제일 가능성이 있음\n나도 gpt-oss-20b를 간단하게 쓰는데, 짧은 프롬프트(단문)에는 무한 반복에 빠질 때가 있음. llama.cpp로 돌릴 때 반복 패널티 값을 작게 잡으니 그런 문제가 없었음(주로 diff 분석에 하루 몇 번 정도 사용함). 단, 내가 운이 좋은 걸 수도 있음\n혹시 agentic 방식(여러 번의 질문과 답변을 주고받는 자동화)으로 쓰고 있는지, 아니면 복사해서 “이 코드 짜줘” 식의 단일 입력/출력으로만 쓰는지 궁금함. 최신 공개 모델이 agentic한 코딩에서 얼마나 상용 모델을 따라잡았는지 알고 싶음\n요즘 오픈 웨이트 LLM들은 아키텍처가 너무 비슷하고, 혁신이 데이터나 RL 쪽에서만 일어나고 있는 점이 흥미로움. 예전 대형 ML 조직에서는 아키텍처 튜닝이 가장 중요했는데 현실은 달라 보임\nLLM 규모에서는 하이퍼파라미터 튜닝 자체가 불가능하다고 봄. 비용이 너무 커서 여러 아키텍처를 기본 테스트만 하고, 하나를 골라 데이터와 RL로 최적화하는 식임\n좋은 지적임. LLM 덕분에 리소스만 충분하면 누구나 도전할 수 있게 되었음. 아키텍처가 꽤 조정에 강하고, 충분한 컴퓨트와 데이터를 넣으면 확장 법칙(scaling law)를 어겨도 괜찮은 모델을 만들 수 있음(Llama 3가 과거에 보여줬던 것처럼)\nQwen3 4B 모델을 로컬에서 정말 잘 사용 중임. 온라인 모델은 거의 안 쓰고, 웹 검색도 훨씬 타깃팅이 잘 됨. 완전히 신뢰하지는 않지만 전반적으로 괜찮음. 이런 오픈소스 모델이 로컬 지식 자동화의 판도를 바꿀 거라고 확신함\nQwen이 직접 더 나은 검색 파라미터를 안내해주는 것인지, 아니면 Qwen이 실제 웹 검색까지 해주는 것인지 궁금함\nLM Arena에서 순수 Transformer 기반이 아닌 모델 중 가장 성능이 좋은 모델은 Jamba임(Transformers와 state space 모델의 하이브리드 구조, 96위). Tencent의 hunyuan-turbos도 역시 하이브리드로, 22위임. arxiv 논문 참고\nLLM은 보통 아주 거대한 데이터셋을 딱 한 번(단일 에폭)만 학습함. 이는 여러 번 반복 학습(수백 에폭) 전제를 깔고 있던 Dropout 방식과는 다른 환경임\n이건 잘 알려진 사실임. GPT-3 논문의 Table 2.2를 참고하면 됨\n대형 연구실에서 공개하는 모델들이 추가적인 학습을 더 하면 얼마나 발전할 수 있을지 궁금함. 예를 들어 GPT-OSS가 210만 시간 학습했다면, 그걸 두 배로 늘리면 얼마나 개선될 수 있을지 알고 싶음\nGPT-4.5는 사실 더 큰 GPT-5로 기획되어 더 많은 데이터를 학습했을 수도 있음. 하지만 너무 비싸서 대규모 상용화는 못 했고, RL 적용 버전도 못 보게 된 아쉬움 있음\nGPT-5에서 활용된 RL 기반 트레이닝 첨단 기법도 무한정 확장되진 않는다는 점이 이미 드러남\n사이트에 접속하면 \"연결이 안전하지 않습니다\"라는 오류 메시지를 받음. \"magazine.sebastianraschka.com 웹사이트가 HSTS를 사용 중이라 지금 방문할 수 없습니다\"라고 나옴. 크롬 최신 버전, Ubuntu 환경임\n원문\nmagazine.sebastianraschka.com\nFrom GPT-2 to gpt-oss: Analyzing the Architectural Advances\nAnd How They Stack Up Against Qwen3\n출처 / GeekNews\nGeekNews – 11 Aug 25\nGPT-OSS vs. Qwen3 및 GPT-2 이후 LLM 아키텍처 발전 상세 비교 | GeekNews\nOpenAI가 gpt-oss-20b/120b 모델을 오픈 가중치로 공개함에 따라 2019년 GPT-2 이후 처음으로 OpenAI의 대형 공개 가중치 LLM이 등장함gpt-oss 모델은 GPT-2와 비교해 Dropout, Absolute Position Embedding, GELU 등을 효율적인 현대 기법인 RoPE, SwiGLU, RMSNorm 등으로 대체\n알려드립니다\n이 글은 국내외 IT 소식들을 공유하는 GeekNews의 운영자이신 xguru님께 허락을 받아 GeekNews에 게제된 AI 관련된 소식을 공유한 것입니다.\n출처의 GeekNews 링크를 방문하시면 이 글과 관련한 추가적인 의견들을 보시거나 공유하실 수 있습니다!\n아래쪽에 좋아요를 눌러주시면 새로운 소식을 정리하고 공유하는데 힘이 됩니다~",
        "date": "2025-08-17",
        "source": "파이토치 한국 사용자 모임",
        "category": "기술"
    },
    {
        "title": "all-smi: 현존하는 대부분의 GPU 및 NPU의 하드웨어 모니터링 CLI 도구",
        "url": "https://discuss.pytorch.kr/t/all-smi-gpu-npu-cli/7496",
        "content": "all-smi 소개\n최근 AI·머신러닝 워크로드와 고성능 컴퓨팅(HPC) 환경이 확산되면서 GPU와 NPU의 실시간 상태 추적은 필수적인 운영 요소가 되고 있습니다. all-smi는 이러한 요구에 대응하기 위해 플랫폼 간 호환성과 확장성을 극대화하였고, 설치와 사용이 간단하며 다양한 배포 방법(Homebrew, PPA, .deb, Cargo, 바이너리 다운로드 등)을 지원하는 명령줄 기반(CLI)의 도구입니다.\nall-smi는 다양한 하드웨어 플랫폼에서 GPU와 NPU 자원을 실시간으로 모니터링할 수 있는 명령줄 기반 유틸리티입니다. 또한, 터미널 기반 UI를 통해 가속기 사용량, 메모리, 온도, 전력 소비량 등을 직관적으로 시각화하며, 색상 코드와 인터랙티브 정렬 기능을 제공해 대규모 모니터링 환경에서도 효율적으로 활용할 수 있습니다.\n전통적으로 NVIDIA GPU 모니터링에 사용되는 nvidia-smi를 대체할 수 있도록 설계되었으며, NVIDIA GPU뿐 아니라 NVIDIA Jetson, Apple Silicon GPU, Tenstorrent NPU, Rebellions NPU, Furiosa NPU 등 여러 가속기 하드웨어를 지원합니다. 특히 단일 PC뿐 아니라 클러스터 환경에서도 여러 노드를 동시에 모니터링할 수 있으며, 로컬·원격 모드와 Prometheus API 통합 기능을 제공합니다. Prometheus와 연동을 하게되면 Grafana 같은 시각화 대시보드로 확장 가능하며, 내장 모의(Mock) 서버로 개발·테스트 환경 구축도 손쉽게 할 수 있어 개발자와 운영자 모두에게 유용합니다.\nnvidia-smi와의 비교\n기능 nvidia-smi all-smi\n지원 하드웨어 NVIDIA GPU 전용 NVIDIA, Apple Silicon, Jetson, Tenstorrent, Rebellions, Furiosa\n운영체제 지원 Linux, Windows (일부 macOS 제한) Linux, macOS\nGPU 외 자원 모니터링 제한적 (GPU 중심) GPU, NPU, CPU, 메모리, 디스크\n클러스터 원격 모니터링 불가능 가능 (256+ 노드 지원)\nPrometheus API 연동 불가능 가능\nUI 인터랙션 기본 텍스트 출력 (정렬/컬러 없음) 컬러 코드, 컬럼별 정렬, 마우스 클릭 지원\n설치 방식 다양성 드라이버 설치 시 포함 Homebrew, PPA, Debian 패키지, Cargo, 바이너리 다운로드, 소스 빌드\n기존의 nvidia-smi는 NVIDIA GPU에 특화되어 있으며, 다른 하드웨어 플랫폼에서는 사용할 수 없는 한계가 있습니다. 반면, all-smi는 다양한 하드웨어 가속기(GPU·NPU)를 단일 인터페이스에서 관리할 수 있고, NVIDIA 외의 플랫폼에서도 동일한 명령과 UI를 제공합니다.\n또한, all-smi는 CPU·메모리·디스크 모니터링까지 통합 제공하며, 클러스터 단위의 원격 모니터링과 Prometheus API 연동 기능을 내장해 확장성이 뛰어납니다. UI 측면에서도 색상 코드, 탭 전환, 컬럼별 정렬, 마우스 클릭 정렬 등 상호작용성이 강화되어 있습니다.\n즉, nvidia-smi가 NVIDIA 전용 도구라면, all-smi는 \"플랫폼 불문 통합 하드웨어 모니터링 도구\"라고 요약할 수 있습니다.\nall-smi의 주요 기능\nGPU 및 NPU 모니터링: all-smi는 GPU 이름·드라이버 버전·사용량·메모리 상태·온도·클럭 속도·전력 소비 등 세부 지표를 실시간으로 표시합니다. NVIDIA, Apple Silicon, Jetson, Tenstorrent, Rebellions, Furiosa 등 각 플랫폼별 특화 지표도 지원하며, 다중 GPU 환경에서도 개별 장치별 모니터링이 가능합니다.\nCPU 및 메모리 모니터링: CPU는 소켓별 사용량, 클럭 속도, 온도, 전력 소비량을 표시하며, Apple Silicon의 경우 P코어·E코어별 사용량과 주파수까지 세분화 지원합니다. 메모리는 총량·사용량·가용량·스왑 영역까지 시각화하며, Linux에서는 버퍼·캐시 메모리 정보까지 제공합니다.\n프로세스·클러스터 관리: GPU 메모리를 사용하는 프로세스 목록, PID, CPU 사용률, 실행 사용자 등을 보여주며 컬럼별 색상 표시와 정렬 기능을 지원합니다. 클러스터 관리 모드에서는 전체 노드와 GPU 상태를 집계해 평균 사용량, 온도, 전력 통계를 실시간으로 제공합니다.\n원격 모니터링 및 API 연동: 256개 이상의 원격 노드를 동시에 모니터링할 수 있으며, Prometheus API 형식으로 메트릭을 노출해 Grafana 등 외부 모니터링 시스템과 연동할 수 있습니다.\nall-smi 설치 방법\nHomebrew (macOS/Linux)\nbrew tap lablup/tap\nbrew install all-smi\nUbuntu PPA\nsudo add-apt-repository ppa:lablup/backend-ai\nsudo apt update\nsudo apt install all-smi\nDebian 패키지: Releases 페이지에서 .deb 다운로드 후 설치\nCargo 설치: cargo install all-smi\n바이너리 다운로드: GitHub Releases에서 직접 내려받아 $PATH에 추가\n소스 빌드: 개발자 문서 참고\n사용 예시\n# 로컬 모니터링 (기본)\nall-smi\nsudo all-smi local --interval 5\n\n# 원격 노드 모니터링\nall-smi view --hosts http://node1:9090 http://node2:9090\nall-smi view --hostfile hosts.csv\n\n# Prometheus API 서버 실행\nall-smi api --port 9090 --processes\n라이선스\nall-smi 프로젝트는 Apache License 2.0으로 공개 및 배포되고 있습니다. 상업적 이용에 제한이 없습니다.\nall-smi GitHub 저장소\ngithub.com\nGitHub - inureyes/all-smi: Command-line utility for monitoring GPU hardware.\nCommand-line utility for monitoring GPU hardware.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "date": "2025-08-16",
        "source": "파이토치 한국 사용자 모임",
        "category": "기술"
    },
    {
        "title": "DINOv3: 자기 지도 학습(SSL)을 활용한 초대규모의 범용 비전 백본(Vision Backbone) 모델(feat. Meta AI)",
        "url": "https://discuss.pytorch.kr/t/dinov3-ssl-vision-backbone-feat-meta-ai/7495",
        "content": "DINOv3 소개\nDINOv3는 Meta AI가 개발한 차세대 자기지도학습(Self-Supervised Learning, SSL) 기반 비전 모델로, 대규모 이미지 데이터에서 범용적으로 활용 가능한 고성능 시각 백본(vision backbone)을 제공합니다. 기존의 강력한 이미지 인코딩 모델들이 웹 캡션 같은 사람 손으로 작성된 메타데이터에 의존했던 것과 달리, DINOv3는 전혀 라벨이 없는 상태에서 이미지 데이터로부터 의미있는 시각 표현을 스스로 학습하는 자기 지도 학습(Self-Supervised Learning, SSL) 기반의 비전 모델입니다.이를 통해 라벨 수집이 어렵거나 불가능한 분야에서도 대규모 학습이 가능해졌습니다.\n특히, DINOv3는 다양한 이미지 도메인에 걸쳐 범용적으로 사용 가능한 백본을 제공함으로써, 웹 이미지뿐만 아니라 위성, 의료, 산업용 이미지를 포함한 다양한 도메인에서 활용할 수 있으며, 이미지 분류, 객체 탐지, 시맨틱 분할, 비디오 객체 추적 등 폭넓은 비전 작업에서 최첨단 성능을 기록합니다. 17억 장의 이미지와 최대 70억 개의 파라미터로 학습하여 단일 백본으로도 밀집 예측(dense prediction) 작업에서 전문 모델을 초월하는 결과를 보여줍니다.\nDINO 계열 모델과 자기 지도 학습(SSL)의 흐름\nDINO는 2021년 공개된 첫 버전부터 Vision Transformer(ViT)를 기반으로, 학습자(학생)-참조자(교사)의 지식 증류(distillation) 메커니즘을 활용해 자기 지도 학습을 구현해왔습니다. DINOv2에서는 이미지 표현 학습을 보다 정제된 이미지 셋에서 수행하면서 범용성과 정확도를 개선했고, DINOv3는 이를 계승하여 더욱 큰 스케일과 강력한 백본으로 성능을 극대화한 버전입니다.\nDINOv2도 자기지도학습 기반으로 높은 성능을 보여줬지만, DINOv3는 이를 한층 발전시켜 더 큰 모델 규모(7B 파라미터)와 12배 더 큰 학습 데이터셋을 사용했습니다. 성능 면에서도, 특히 시맨틱 분할, 객체 탐지, 깊이 추정 같은 밀집 예측 작업에서 DINOv2 대비 월등히 향상되었습니다.\n또한 DINOv3는 CLIP 기반 모델(SigLIP 2, Perception Encoder 등)과 비교 시 다양한 이미지 분류 벤치마크에서 동등하거나 그 이상을 기록하며, 밀집 예측에서는 압도적인 우위를 보입니다. DINOv3는 라벨 없는 상태에서 학습했음에도, 라벨이 있는 약지도(weakly supervised) 모델보다 성능이 더 높다는 점이 특징입니다.\nDINO 계열 모델들의 핵심은, 텍스트나 외부 라벨 없이 이미지 자체만으로도 강력한 표현을 학습할 수 있다는 점입니다. CLIP과 같은 텍스트-이미지 쌍 기반 모델과는 달리, DINO는 오직 이미지 간 유사도와 구조만으로 시각적 의미를 파악하고 학습합니다. 이를 위해서는 모델 아키텍처와 학습 기법 모두에서 섬세한 설계가 요구됩니다.\nDINOv3 아키텍처 구성\nDINOv3는 Vision Transformer(ViT)와 ConvNeXt 두 가지 백본 구조를 지원하며, 그 중에서도 ViT 기반이 주류입니다. 특히 ViT-g/14 모델은 약 70억 개의 파라미터를 가지며, 이는 ViT-H/14보다도 크고 CLIP ViT 모델을 능가하는 스케일입니다.\n교사-학생 구조\n학생(Student) 네트워크는 인코딩하고자 하는 입력 이미지를 다양한 증강 방식으로 변형된 뷰(views)로 처리합니다.\n교사(Teacher) 네트워크는 EMA(Exponential Moving Average)를 통해 학생 네트워크의 가중치로부터 업데이트되며, 직접 학습되지 않습니다.\nLoss는 서로 다른 뷰들 간의 예측 일관성을 최대화하도록 설계됩니다. 이는 서로 다른 증강 이미지를 같은 것으로 인식하도록 모델이 스스로 학습하게 합니다.\n입력 증강 및 멀티크롭 전략\n입력 이미지는 다양한 해상도와 위치로 잘라진 여러 crop(예: 224×224, 96×96)으로 생성됩니다.\n교사는 대형 crop만 사용하고, 학생은 대형 및 소형 crop을 함께 사용하여 더욱 강건한 표현을 유도합니다.\n이러한 멀티크롭(multicropping)은 모델이 다양한 공간 및 시각 스케일에서 일관된 표현을 학습하도록 도와줍니다.\nMLP Head\n각 백본 출력 후에는 MLP 헤드가 붙어, 최종적으로 표현(feature embedding)을 생성합니다.\n이 표현은 downstream task (분류, 분할 등)에 그대로 활용될 수 있습니다.\nMLP 헤드는 교사-학생 간 출력을 정렬시키는 핵심 위치입니다.\n학습 방식과 최적화 전략\n대규모 비지도 학습\nDINOv3는 17억 개의 웹 이미지로 학습되었습니다. 이 이미지들은 라벨이 없으며, CLIP의 사전학습 이미지보다 4배 많고 품질도 뛰어납니다. 또한 단일 GPU로는 학습이 불가능하며, 대규모 클러스터에서 수 주간 학습됩니다.\n성능을 끌어올린 최적화 기법\nSharpness-Aware Minimization(SAM) 기법이 적용되어, 일반적인 SGD나 Adam보다 일반화 성능이 뛰어납니다.\nEMA decay 파라미터 조정으로 학생-교사 간 동기화를 세밀히 조절합니다.\nCosine similarity loss 및 centering 기법을 통해 출력의 다양성과 안정성을 동시에 확보합니다.\nDINOv3의 주요 특징\n자기지도학습(SSL)과 대규모 학습: DINOv3의 핵심은 대규모 라벨 없는 데이터로부터 범용 시각 표현을 학습하는 능력입니다. 이를 통해 위성 이미지, 의료 영상, 산업용 데이터 등 라벨링이 어려운 데이터셋에서도 고성능 모델을 만들 수 있습니다. 예를 들어, 위성 이미지 기반 숲 캐노피 높이 추정에서 DINOv2 대비 오차를 4.1m → 1.2m로 줄였습니다.\n파인튜닝 없이도 최첨단 성능: DINOv3는 백본 가중치를 고정(frozen)한 상태에서도 객체 탐지, 시맨틱 분할, 깊이 추정 같은 핵심 비전 작업에서 최첨단 성능을 기록합니다. 이는 다양한 애플리케이션에서 동일한 백본을 공유할 수 있어, 엣지 디바이스나 멀티태스크 환경에서 연산 효율성을 극대화합니다.\n다양한 모델 크기 제공: Meta는 7B 모델뿐만 아니라, 경량화된 ViT-B, ViT-L, ConvNeXt(T/S/B/L) 구조로도 DINOv3를 distillation하여 공개했습니다. 이를 통해 모바일·임베디드 환경 등 자원 제약이 있는 환경에서도 활용할 수 있습니다.\n실제 활용 사례\n세계자원연구소(WRI): 위성 이미지로 산림 훼손과 토지 이용 변화를 감지, 탄소 복원 프로젝트의 검증 및 자금 지원을 효율화합니다. (더 자세히 보기)\nNASA JPL: DINO 백본을 활용한 화성 탐사 로봇의 멀티태스크 비전 인식에 활용합니다. (더 자세히 보기)\nOrakl Oncology: DINO를 장기유사체 이미지로 사전 훈련시켜, 암 치료에 대한 환자 반응 예측을 지원하는 기반 모델을 생성합니다. (더 자세히 보기)\n라이선스\nDINOv3 모델은 Commercial License로 공개 및 배포되고 있습니다. 상업적 사용이 가능하지만, 세부 조건은 라이선스 문서를 반드시 확인해야 합니다.\nDINOv3 공식 홈페이지\nAI at Meta\nDINOv3\nDINOv3 scales self-supervised learning (SSL) for images to produce our strongest universal vision backbones, enabling breakthrough performance across diverse domains.\nMeta AI의 DINOv3 공개 블로그\nMeta AI\nDINOv3: Self-supervised learning for vision at unprecedented scale\nDINOv3 scales self-supervised learning for images to create universal vision backbones that achieve absolute state-of-the-art performance across diverse domains, including web and satellite imagery.\nDINOv3 논문\narXiv.org\nDINOv3\nSelf-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training...\nDINOv3 GitHub 저장소\ngithub.com\nGitHub - facebookresearch/dinov3: Reference PyTorch implementation and models for...\nReference PyTorch implementation and models for DINOv3\nDINOv3 모델 다운로드\nhuggingface.co\nDINOv3 - a facebook Collection\nDINOv3: foundation models producing excellent dense features, outperforming SotA w/o fine-tuning - https://arxiv.org/abs/2508.10104\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "date": "2025-08-16",
        "source": "파이토치 한국 사용자 모임",
        "category": "기술"
    },
    {
        "title": "ECA(Editor Code Assistant): 다양한 코드 편집기를 지원하는 오픈소스 AI Pair Programming 도구",
        "url": "https://discuss.pytorch.kr/t/eca-editor-code-assistant-ai-pair-programming/7491",
        "content": "ECA 소개\nECA(Editor Code Assistant)는 다양한 코드 편집기와 대형 언어 모델(LLM)을 연결해주는 오픈소스 툴로, AI 기반 페어 프로그래밍을 보다 간편하게 경험할 수 있도록 설계되었습니다. 이 프로젝트의 가장 큰 특징은 에디터에 종속되지 않는 통합 프로토콜을 제공한다는 점입니다. 이를 통해 Emacs, VSCode, Vim 등 다양한 편집기에서 동일한 환경과 기능을 구현할 수 있습니다. ECA는 LSP(Language Server Protocol)에서 영감을 받아 설계되었으며, 서버-클라이언트 구조를 기반으로 합니다.\nLLM 기술 발전이 가속화되면서 모델 간 성능 격차는 점차 줄어드는 반면, 개발자가 편리하게 코드를 작성하고 변경 사항을 계획할 수 있는 **사용자 경험(UX)**의 중요성은 더욱 커지고 있습니다. ECA는 바로 이 UX 향상을 위해, 편집기 개발자가 모델 연동과 같은 복잡한 구현 대신 UI/UX에 집중할 수 있는 환경을 제공합니다.\n또한, 단일 설정 파일을 통해 전역 혹은 로컬에서 동일한 환경을 구성할 수 있으며, OpenAI, Anthropic, Ollama와 같은 다양한 모델을 동시에 사용할 수 있습니다. 이를 통해 사용자는 프로젝트 요구사항과 개인 선호에 맞춰 유연하게 모델을 선택하고 조합할 수 있습니다.\nECA는 GitHub Copilot이나 Cursor 등 특정 에디터에 종속된 AI 코드 도우미와 달리, **편집기 비종속성(editor-agnostic)**을 핵심 가치로 삼습니다. 예를 들어, GitHub Copilot은 VSCode, JetBrains IDE 중심으로 동작하지만, ECA는 Emacs, VSCode, Vim뿐 아니라 앞으로 Intellij 등 다른 에디터로도 확장될 계획입니다. 또한, LSP처럼 표준화된 통신 프로토콜을 적용하여 신규 에디터 통합 시 최소한의 개발로 빠르게 연동할 수 있습니다.\n또한 ECA는 단순히 코드 자동완성에 그치지 않고, 채팅 기반 상호작용, 도구 호출 관리, 다중 모델 지원, 맥락 정보 제공 등 LLM 활용의 확장성을 극대화할 수 있는 기능을 제공합니다. 이 점에서 단일 모델 기반 도구보다 훨씬 유연하고 확장성 있는 아키텍처를 갖추고 있습니다.\nECA의 주요 기능\n편집기에 종속되지 않는 프로토콜(Editor-agnostic Protocol): ECA는 모든 편집기에서 동일한 UX를 제공하기 위해 자체 정의한 통신 프로토콜을 사용합니다. 이는 LSP와 유사하게 표준화된 방식으로 서버와 편집기가 stdin/stdout을 통해 데이터를 교환하도록 설계되었습니다. 덕분에 새로운 에디터가 추가되더라도 복잡한 수정 없이 쉽게 통합할 수 있습니다.\n단일 설정 관리(Single configuration): 사용자는 .eca/config.json 파일에 API 키와 모델 정보를 입력하여 환경을 설정할 수 있습니다. 이 파일은 프로젝트 루트나 전역 설정 경로에 위치할 수 있으며, OpenAI, Anthropic, Ollama, 그리고 사용자 정의 모델까지 모두 지원합니다. 설정은 다음과 같은 방식으로 구성됩니다:\n{\n  \"openaiApiKey\": \"your-openai-api-key-here\",\n  \"anthropicApiKey\": \"your-anthropic-api-key-here\"\n}\n채팅 기반 상호작용: ECA는 코드 작성 중 질문, 코드 리뷰, 리팩토링 제안 등을 채팅 형태로 지원합니다. 사용자는 마치 동료 개발자와 협업하듯 LLM과 대화하며 개발을 진행할 수 있습니다.\n다중 모델 및 컨텍스트 지원: ECA는 하나의 세션에서 여러 LLM 모델을 사용할 수 있으며, 프로젝트 코드나 MCP(Model Context Protocol) 리소스, 사용자 정의 프롬프트 등을 포함해 풍부한 컨텍스트를 LLM에 전달할 수 있습니다. 이를 통해 답변 품질과 코드 추천의 정확도를 높일 수 있습니다.\nECA 설치 및 시작\n편집기용 플러그인을 설치하면 ECA 서버가 자동으로 다운로드 및 실행됩니다.\nEmacs 플러그인\nVSCode 확장\nVim 플러그인\nIntellij 지원 예정\n.eca/config.json에 모델 설정을 추가합니다.\n편집기 내 채팅 인터페이스를 통해 바로 AI 코딩 지원을 시작할 수 있습니다.\n라이선스\nECA 프로젝트는 Apache License 2.0으로 공개 및 배포되고 있습니다. 상업적 사용에 제한이 없습니다.\nECA 프로젝트 공식 홈페이지\neca.dev\nOverview - ECA - Editor Code Assistant\nECA - AI pair programming capabilities in any editor\nECA 프로젝트 GitHub 저장소\ngithub.com\nGitHub - editor-code-assistant/eca: Editor Code Assistant (ECA) - AI pair programming...\nEditor Code Assistant (ECA) - AI pair programming capabilities agnostic of editor\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "date": "2025-08-15",
        "source": "파이토치 한국 사용자 모임",
        "category": "기술"
    },
    {
        "title": "Anthropic의 Claude 4 Sonnet 모델이 1백만 토큰(1M tokens)의 컨텍스트 지원",
        "url": "https://discuss.pytorch.kr/t/anthropic-claude-4-sonnet-1-1m-tokens/7489",
        "content": "Claude Sonnet 4의 1M Context Window 소개\nAnthropic이 자사의 언어 모델 Claude Sonnet 4에 1백만 토큰 규모의 콘텍스트 윈도우를 도입했습니다. 이는 이전 대비 5배 이상 향상된 수치로, 이제 개발자나 연구자들은 수십 개의 논문, 대규모 코드베이스 전체, 방대한 문서 집합을 한 번의 요청으로 처리할 수 있습니다. 이러한 기능 확장은 자연어 처리(NLP) 및 AI 모델 활용에 있어 매우 실질적인 전환점을 의미합니다.\n기존의 대부분 언어 모델은 수만 개 수준의 토큰 콘텍스트만 지원했기 때문에, 대규모 데이터를 다루는 데 있어 제한적이었습니다. 특히 긴 코드를 다루거나 문서 간의 상호 연관성을 파악해야 할 때, 콘텍스트 제한으로 인한 정보 손실이나 문맥 부재 문제가 자주 발생했습니다. 하지만 1백만 토큰 지원은 이러한 한계를 근본적으로 해소하며, AI 모델이 전반적인 시스템 구조를 보다 깊이 이해하도록 돕습니다.\n이번 발표는 Anthropic API 및 Amazon Bedrock을 통해 퍼블릭 베타로 제공되고 있으며, 곧 Google Cloud의 Vertex AI에서도 사용할 수 있도록 확장될 예정입니다. 모델의 콘텍스트 한계가 늘어나면서, 보다 현실적인 엔지니어링 워크플로우가 가능해졌고, 이를 활용한 다양한 사례도 함께 소개되었습니다.\n확장된 Context Window로는 무엇을 할 수 있나요?\n대규모 코드 분석: 1백만 토큰 지원은 전체 소스코드, 테스트 파일, 문서 등을 한 번에 로딩하고 분석하는 데 유리합니다. Claude는 코드 간의 의존 관계를 파악하고 프로젝트 전체의 아키텍처를 이해한 상태에서 개선 방향을 제안할 수 있습니다. 예전에는 파일별로 분할하여 처리하고, 모델에게 반복적으로 맥락을 제공해야 했다면 이제는 이 과정을 대폭 줄일 수 있습니다.\n문서 통합 및 요약: 수백 개에 달하는 법률 문서, 연구 논문, 기술 명세서도 한 번의 요청으로 통합 분석이 가능합니다. Claude는 이들 문서 간의 연관성과 흐름을 유지한 채, 보다 일관된 해석과 분석을 제공할 수 있습니다. 복잡한 문서 간의 연관성 분석이 필요한 분야에서 매우 유용하게 사용될 수 있습니다.\n콘텍스트를 유지하는 에이전트 구축: 에이전트 기반 시스템에서는 수많은 도구(tool) 호출과 멀티스텝 워크플로우가 필요한데, 이때 이전 대화 내역과 API 문서, 도구 정의 등을 모두 기억하는 것이 중요합니다. 1백만 토큰 콘텍스트는 이 모든 정보를 한 번에 포함하여, 더욱 일관되고 지능적인 대화를 유지할 수 있게 해줍니다.\n가격 정책 및 최적화 팁\n1백만 토큰 처리에는 많은 계산 자원이 필요하기 때문에, 200K 토큰을 초과한 요청부터는 가격이 상승합니다. 예를 들어, 200K 이하 요청은 입력 기준 1백만 토큰(1MTok)당 $3, 출력을 기준으로는 $15이며, 200K 초과 시에는 각각 $6, $22.5로 책정됩니다.\n하지만 프롬프트 캐싱이나 배치 처리를 활용하면 비용과 지연시간을 절감할 수 있습니다. 특히 배치 처리 시 최대 50%까지 절감 효과를 얻을 수 있어, 대규모 작업에서 효율적인 전략이 될 수 있습니다.\n고객 사례 소개\nBolt.new: 웹 기반 개발 플랫폼 Bolt.new는 Claude Sonnet 4를 코드 생성 워크플로우에 적극 활용하고 있습니다. CEO인 Eric Simons는 “1M 콘텍스트 윈도우 덕분에 훨씬 더 큰 프로젝트를 정확하게 처리할 수 있게 되었다”며 실사용 성과를 강조합니다. Claude는 생산 환경에서 여타 모델보다 높은 정확도를 보이고 있다고 합니다.\niGent AI: 런던의 iGent AI는 ‘Maestro’라는 AI 코딩 파트너를 통해 실제 대규모 코드베이스를 며칠간 지속적으로 다루는 기능을 구현했습니다. 이들은 Claude Sonnet 4가 “자율적인 소프트웨어 엔지니어링의 새로운 패러다임”을 가능하게 한다고 평가하고 있습니다.\nClaude Sonnet 4 1M 콘텍스트 발표 블로그\nanthropic.com\nClaude Sonnet 4 now supports 1M tokens of context\nClaude Sonnet 4 now supports up to 1 million tokens of context on the Anthropic API—a 5x increase.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "date": "2025-08-15",
        "source": "파이토치 한국 사용자 모임",
        "category": "기술"
    }
]
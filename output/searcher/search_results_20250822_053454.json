[
    {
        "title": "mobile-use: 자연어로 Android 및 iOS 단말기를 조작하는 오픈소스 AI Agent",
        "content": "mobile-use 소개\nmobile-use는 안드로이드와 iOS 기기를 자연어 명령으로 제어할 수 있게 해주는 오픈소스 AI 에이전트 프로젝트입니다. 단순한 매크로나 스크립트 실행이 아닌, 실제 사람이 스마트폰을 조작하듯이 UI를 인식하고, 앱을 탐색하며, 데이터를 추출하는 것이 가능합니다. 예를 들어 “Gmail을 열고 읽지 않은 메일 3개의 발신자와 제목을 알려줘”라는 명령을 입력하면, 모바일 기기에서 해당 작업을 수행한 뒤 JSON 형태로 결과를 반환해줍니다.\nmobile-use는 샌프란시스코와 파리에 거점을 둔 AI 연구소 minitap에서 개발 중이며, 현재 활발히 개선되고 있습니다. 특히 AndroidWorld라는 국제 벤치마크에서 오픈소스 부문 1위를 차지하면서 그 성능을 입증했습니다. 단순한 실험용이 아니라 실제 스마트폰을 대상으로 한 강력한 자동화 도구라는 점에서 주목할 만합니다. mobile-use 프로젝트의 AndroidWorld Benchmark Results 관련 내용을 더 자세히 보고 싶으시다면, 여기를 눌러 문서를 확인해주세요.\n오늘날 스마트폰은 개인과 업무 생활에서 핵심적인 도구로 자리 잡았지만, 많은 반복 작업이나 앱 간 전환이 필요할 때 생산성을 떨어뜨리는 요인이 되곤 합니다. mobile-use는 이러한 문제를 해결하기 위해 등장했으며, AI와 LLM 기반의 자연어 처리 기술을 활용해 사용자의 요청을 스마트폰 자동화 작업으로 변환합니다. 따라서 개발자뿐 아니라 비개발자도 직관적으로 활용할 수 있는 점이 큰 장점입니다.\n지금까지 모바일 자동화는 주로 Appium, UIAutomator, XCUITest 같은 테스트 프레임워크를 통해 이루어졌습니다. 하지만 이러한 도구들은 설정이 복잡하고 코드 작성이 필요하며, 특정 시나리오에 맞게 커스터마이징해야 하는 단점이 있습니다. 반면 mobile-use는 자연어 기반 접근 방식을 택해 “설정을 켜줘”, “카카오톡에서 마지막 대화 상대에게 메시지 보내”와 같은 명령으로 즉시 동작할 수 있습니다.\n또한, 기존 프레임워크는 주로 앱 테스트를 목적으로 설계된 반면, mobile-use는 범용적인 스마트폰 자동화를 겨냥합니다. 이를 통해 개인 비서, 데이터 수집, 앱 간 워크플로우 실행 등 다양한 활용 가능성을 보여줍니다.\nmobile-use의 주요 기능\n자연어 제어: mobile-use의 핵심은 LLM을 활용한 자연어 처리입니다. 사용자는 영어뿐 아니라 모국어로 명령을 내릴 수 있으며, AI 에이전트가 이를 해석해 실제 스마트폰 조작으로 변환합니다.\nUI 기반 자동화: 단순히 앱을 실행하는 수준이 아니라 UI 컴포넌트를 인식하여 앱 내에서 버튼을 누르거나 스크롤하는 등의 세밀한 동작을 수행할 수 있습니다.\n데이터 추출: 앱에서 필요한 데이터를 직접 읽어 JSON 같은 구조화된 형식으로 반환합니다. 예를 들어, “네이버 뉴스 앱에서 첫 페이지 기사 제목을 JSON으로 뽑아줘” 같은 요청이 가능합니다.\n확장성과 유연성: 사용자가 원하는 LLM 모델(OpenAI, Anthropic, HuggingFace 등)을 설정 파일에서 교체해 사용할 수 있도록 설계되었습니다.\nmobile-use의 설치 및 실행 방법\nmobile-use는 두 가지 방식으로 실행할 수 있습니다.\nDocker 기반 빠른 실행\nAndroid 기기 또는 에뮬레이터 연결\nDocker 설치 후 스크립트 실행\n예시:\n./mobile-use.sh \"Open Gmail, find first 3 unread emails, and list their sender and subject line\" --output-description \"A JSON list of objects, each with 'sender' and 'subject' keys\"\n개발자 모드(수동 실행)\nPython 환경 세팅 (uv 패키지 관리자 사용)\nAndroid의 경우 ADB 설치 필요, iOS의 경우 Xcode 필요\nMaestro 프레임워크를 활용하여 기기와 상호작용\n사용 예시\n배터리 상태 확인\npython ./src/mobile_use/main.py \"Go to settings and tell me my current battery level\"\nGmail 읽지 않은 메일 목록 추출\npython ./src/mobile_use/main.py \\\n\"Open Gmail, find all unread emails, and list their sender and subject line\" \\\n--output-description \"A JSON list of objects, each with 'sender' and 'subject' keys\"\n라이선스\nmobile-use 프로젝트는 MIT License로 공개 및 배포되고 있습니다. 상업적 사용에 제한이 없으며, 누구나 자유롭게 수정 및 재배포할 수 있습니다.\nmobile-use를 개발한 minitap 홈페이지\nminitap.ai\nminitap | mobile ai research lab - autonomous device control\nMinitap is an AI research lab in San Francisco and Paris focusing on building models for AI to device interaction.\nmobile-use 프로젝트 GitHub 저장소\ngithub.com\nGitHub - minitap-ai/mobile-use: AI agents can now use real Android and iOS apps,...\nAI agents can now use real Android and iOS apps, just like a human.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/mobile-use-android-ios-ai-agent/7535",
        "date": "2025-08-21T19:00:42.767000",
        "source": "pytorch_kr"
    },
    {
        "title": "Lemonade: 로컬 GPU 및 NPU에서 구동되는 오픈소스 / 고성능 LLM 추론 서버 (feat. AMD)",
        "content": "Lemonade 소개\nLemonade는 AMD의 후원으로 개발된 로컬 환경에서 대규모 언어 모델(LLM)을 빠르고 손쉽게 실행할 수 있는 오픈소스 프로젝트입니다. 기존에는 클라우드 기반의 LLM 서비스에 의존해야 했지만, Lemonade를 활용하면 개인 PC, 특히 GPU 또는 NPU가 탑재된 장비에서 직접 LLM을 실행할 수 있습니다. 이는 데이터 프라이버시, 성능, 비용 측면에서 상당한 장점을 제공합니다.\n특히 Lemonade는 단순히 모델 실행만 지원하는 것이 아니라, 다양한 애플리케이션과 연동할 수 있는 구조를 제공하고 있습니다. 사용자는 Open WebUI, Continue, AnythingLLM, AI Dev Gallery, LM-Eval, CodeGPT, AI Toolkit 등과 같은 주요 오픈소스 앱과 쉽게 통합할 수 있으며, 이를 통해 로컬 AI 개발 환경을 빠르게 구축할 수 있습니다.\nIntroducing Lemonade Server: Local LLM Serving with GPU and NPU Acceleration\n스타트업(Styrk AI), 학계(Stanford Hazy Research), 대기업(AMD) 등 다양한 조직들이 Lemonade를 활용하고 있으며, 특히 AMD의 Ryzen AI 하드웨어와 결합해 최적화된 성능을 보여주고 있습니다. 개발자 입장에서는 설치 과정이 간단하고, OpenAI API 호환성을 제공하여 기존 앱이나 코드와 쉽게 통합할 수 있다는 점이 큰 장점입니다. 개발자 입장에서는 Lemonade를 활용하며 클라우드 기반 API 호출에 의존하지 않고도 실험적이거나 개인화된 AI 모델을 활용할 수 있기 때문에, 데이터 보안과 비용 절감뿐 아니라 독립적인 AI 서비스 개발이 가능해집니다.\n다른 Local LLM 추론 서버와의 비교\nLemonade와 유사한 프로젝트로는 Ollama, LM Studio 등이 있습니다. Ollama와 LM Studio는 로컬 환경에서 LLM 실행을 지원하지만, Lemonade는 GPU뿐 아니라 NPU 지원까지 고려해 더 넓은 하드웨어 호환성을 제공합니다. 또한 Lemonade는 단순 실행기를 넘어 여러 오픈소스 앱과 통합되는 생태계를 적극적으로 구축하고 있다는 점에서 차별화됩니다.\n예를 들어, Ollama는 모델 관리와 실행에 초점을 맞추고 있지만, Lemonade는 Open WebUI 같은 인터페이스와 Continue 같은 개발 워크플로우 툴을 포함하여 더 확장된 개발 경험을 제공합니다. 따라서 로컬 LLM 생태계의 확장성을 중시하는 개발자라면 Lemonade가 더 적합할 수 있습니다.\nLemonade Server의 주요 기능과 특징\n간단한 설치 및 실행\nLemonade는 GUI 기반 설치 프로그램(Windows), pip 설치, 혹은 소스 코드 설치를 모두 지원합니다. 설치 후에는 내장된 모델 관리자를 통해 원하는 모델을 다운로드하고 실행할 수 있으며, 기본적으로 채팅 인터페이스도 포함되어 있어 바로 대화형 테스트가 가능합니다. GPU 및 NPU 최적화를 통해 빠른 속도로 모델을 구동할 수 있으며, 클라우드 비용이나 인터넷 연결 문제에 구애받지 않는다는 점이 큰 장점입니다.\nLemonade는 Windows 11용 설치 파일을 제공하며, Ubuntu와 Windows 환경에서 모두 동작합니다. 설치 후에는 명령어를 사용하여 몇 분 만에 LLM 서버를 실행하고 다양한 애플리케이션과 연동할 수 있습니다. 예를 들어, 다음과 같은 명령어로 Gemma 3 모델을 실행할 수 있습니다:\nlemonade-server run Gemma-3-4b-it-GGUF\nrun 명령어 외에도 사전 모델 다운로드(pull)나 모델 목록 조회(list) 등의 명령어도 사용 가능하며, 특정 백엔드(Vulkan, ROCm)를 직접 지정할 수 있어 개발자가 성능을 세밀하게 조정할 수 있습니다. 지원하는 전체 모델 목록은 공식 홈페이지에서 확인하실 수 있습니다.\n다양한 앱 통합 지원\nOpen WebUI: 직관적인 사용자 인터페이스 제공\nContinue: VS Code 기반 AI 개발 워크플로우 지원\nAnythingLLM: 다목적 LLM 애플리케이션 실행 플랫폼\nGaia: AMD 기반 AI 생태계 지원\nAI Dev Gallery: 다양한 AI 실험 환경 제공\nLM-Eval: 모델 성능 평가 및 벤치마크\nCodeGPT: 코드 중심 LLM 활용\nAI Toolkit: 개발자 중심의 AI 유틸리티 모음\n또한, Lemonade는 OpenAI API와 완벽히 호환되는 API를 제공합니다. 즉, 기존에 openai 패키지를 사용해 GPT 모델을 호출하던 코드에서, Endpoint(base_url)만 Lemonade의 URL(예: http://localhost:8000/api/v1)로 바꾸면 곧바로 Lemonade 로컬 모델을 활용할 수 있습니다.\n아래는 OpenAI SDK를 사용하는 Python 예시 코드에서 Lemonade를 사용하도록 base_url을 지정하는 방법입니다:\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\"   # Lemonade의 API 주소 적용\n)\n\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",\n    messages=[{\"role\": \"user\", \"content\": \"프랑스의 수도는 어디인가?\"}]\n)\n\nprint(completion.choices[0].message.content)\n즉, 다양한 언어에서 지원하는 OpenAI 및 OpenAI 호환 SDK들을 활용할 수 있습니다:\nPython C++ Java C# Node.js Go Ruby Rust PHP\nopenai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php\n모델 라이브러리 및 호환성\nGGUF와 ONNX 모델을 모두 지원합니다.\nHugging Face에서 직접 모델을 가져와 사용할 수 있으며, 자체 제공되는 모델도 풍부합니다.\n사용자는 로컬 실행 환경에서 다양한 추론 엔진(OGA, llamacpp, Hugging Face)을 선택할 수 있습니다.\n지원 하드웨어 및 구성\nCPU: 모든 플랫폼에서 지원\nGPU: Vulkan 및 ROCm 엔진 지원 (AMD Radeon 시리즈 최적화 포함)\nNPU: AMD Ryzen AI 300 시리즈 지원\n특히 ROCm 지원을 통해 최신 AMD GPU 환경에서 높은 성능을 기대할 수 있습니다.\n확장된 SDK 기능\nLemonade API: Python 애플리케이션에 직접 모델을 임베딩할 수 있는 고수준 API\nLemonade CLI: 모델 정확도 테스트, 성능 벤치마크, 메모리 프로파일링 등 개발자를 위한 고급 도구 제공\n위와 같은 기능들을 통해 단순히 LLM을 실행하는 것을 넘어, 모델 연구 및 애플리케이션 최적화에도 활용할 수 있습니다.\n커뮤니티와 생태계\nLemonade는 GitHub 저장소와 Discord 커뮤니티를 통해 개발자들과 소통하고 있으며, 최신 릴리스와 뉴스를 빠르게 공유하고 있습니다. GitHub 저장소에 이슈를 남기거나 이메일을 통해 메인테이너들과 소통할 수도 있습니다.\n라이선스\nLemonade 프로젝트는 Apache-2.0 라이선스로 공개되어 있습니다. 상업적인 목적으로 사용이 가능하며, 자유롭게 소스 코드를 수정할 수 있습니다. 단, 저작권 및 라이선스 고지 유지, 그리고 특허 관련 조건 준수가 필요합니다.\nLemonade 공식 홈페이지\nlemonade-server.ai\nLemonade Server\nLemonade 공식 문서\nlemonade-server.ai\nLemonade Server\nLemonade Server GitHub 저장소\ngithub.com\nGitHub - lemonade-sdk/lemonade: Lemonade helps users run local LLMs with the...\nLemonade helps users run local LLMs with the highest performance by configuring state-of-the-art inference engines for their NPUs and GPUs. Join our discord: https://discord.gg/5xXzkMu8Zk\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/lemonade-gpu-npu-llm-feat-amd/7522",
        "date": "2025-08-20T12:30:27.695000",
        "source": "pytorch_kr"
    },
    {
        "title": "OpenAI, 해커톤에서 탄생한 실험적인 Hardware AI Assistant 프로젝트 ‘Reflect’ 공개",
        "content": "OpenAI의 Reflect 프로젝트 소개\nReflect는 OpenAI 해커톤에서 실험적으로 만들어진 하드웨어 기반 AI 어시스턴트 프로젝트입니다. 이 프로젝트는 정식 제품이나 안정화된 SDK라기보다는, 새로운 아이디어를 빠르게 시도해보기 위한 프로토타입에 가깝습니다. 특히 ESP32 계열의 마이크로컨트롤러를 중심으로 설계되었으며, 화면보다는 소리·빛·색상 같은 직관적인 인터페이스를 활용해 사람과 자연스럽게 소통할 수 있도록 고안되었습니다.\nReflect - The OpenAI Physical Assistant that Illuminates your life\n이 프로젝트의 특징은 ‘가볍고 수정하기 쉬운 AI 어시스턴트’라는 점에 있습니다. Reflect는 사용자의 스마트폰을 핵심 키(key)로 활용하며, 디바이스 자체에는 개인 데이터를 저장하지 않습니다. 즉, 보안과 확장성 측면에서 간단하면서도 유연하게 구성할 수 있습니다. 이러한 접근 방식은 AI를 활용한 새로운 인터랙션 실험을 하고 싶은 개발자나 하드웨어 메이커들에게 매력적인 출발점이 될 수 있습니다.\n주의: reflect 프로젝트는 OpenAI 해커톤에서 시범적으로 제작된 데모 성격의 프로젝트입니다. reflect 저장소의 코드는 학습과 실험을 목적으로 공개된 것으로, OpenAI는 공개한 코드의 정확성, 완전성, 성능 등에 대한 어떠한 보증도 하지 않습니다. 특히, 보안 취약점이나 신뢰성, 안정성 등에 대한 충분한 검토를 거치지 않았으므로 실제 운영 환경에는 적합하지 않으며, 상업적 서비스나 민감한 데이터 처리, 운영 환경에는 적합하지 않습니다. reflect 활용 및 실행 과정에서 발생하는 모든 문제, 손실, 손해는 전적으로 사용자의 책임이므로, 연구나 학습, 프로토타이핑 등의 목적으로만 활용하는 것을 권장합니다.\nReflect는 일정 관리, 학습 보조, 위치 기반 행동, 음악 재생 등 다양한 기능을 제안하고 있으며, 이를 통해 ‘화면 없는 AI 어시스턴트’라는 새로운 가능성을 탐구합니다. 프로젝트 자체는 해커톤 산출물이므로 완성도가 높지는 않지만, 직접 수정하거나 새로운 기능을 시도해볼 수 있도록 공개되어 있어 오픈소스 커뮤니티의 창의적인 활용이 기대됩니다.\n아마존 Alexa, 구글 Nest Hub, 애플 HomePod 같은 기존 AI 스피커 제품과 Reflect의 가장 큰 차별점은 ‘화면 없는 경험’을 지향한다는 것입니다. 일반적인 AI 스피커들은 음성 응답 위주로 작동하지만, 대부분 클라우드와 밀접히 연결되어 있고 때로는 디스플레이와 연동되기도 합니다. 반면 Reflect는 소리와 조명 같은 직관적인 피드백을 중시하며, 사용자의 휴대폰을 핵심 허브로 삼습니다.\n또 하나의 차별점은 확장성과 수정 가능성입니다. 상용 제품들은 폐쇄적인 생태계 속에서 제공되는 기능만 사용할 수 있지만, Reflect는 오픈소스 프로젝트로 누구나 새로운 기능을 추가하거나 디바이스를 개조할 수 있습니다. 가격 접근성 또한 중요한 요소인데, Reflect는 가능한 많은 사람들이 쉽게 접근할 수 있도록 저비용 하드웨어 환경을 고려하고 있습니다.\nReflect 주요 특징 및 기능\nReflect의 주요 아이디어\nReflect는 단순한 AI 스피커라기보다, 여러 가지 실험적인 아이디어가 결합된 어시스턴트입니다:\n자연스러운 상호작용: 화면 대신 빛·소리·색상을 이용해 사용자와 교감합니다.\n스마트폰 중심 구조: 기기 자체에 데이터가 남지 않고, 모든 개인 정보는 스마트폰에 저장됩니다.\n일정 기반 기능: “어제 있었던 일정 알려줘” 같은 요청을 할 수 있습니다.\n미래 준비: “내일 시험이 있는데, 공부할까?” 같은 맥락 이해형 제안을 할 수 있습니다.\n몰입 지원: 공부할 때 음악을 틀어주거나 간단한 질문에 답변하는 등 ‘플로우 유지’를 돕습니다.\n위치 인식: 주방, 사무실 등 위치에 따라 다른 행동을 수행합니다.\n개조 용이성: 개발자가 직접 기능을 수정하거나 추가하기 쉽게 설계되었습니다.\nReflect의 지원 디바이스\nReflect는 현재 다음 하드웨어를 지원합니다:\n마이크로컨트롤러: M5Stack CoreS3 ESP32S3 IoT Development Kit\n스마트 조명: LIFX Color A19\nReflect 설치 방법\nReflect를 직접 실행하기 위해서는 ESP-IDF 환경을 먼저 구축해야 합니다. 다음과 같이 설치할 수 있습니다:\ngit clone -b v5.5 --recursive https://github.com/espressif/esp-idf.git\ncd esp-idf\n./install.sh esp32s3\n. ~/esp-idf/export.sh\nESP-IDF 환경을 구축한 뒤에는, 아래 명령어로 코드를 설치할 수 있습니다:\n. ~/esp-idf/export.sh\nidf.py flash\n설치 후 기기는 reflect라는 이름의 WiFi AP를 생성합니다. 해당 AP에 접속 후, 브라우저에서 http://192.168.4.1 에 접속하면 세션을 시작할 수 있습니다. 이 과정에서 실시간 오디오 API와 디바이스 오디오 스트림을 확인할 수 있으며, 디버깅 목적으로 오디오 요소를 직접 활성화할 수도 있습니다.\n라이선스\nOpenAI의 refelct 프로젝트는 MIT 라이선스로 공개 및 배포되고 있습니다. 따라서 상업적 이용이 가능하지만, 보증이나 안정성을 보장하지 않으며 모든 책임은 사용자에게 있습니다.\nReflect 프로젝트 GitHub 저장소\ngithub.com\nGitHub - openai/openai-reflect: Physical AI Assistant that illuminates your life\nPhysical AI Assistant that illuminates your life\n더 읽어보기\nESP-IDF 공식 문서\nM5Stack CoreS3 소개\nLIFX 스마트 조명 개요\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/openai-hardware-ai-assistant-reflect/7521",
        "date": "2025-08-20T06:00:11.824000",
        "source": "pytorch_kr"
    },
    {
        "title": "Llama-Scan: ollama의 멀티모달 모델을 활용하여 로컬에서 PDF를 텍스트로 변환하는 도구",
        "content": "llama-scan 소개\nLlama-Scan은 PDF 파일을 손쉽게 텍스트 파일로 변환해 주는 도구로, 특히 Ollama의 최신 멀티모달 모델을 활용하여 이미지나 다이어그램까지 상세한 텍스트 설명으로 바꿀 수 있습니다. 단순히 텍스트 추출에 그치는 것이 아니라, 그림과 시각적 요소까지 자연어로 변환한다는 점이 특징입니다. 기존의 PDF 텍스트 추출 도구들이 이미지 기반 문서에서 한계를 드러낸 것과 달리, Llama-Scan은 최신 AI 모델을 이용해 이러한 문제를 보완하고 있습니다.\n또한 Llama-Scan은 로컬 환경에서 실행되므로 별도의 API 호출이나 토큰 비용이 발생하지 않습니다. 이는 대규모 PDF 문서를 처리해야 하는 개발자나 연구자들에게 매우 중요한 장점으로, 데이터 프라이버시 측면에서도 안심할 수 있습니다. Python 환경만 갖추고 Ollama를 설치하면 바로 사용할 수 있어 접근성 또한 뛰어납니다.\n최근 논문, 기술 문서, 학술 자료들이 이미지와 그래프 중심으로 작성되는 경우가 많은데, Llama-Scan을 활용하면 이러한 자료를 텍스트 중심의 데이터셋으로 가공할 수 있습니다. 이는 연구자뿐 아니라, AI 학습 데이터 전처리 과정에도 큰 도움이 될 것으로 기대합니다.\n기존 PDF 추출 도구와의 비교\n대표적인 PDF 텍스트 추출 도구로는 PyPDF2와 같이 단순히 문서 내의 텍스트를 파싱하여 출력하는 방식의 도구들이 대부분이었습니다. 이미지 기반 문서에서는 거의 작동하지 않거나 OCR(광학 문자 인식) 같은 별도 도구를 병행해야 했습니다.\n반면 Llama-Scan은 Ollama의 멀티모달 모델을 활용하여 이미지 자체를 이해하고 설명으로 변환할 수 있기 때문에, 기존 도구들보다 훨씬 풍부한 데이터를 얻을 수 있습니다. 특히 논문에 포함된 수식, 다이어그램, 표 등을 문맥 기반으로 해석하는 데 강점을 보입니다.\n따라서 텍스트만 있는 문서라면 기존 도구가 가볍고 빠를 수 있지만, 시각 자료가 포함된 PDF라면 Llama-Scan이 훨씬 유용한 선택지가 될 수 있습니다.\n그 외에도 Mistral OCR과 같이 VLM 모델을 사용하는 API들이 있지만, 유료이고 변환 처리가 로컬에서 되지 않은 점이 Llama-Scan과의 다른 점입니다.\nllama-scan의 주요 기능 및 사용법\n설치\nLlama-Scan을 설치하려면 Python 3.10 이상과 Ollama가 필요합니다.\nOllama를 설치한 뒤, 기본 모델을 가져와야 합니다.\nollama run qwen2.5vl:latest\n패키지는 pip 또는 uv를 통해 설치할 수 있습니다:\n# pip 사용 시\npip install llama-scan\n\n# 또는, uv 사용 시\nuv tool install llama-scan\n기본 사용법\n아래와 같은 명령어를 실행하면 해당 PDF가 텍스트 파일로 변환되어 output/ 디렉터리에 저장됩니다:\nllama-scan path/to/your/file.pdf\n다음의 옵션들을 사용할 수 있습니다:\n--output, -o: 결과 저장 디렉터리 (기본값: “output”)\n--model, -m: 사용할 Ollama 모델 지정 (기본값: “qwen2.5vl:latest”)\n--keep-images, -k: 변환 과정에서 생성된 중간 이미지 보관 여부 (기본값: False)\n--width, -w: 이미지 리사이즈 너비 (0이면 리사이즈 생략, 기본값: 0)\n--start, -s: 시작 페이지 지정 (기본값: 0)\n--end, -e: 종료 페이지 지정 (기본값: 0, 즉 전체 페이지 처리)\n활용 예시\n다음은 document.pdf 파일로부터 1~5 페이지까지만 추출하고, 이미지를 가로 1000px로 리사이즈하는 예시입니다:\nllama-scan document.pdf --start 1 --end 5 --width 1000\n다음은 document.pdf 파일을 파싱할 때, 사용하려는 Ollama 모델을 지정하는 예시입니다:\nllama-scan document.pdf --model qwen2.5vl:3b\n라이선스\nllama-scan은 MIT 라이선스로 공개되어 있습니다. 상업적 사용이 가능합니다.\nllama-scan 프로젝트 GitHub 저장소\ngithub.com\nGitHub - ngafar/llama-scan: Transcribe PDFs with local LLMs\nTranscribe PDFs with local LLMs\n더 읽어보기\nOCR4all: 누구나 사용할 수 있는 무료 & 오픈소스 OCR 솔루션\nolmOCR, PDF 및 이미지를 효과적으로 분석하고 구조화하는 오픈소스 OCR 도구 (feat. Allen AI)\nDolphin: ByteDance가 공개한, 이종간 앵커 프롬프트 기반 문서 이미지 파싱 모델\nMinerU, PDF를 JSON/Markdown 변환 및 OCR 등을 지원하는 데이터 추출 도구 (feat. 한국어 지원)\nMistral OCR, Mistral이 공개한 문서 인식 API\nLumina.ai, 문서 처리 및 OCR가 가능한 오픈소스 프로젝트 Chunkr (및 API) 공개\nTarsier: LLM&LMM Agent를 위한 시각 도구 (🙈 Vision utilities for web interaction agents 🙈)\nMegaParse, PDF 및 오피스 문서 파싱이 가능한, 멀티모달 모델과의 통합을 지원하는 오픈소스 프로젝트\nScreenAI: UI와 시각적 언어 이해를 위한 시각-언어 모델(feat. Google)\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/llama-scan-ollama-pdf/7511",
        "date": "2025-08-19T12:30:50.651000",
        "source": "pytorch_kr"
    },
    {
        "title": "ccusage: Claude Code 사용량을 다각도로 분석하는 CLI 도구",
        "content": "ccusage 소개\n최근 AI 모델을 활용한 개발이 늘어남에 따라, 사용량 및 비용을 정밀하게 관리하는 도구에 대한 수요가 높아지고 있습니다. 특히 Anthropic의 Claude 모델, 특히 Claude Code를 사용하는 개발자라면, API 호출이 얼마나 이루어졌고 어떤 모델이 얼마나 비용을 유발했는지를 파악하는 것은 매우 중요한 관리 포인트입니다. ccusage는 바로 이런 요구를 충족시키기 위해 만들어진 경량화된 CLI 도구입니다. 사용자의 컴퓨터에 저장된 JSONL 로그 파일(~/.claude.json)을 기반으로 Claude API의 사용량과 비용을 빠르고 직관적으로 분석할 수 있도록 도와줍니다.\nccusage는 경량화된 바이너리를 제공하여 설치 없이도 바로 실행이 가능하며, 다양한 뷰(일별, 월별, 세션별, 프로젝트별 등)를 통해 데이터를 시각화합니다. 특히 실시간 모니터링 기능까지 제공되어, Claude API 사용량을 지속적으로 트래킹하고자 하는 개발자들에게 큰 도움을 줄 수 있습니다. 또한, 특정 프로젝트 단위로 분석하거나 모델별로 비용을 분류할 수 있는 기능도 있어 팀 단위의 비용 관리에도 적합합니다.\n이 도구는 번들 사이즈가 매우 작기 때문에, npx, bunx, deno를 통해 즉시 실행 가능하며, 특별한 환경 구성 없이도 간편하게 활용할 수 있습니다. Claude API를 자주 사용하는 사용자라면 반드시 눈여겨볼 만한 유틸리티입니다.\nccusage는 기존에 존재하던 로그 분석 도구들과 비교해도 매우 경량화되어 있으며, 실시간 대시보드나 모델별 비용 분해 기능과 같은 세부 기능들이 탑재되어 있다는 점이 큰 강점입니다. 예를 들어 OpenAI 관련 CLI 분석 도구들 중 일부는 로그 포맷 제한이 있거나, 실시간 기능이 없고 시각화가 부족한 경우가 많습니다.\nccusage는 Claude 모델 전용 분석에 초점을 맞추고 있으며, ClaudeLog와 같은 커뮤니티 도구들과도 연계되어 있어 Claude 사용 문화를 잘 반영하고 있습니다. 또한 MCP 서버 통합 기능을 통해 외부 툴과도 쉽게 연동될 수 있다는 점도 타 도구와 차별화되는 부분입니다.\nccusage의 주요 기능 및 사용 방법\n설치 및 실행\nccusage는 번들 크기가 매우 작아 설치 없이도 npx, bunx, 또는 deno를 이용해 곧바로 실행할 수 있습니다. 다음 명령어 중 하나를 실행하여 바로 실행해볼 수 있습니다:\n# bunx 사용 시 (권장)\nbunx ccusage\n\n# npx 사용 시\nnpx ccusage@latest\n\n# pnpm 사용 시\npnpm dlx ccusage\n\n# deno 사용 시\ndeno run -E -R=$HOME/.claude/projects/ -S=homedir -N='raw.githubusercontent.com:443' npm:ccusage@latest\n사용자의 컴퓨터에 바이너리를 설치하려면 다음과 같은 설치 명령어를 실행합니다:\n# npm 사용 시\nnpm install -g ccusage\n\n# bun 사용 시\nbun install -g ccusage\n\n# yarn 사용 시\nyarn global add ccusage\n\n# pnpm 사용 시\npnpm add -g ccusage\n설치 후에는 다음과 같은 명령어로 바로 실행해볼 수 있습니다:\nccusage daily\nccusage monthly --breakdown\nccusage blocks --live\n사용 예시\n다양한 명령어를 통해 로그를 시각화할 수 있습니다:\nccusage daily                # 일별 사용량 및 비용\nccusage monthly              # 월별 통계\nccusage session              # 세션별 통계\nccusage blocks               # 5시간 단위 청구 구간\nccusage blocks --live        # 실시간 대시보드\nccusage daily --since 20250801 --until 20250815 --breakdown\n또한 JSON 출력, 프로젝트 필터링, 시간대/로케일 설정 등의 다양한 옵션도 제공합니다.\n주요 기능\n실시간 모니터링: API 호출 상황을 실시간으로 추적 가능\n모델별 분석: Claude Opus, Sonnet 등 모델별 사용량/비용 확인\n다양한 출력 옵션: 컬러 테이블, JSON 포맷, 상태바 통합 등\n오프라인 모드: 캐시된 가격 데이터를 이용한 분석 가능\n프로젝트 기반 그룹화: 인스턴스 또는 프로젝트별 통계 지원\n초소형 번들: 설치 없이 빠르게 실행 가능\n라이선스\nccusage 프로젝트는 MIT 라이선스로 공개 및 배포되고 있습니다. 상업적 사용에 제한이 없습니다.\nccusage 홈페이지 (및 문서)\nccusage\nccusage | Claude Code Usage Analysis\nUsage analysis tool for Claude Code\nccusage 프로젝트 GitHub 저장소\ngithub.com\nGitHub - ryoppippi/ccusage: A CLI tool for analyzing Claude Code usage from...\nA CLI tool for analyzing Claude Code usage from local JSONL files.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/ccusage-claude-code-cli/7508",
        "date": "2025-08-19T10:30:31.723000",
        "source": "pytorch_kr"
    },
    {
        "title": "[2025/08/11 ~ 17] 이번 주에 살펴볼 만한 AI/ML 논문 모음",
        "content": "[2025/08/11 ~ 17] 이번 주에 살펴볼 만한 AI/ML 논문 모음\nPyTorchKR\n이번 주 선정된 논문들을 살펴보면, 첫째로 대규모 비전 및 멀티모달 모델의 효율성 향상과 토큰 압축에 관한 연구가 두드러집니다. DINOv3와 Fourier-VLM, VisionThink 논문들은 각각 자가 지도 학습을 통한 고성능 시각 표현 학습, 주파수 영역에서의 시각 토큰 압축, 그리고 동적 해상도 조절을 통한 시각-언어 모델의 효율적 처리 방식을 제안하며, 대용량 데이터와 모델을 효과적으로 다루기 위한 다양한 전략을 보여줍니다.\n또한, 강화학습(RL)을 활용한 대형 언어 모델(LLM) 및 멀티모달 에이전트의 추론 및 적응 능력 향상에 관한 연구가 활발히 진행되고 있습니다. ‘Thinking With Videos’와 ‘Part I: Tricks or Traps?’, ‘VisionThink’ 논문은 RL 기법을 통해 긴 영상 추론, LLM 추론 최적화, 그리고 시각-언어 모델의 동적 처리 결정에 적용하여 복잡한 환경에서의 문제 해결 능력을 높이고자 하는 시도를 보여줍니다.\n마지막으로, 자율적이고 지속적으로 진화하는 AI 에이전트 및 대규모 다중 에이전트 시스템에 대한 관심이 증가하고 있습니다. ‘A Comprehensive Survey of Self-Evolving AI Agents’와 ‘DeepFleet’, ‘TheAgentCompany’ 논문들은 환경과 상호작용하며 스스로 적응 및 진화하는 에이전트 시스템, 대규모 로봇 군집의 협업 및 계획, 그리고 실제 업무 환경에서의 AI 에이전트 활용 가능성 평가에 초점을 맞추고 있습니다.\nDINOv3: 확장성과 효율성을 갖춘 차세대 자기지도 학습 비전 파운데이션 모델 / DINOv3\n논문 소개\nDINOv3는 대규모 데이터셋과 모델 크기 확장의 이점을 극대화하기 위해 정교한 데이터 준비와 최적화 기법을 활용합니다. Gram anchoring이라는 새로운 방법을 도입하여 장기 학습 시 밀집 특징 맵(dense feature maps)의 성능 저하 문제를 효과적으로 해결합니다. 또한, 해상도, 모델 크기, 텍스트 정렬과 관련된 유연성을 높이는 사후 처리 전략을 적용하여 다양한 비전 작업에서 미세 조정 없이도 최첨단 성능을 달성합니다. 이를 통해 DINOv3는 기존의 자기지도 및 약지도 학습 기반 모델을 크게 능가하는 고품질 밀집 특징을 생성하며, 다양한 자원 환경과 배포 시나리오에 적합한 확장 가능한 비전 모델 군을 제공합니다.\n논문 초록(Abstract)\n자기지도 학습은 수동 데이터 주석의 필요성을 제거하고, 모델이 대규모 데이터셋과 더 큰 아키텍처로 손쉽게 확장할 수 있는 가능성을 제시합니다. 특정 작업이나 도메인에 맞추어지지 않은 이 학습 패러다임은 단일 알고리즘을 사용하여 자연 이미지부터 항공 이미지에 이르기까지 다양한 출처에서 시각적 표현을 학습할 잠재력을 가지고 있습니다. 본 기술 문서에서는 이러한 비전을 실현하기 위한 중요한 이정표인 DINOv3를 소개합니다. DINOv3는 간단하면서도 효과적인 전략을 활용합니다. 첫째, 신중한 데이터 준비, 설계 및 최적화를 통해 데이터셋과 모델 크기 확장의 이점을 극대화합니다. 둘째, 장기간 학습 스케줄 동안 밀집 특징 맵이 저하되는 알려졌으나 해결되지 않은 문제를 효과적으로 다루는 새로운 방법인 Gram anchoring을 도입합니다. 마지막으로, 해상도, 모델 크기 및 텍스트 정렬에 대한 모델의 유연성을 더욱 향상시키는 사후 전략을 적용합니다. 그 결과, 미세조정 없이도 다양한 설정에서 특화된 최첨단 모델들을 능가하는 다목적 비전 파운데이션 모델을 제시합니다. DINOv3는 다양한 비전 작업에서 탁월한 성능을 발휘하는 고품질 밀집 특징을 생성하며, 이전의 자기지도 및 약지도 파운데이션 모델들을 크게 능가합니다. 또한, 다양한 자원 제약과 배포 시나리오에 대응할 수 있는 확장 가능한 솔루션을 제공하여 광범위한 작업과 데이터에서 최첨단 성능을 진전시키기 위해 설계된 DINOv3 비전 모델군도 함께 공개합니다.\nSelf-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.\n논문 링크\narXiv.org\nDINOv3\nSelf-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training...\n더 읽어보기\nAI at Meta\nDINOv3\nDINOv3 scales self-supervised learning (SSL) for images to produce our strongest universal vision backbones, enabling breakthrough performance across diverse domains.\ngithub.com\nGitHub - facebookresearch/dinov3: Reference PyTorch implementation and models for...\nReference PyTorch implementation and models for DINOv3\nhuggingface.co\nDINOv3 - a facebook Collection\nDINOv3: foundation models producing excellent dense features, outperforming SotA w/o fine-tuning - https://arxiv.org/abs/2508.10104\narXiv.org\nDINOv3\nSelf-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training...\nDINOv3: 자기 지도 학습(SSL)을 활용한 초대규모의 범용 비전 백본(Vision Backbone) 모델(feat. Meta AI)\n읽을거리&정보공유\n[DINOv3: 자기 지도 학습(SSL)을 활용한 초대규모의 범용 비전 백본(Vision Backbone) 모델(feat. Meta AI)] DINOv3 소개 DINOv3는 Meta AI가 개발한 차세대 자기지도학습(Self-Supervised Learning, SSL) 기반 비전 모델로, 대규모 이미지 데이터에서 범용적으로 활용 가능한 고성능 시각 백본(vision backbone)을 제공합니다. 기존의 강력한 이미지 인코딩 모델들이 웹 캡션 같은 사람 손으로 작성된 메타데이터에 의존했던 것과 달리, DINOv3는 전혀 라벨이 없는 상태에서 이미지 데이터로부터 의미있는 시각 표현을 스스로 학습하는 자기 지도 학습(Self-Supervised Learning, SSL) 기반의 비전 모델입니다.이를 통해 라벨 수집이 어렵거나 불가능한 분야에서도 대규모 학습이 가능해졌습니다. 특히, DINOv3는 다양한 이미지 도메인에 걸쳐 범용적으로 사용 가능한 백본을 제공함으로써, …\nFourier-VLM: 대규모 비전-언어 모델을 위한 주파수 영역 시각 토큰 압축 기법 / Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models\n논문 소개\nFourier-VLM은 대형 비전-언어 모델에서 시각 토큰 수를 줄이기 위해 주파수 영역에서 시각 표현을 압축하는 효율적인 방법입니다. 비전 인코더 출력의 저주파 성분에 에너지가 집중된 점에 착안하여, 2차원 이산 코사인 변환(Discrete Cosine Transform, DCT)과 고속 푸리에 변환(Fast Fourier Transform, FFT)을 활용해 저역통과 필터링을 수행합니다. 이 과정은 추가 파라미터 없이 계산 복잡도를 크게 낮추며, 실험 결과 LLaVA 및 Qwen-VL 아키텍처에서 최대 83.8%의 추론 FLOPs 감소와 31.2%의 생성 속도 향상을 달성하면서도 경쟁력 있는 성능과 우수한 일반화 능력을 보였습니다.\n논문 초록(Abstract)\n비전-언어 모델(VLM)은 일반적으로 텍스트 지시문 내에 미리 정의된 이미지 플레이스홀더 토큰()을 이미지 인코더에서 추출한 시각적 특징으로 대체하여, 이를 백본 대규모 언어 모델(LLM)의 입력으로 사용합니다. 그러나 다수의 비전 토큰은 컨텍스트 길이를 크게 증가시켜 높은 계산 비용과 추론 지연을 초래합니다. 기존 연구들은 중요한 시각적 특징만을 선택하거나 학습 가능한 쿼리를 활용하여 토큰 수를 줄임으로써 이를 완화하고자 했으나, 종종 성능 저하나 상당한 추가 비용을 수반하는 문제가 있었습니다. 이에 본 논문에서는 주파수 영역에서 시각적 표현을 압축하는 간단하면서도 효율적인 방법인 Fourier-VLM을 제안합니다. 본 접근법은 비전 인코더에서 출력되는 시각적 특징이 저주파 성분에 에너지가 집중되어 있다는 관찰에 기반합니다. 이를 활용하여 2차원 이산 코사인 변환(DCT)을 이용해 저역 통과 필터를 시각적 특징에 적용합니다. 특히, DCT는 빠른 푸리에 변환(FFT) 연산자를 통해 시간 복잡도 $\\mathcal{O}(n\\log n)$로 효율적으로 계산되어, 추가 파라미터 없이도 연산 비용을 최소화합니다. 다양한 이미지 기반 벤치마크에서의 광범위한 실험 결과, Fourier-VLM은 LLaVA 및 Qwen-VL 아키텍처 모두에서 강력한 일반화 능력과 경쟁력 있는 성능을 보였습니다. 특히, LLaVA-v1.5 대비 추론 FLOPs를 최대 83.8%까지 감소시키고 생성 속도를 31.2% 향상시켜, 뛰어난 효율성과 실용성을 입증합니다.\nVision-Language Models (VLMs) typically replace the predefined image placeholder token () in textual instructions with visual features from an image encoder, forming the input to a backbone Large Language Model (LLM). However, the large number of vision tokens significantly increases the context length, leading to high computational overhead and inference latency. While previous efforts mitigate this by selecting only important visual features or leveraging learnable queries to reduce token count, they often compromise performance or introduce substantial extra costs. In response, we propose Fourier-VLM, a simple yet efficient method that compresses visual representations in the frequency domain. Our approach is motivated by the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components. Leveraging this, we apply a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier Transform (FFT) operator with a time complexity of \\mathcal{O}(n\\log n), minimizing the extra computational cost while introducing no additional parameters. Extensive experiments across various image-based benchmarks demonstrate that Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.\n논문 링크\narXiv.org\nFourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large...\nVision-Language Models (VLMs) typically replace the predefined image placeholder token () in textual instructions with visual features from an image encoder, forming the input to a backbone Large Language Model (LLM). However, the large number of...\n비디오 사고: 장기 영상 추론을 위한 다중모달 도구 증강 강화학습 / Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning\n논문 소개\n멀티모달 대형 언어 모델(MLLM)의 영상 추론 능력은 영상 질문 응답과 시간적 위치 지정과 같은 하위 과제에서 매우 중요합니다. 기존의 텍스트 기반 연쇄 사고(chain-of-thought, CoT) 추론 방식은 교차 모달 상호작용이 제한적이고, 긴 영상이나 복잡한 추론 시 환각(hallucination) 문제가 발생하는 단점이 있습니다. 이를 해결하기 위해 VITAL이라는 시각 도구 상호작용 기반의 종단 간 영상 추론 프레임워크를 제안하며, 이는 필요에 따라 영상 프레임을 밀도 있게 샘플링하고 멀티모달 CoT를 생성하여 긴 영상에 대한 정밀한 추론을 가능하게 합니다. 또한, 시간적 위치 지정과 질문 응답을 상호 보완적인 다중 과제로 보고, 이를 위한 대규모 데이터셋과 난이도 인지 그룹 상대 정책 최적화(DGRPO) 알고리즘을 도입하여 다중 과제 강화학습의 난이도 불균형 문제를 완화하였으며, 다양한 벤치마크에서 기존 방법을 능가하는 성능을 입증하였습니다.\n논문 초록(Abstract)\n멀티모달 대규모 언어 모델(MLLM)의 비디오 추론 능력은 비디오 질문 응답 및 시간적 그라운딩과 같은 하위 작업에 매우 중요합니다. 최근 연구들은 MLLM을 위한 텍스트 기반 사고의 연쇄(CoT) 추론을 탐구해왔으나, 이러한 방법들은 특히 긴 비디오나 긴 추론 체인에서 제한된 교차 모달 상호작용과 환각 현상 증가 문제를 겪고 있습니다. 이러한 문제를 해결하기 위해, 본 논문에서는 도구 보강 학습을 통한 비디오 인텔리전스(VITAL)라는 새로운 엔드투엔드 에이전트형 비디오 추론 프레임워크를 제안합니다. 시각적 도구 상자를 활용하여 모델은 필요에 따라 새로운 비디오 프레임을 밀도 있게 샘플링하고, 정밀한 긴 비디오 추론을 위한 멀티모달 사고의 연쇄(CoT)를 생성할 수 있습니다. 또한, 시간적 그라운딩과 질문 응답이 비디오 이해 작업에서 상호 보완적임을 관찰하였습니다. 이에 따라, 감독 학습을 위한 고품질 다중 작업 비디오 추론 데이터셋 MTVR-CoT-72k와 강화 학습을 위한 MTVR-RL-110k를 구축하였습니다. 더불어, 다중 작업 강화 학습에서 난이도 불균형 문제를 완화하기 위해 난이도 인지 그룹 상대 정책 최적화 알고리즘(DGRPO)을 제안합니다. 11개의 도전적인 비디오 이해 벤치마크에서 수행한 광범위한 실험 결과, VITAL은 기존 방법들을 능가하는 우수한 추론 능력을 보였으며, 특히 긴 비디오 시나리오에서 비디오 질문 응답 및 시간적 그라운딩 작업에서 뛰어난 성능을 입증하였습니다. 모든 코드, 데이터 및 모델 가중치는 공개될 예정입니다.\nThe video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal interaction and increased hallucination, especially with longer videos or reasoning chains. To address these challenges, we propose Video Intelligence via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning framework. With a visual toolbox, the model can densely sample new video frames on demand and generate multimodal CoT for precise long video reasoning. We observe that temporal grounding and question answering are mutually beneficial for video understanding tasks. Therefore, we construct two high-quality multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and MTVR-RL-110k for reinforcement learning. Moreover, we propose a Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to mitigate difficulty imbalance in multi-task reinforcement learning. Extensive experiments on 11 challenging video understanding benchmarks demonstrate the advanced reasoning ability of VITAL, outperforming existing methods in video question answering and temporal grounding tasks, especially in long video scenarios. All code, data and model weight will be made publicly available.\n논문 링크\narXiv.org\nThinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for...\nThe video reasoning ability of multimodal large language models (MLLMs) is crucial for downstream tasks like video question answering and temporal grounding. While recent approaches have explored text-based chain-of-thought (CoT) reasoning for MLLMs,...\n속도가 승리한다: 대규모 언어 모델을 위한 효율적 아키텍처 서베이 / Speed Always Wins: A Survey on Efficient Architectures for Large Language Models\n논문 소개\n대형 언어 모델(LLM)은 언어 이해, 생성, 추론뿐만 아니라 멀티모달 모델의 능력 확장에 중요한 역할을 하고 있습니다. 전통적인 트랜스포머(transformer) 구조는 뛰어난 확장성을 지니지만, 대규모 학습과 실용적 적용에 있어 높은 계산 비용이라는 한계가 존재합니다. 이에 본 서베이는 선형 및 희소 시퀀스 모델링, 효율적인 전(全) 어텐션(attention) 변형, 희소 전문가 혼합(sparse mixture-of-experts), 하이브리드 모델 구조, 그리고 확산(diffusion) 기반 LLM 등 다양한 효율적 아키텍처를 체계적으로 정리합니다. 또한 이러한 기술들이 다른 모달리티에 적용되는 사례와 확장 가능하고 자원 효율적인 기초 모델 개발에 미치는 영향도 함께 논의합니다.\n논문 초록(Abstract)\n대규모 언어 모델(LLM)은 언어 이해, 생성, 추론 분야에서 뛰어난 성과를 보여주었으며, 멀티모달 모델의 능력 한계를 확장시키고 있습니다. 현대 LLM의 기반이 되는 트랜스포머(transformer) 모델은 우수한 확장성을 갖춘 강력한 기준점을 제공합니다. 그러나 전통적인 트랜스포머 아키텍처는 막대한 연산량을 요구하며, 대규모 학습과 실질적 배포에 있어 상당한 장애물이 됩니다. 본 서베이 논문에서는 트랜스포머의 내재적 한계를 극복하고 효율성을 향상시키는 혁신적인 LLM 아키텍처들을 체계적으로 검토합니다. 언어 모델링을 출발점으로 하여, 본 서베이는 선형(linear) 및 희소(sparse) 시퀀스 모델링 기법, 효율적인 전(全) 어텐션(full attention) 변형, 희소 전문가 혼합(sparse mixture-of-experts), 앞서 언급한 기법들을 통합한 하이브리드 모델 아키텍처, 그리고 신흥 확산(diffusion) LLM을 포함한 기술적 배경과 세부 내용을 다룹니다. 또한 이러한 기법들의 다른 모달리티 적용 사례와 확장 가능하며 자원 효율적인 파운데이션 모델 개발에 미치는 광범위한 함의도 논의합니다. 최근 연구들을 위 분류에 따라 체계적으로 정리함으로써, 본 서베이는 현대적이고 효율적인 LLM 아키텍처의 청사진을 제시하며, 향후 더욱 효율적이고 다재다능한 AI 시스템 연구에 동기를 부여하고자 합니다.\nLarge Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.\n논문 링크\narXiv.org\nSpeed Always Wins: A Survey on Efficient Architectures for Large Language Models\nLarge Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with...\n더 읽어보기\ngithub.com\nGitHub - weigao266/Awesome-Efficient-Arch\nContribute to weigao266/Awesome-Efficient-Arch development by creating an account on GitHub.\n1부: 대규모 언어 모델 추론을 위한 강화학습의 함정과 요령 심층 분석 / Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning\n논문 소개\n강화학습(Reinforcement Learning, RL)을 활용한 대형언어모델(LLM) 추론 연구가 빠르게 발전하고 있으나, 표준화된 가이드라인 부재와 기법들의 내부 작동 원리에 대한 이해 부족, 실험 환경 및 데이터 차이로 인한 상반된 결과들이 혼재되어 있습니다. 본 연구는 통합된 오픈소스 프레임워크 내에서 주요 RL 기법들을 엄밀히 재현하고 개별 평가하여, 각 기법의 내부 메커니즘, 적용 가능 시나리오, 핵심 원리를 세밀한 실험을 통해 분석하였습니다. 이를 바탕으로 특정 환경에 적합한 RL 기법 선택을 위한 명확한 가이드라인과 실무자들이 참고할 수 있는 신뢰성 높은 로드맵을 제시합니다. 또한, 두 가지 기법의 최소 조합만으로도 vanilla PPO 손실 함수를 사용하는 크리틱 없는 정책의 학습 능력을 크게 향상시켜, 기존 GRPO 및 DAPO 전략을 능가하는 성과를 입증하였습니다.\n논문 초록(Abstract)\n대규모 언어 모델(LLM) 추론을 위한 강화학습은 알고리즘 혁신과 실용적 응용 분야에서 관련 연구가 급격히 증가하며 주목받는 연구 영역으로 부상하였습니다. 이러한 진전에도 불구하고, 강화학습 기법 적용에 대한 표준화된 지침의 부재와 그 기저 메커니즘에 대한 단편적인 이해 등 여러 중요한 과제가 여전히 남아 있습니다. 또한, 실험 환경의 불일치, 학습 데이터의 차이, 모델 초기화의 변동성으로 인해 상충되는 결론들이 도출되어, 해당 기법들의 핵심 특성이 모호해지고 실무자들이 적절한 기법을 선택하는 데 혼란을 초래하고 있습니다. 본 논문에서는 통합된 오픈소스 프레임워크 내에서 엄격한 재현과 개별 평가를 통해 널리 사용되는 강화학습 기법들을 체계적으로 검토합니다. 다양한 난이도의 데이터셋, 모델 크기 및 아키텍처를 포함한 세밀한 실험을 통해 각 기법의 내부 메커니즘, 적용 가능 시나리오, 핵심 원리를 분석합니다. 이러한 통찰을 바탕으로 특정 환경에 맞는 강화학습 기법 선택을 위한 명확한 가이드라인을 제시하며, LLM 분야에서 강화학습을 활용하는 실무자들을 위한 신뢰할 수 있는 로드맵을 제공합니다. 마지막으로, 본 연구는 두 가지 기법의 최소한의 조합만으로도 vanilla PPO loss를 활용한 크리틱 없는 정책의 학습 능력을 극대화할 수 있음을 밝힙니다. 실험 결과, 본 단순 조합은 GRPO 및 DAPO와 같은 전략들을 능가하며 일관되게 성능을 향상시킴을 보여줍니다.\nReinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.\n논문 링크\narXiv.org\nPart I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning\nReinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges...\n자기진화 AI 에이전트 종합 서베이: 기초 모델과 평생 에이전트 시스템을 잇는 새로운 패러다임 / A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems\n논문 소개\n최근 대형 언어 모델(large language models)의 발전으로 복잡한 실제 문제를 해결하는 AI 에이전트에 대한 관심이 증가하고 있습니다. 기존 에이전트 시스템은 배포 후 고정된 수동 설정에 의존해 동적인 환경 변화에 적응하는 데 한계가 있으나, 상호작용 데이터와 환경 피드백을 활용해 자동으로 성능을 향상하는 자기 진화(self-evolving) 에이전트 연구가 주목받고 있습니다. 본 서베이는 자기 진화 에이전트 시스템의 설계에 필수적인 네 가지 구성 요소(시스템 입력, 에이전트 시스템, 환경, 최적화기)를 중심으로 다양한 진화 기법을 체계적으로 정리하고, 생의학, 프로그래밍, 금융 등 특화 분야별 진화 전략도 함께 다룹니다. 또한 평가, 안전성, 윤리적 고려사항을 논의하여 자기 진화 에이전트의 신뢰성과 효과성을 확보하는 데 중요한 방향성을 제시합니다.\n논문 초록(Abstract)\n최근 대규모 언어 모델(LLM)의 발전은 복잡하고 실제적인 과제를 해결할 수 있는 AI 에이전트에 대한 관심을 증대시켰습니다. 그러나 대부분의 기존 에이전트 시스템은 배포 이후에도 정적으로 유지되는 수동으로 설계된 구성에 의존하여, 동적이고 변화하는 환경에 적응하는 능력이 제한적입니다. 이를 해결하기 위해 최근 연구들은 상호작용 데이터와 환경 피드백을 기반으로 에이전트 시스템을 자동으로 향상시키는 에이전트 진화 기법을 탐구하고 있습니다. 이 새로운 연구 방향은 기초 모델의 정적인 능력과 평생 에이전트 시스템이 요구하는 지속적인 적응성을 연결하는 자가 진화 AI 에이전트의 토대를 마련합니다. 본 서베이 논문에서는 자가 진화 에이전트 시스템을 위한 기존 기법들을 포괄적으로 검토합니다. 구체적으로, 먼저 자가 진화 에이전트 시스템 설계의 근간이 되는 피드백 루프를 추상화한 통합 개념적 프레임워크를 소개합니다. 이 프레임워크는 시스템 입력(System Inputs), 에이전트 시스템(Agent System), 환경(Environment), 최적화기(Optimisers)의 네 가지 핵심 구성 요소를 강조하여 다양한 전략을 이해하고 비교하는 기반을 제공합니다. 해당 프레임워크를 바탕으로 에이전트 시스템의 각기 다른 구성 요소를 대상으로 하는 다양한 자가 진화 기법들을 체계적으로 검토합니다. 또한, 생명 의학, 프로그래밍, 금융과 같이 최적화 목표가 도메인 제약과 밀접하게 연관된 특수 분야를 위한 도메인 특화 진화 전략도 조사합니다. 아울러 자가 진화 에이전트 시스템의 효과성과 신뢰성을 보장하는 데 필수적인 평가, 안전성 및 윤리적 고려사항에 대해 별도의 논의를 제공합니다. 본 서베이는 연구자와 실무자에게 자가 진화 AI 에이전트에 대한 체계적인 이해를 제공하여, 보다 적응적이고 자율적이며 평생 지속 가능한 에이전트 시스템 개발의 토대를 마련하는 것을 목표로 합니다.\nRecent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.\n논문 링크\narXiv.org\nA Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging...\nRecent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment,...\n더 읽어보기\ngithub.com\nGitHub - EvoAgentX/Awesome-Self-Evolving-Agents\nContribute to EvoAgentX/Awesome-Self-Evolving-Agents development by creating an account on GitHub.\nTheAgentCompany: 실제 업무 과제에서 LLM 에이전트 성능 평가 벤치마크 / TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks\n논문 소개\n일상과 업무에서 컴퓨터와 인터넷을 활용하는 작업이 많아진 가운데, 대형 언어 모델(LLM)을 기반으로 한 AI 에이전트가 주변 환경과 상호작용하며 업무를 자동화하는 능력이 주목받고 있습니다. 본 연구에서는 실제 소프트웨어 회사 환경을 모사한 자체 웹사이트와 데이터를 포함한 환경에서, 웹 탐색, 코드 작성, 프로그램 실행, 동료와의 소통 등 디지털 근로자와 유사한 방식으로 작업하는 AI 에이전트의 성능을 평가하는 확장 가능한 벤치마크인 TheAgentCompany를 제안합니다. 폐쇄형 API와 공개 가중치 언어 모델을 활용한 기본 에이전트 실험 결과, 가장 우수한 에이전트가 전체 작업의 약 30%를 자율적으로 수행할 수 있음을 확인하였으며, 단순 작업은 자동화가 가능하지만 복잡한 장기 과제는 아직 한계가 있음을 시사합니다. 연구에 사용된 코드, 데이터, 환경 및 실험 결과는 공개되어 산업 및 경제 정책 분야에서 AI 도입 효과를 평가하는 데 기여할 수 있습니다.\n논문 초록(Abstract)\n우리는 일상생활이나 업무에서 컴퓨터와 상호작용하며, 많은 업무가 컴퓨터와 인터넷 접근만으로 완전히 수행될 수 있습니다. 동시에, 대규모 언어 모델(LLM)의 발전 덕분에 주변 환경과 상호작용하며 변화를 일으키는 AI 에이전트가 빠르게 발전하고 있습니다. 그렇다면 AI 에이전트는 업무 관련 작업을 가속화하거나 자율적으로 수행하는 데 얼마나 뛰어난 성능을 보일까요? 이 질문에 대한 답은 AI를 업무 흐름에 도입하려는 산업계와 AI 도입이 노동 시장에 미칠 영향을 이해하고자 하는 경제 정책 모두에 중요한 시사점을 제공합니다. 본 논문에서는 이러한 LLM 에이전트가 실제 전문 업무를 수행하는 성능 진척도를 측정하기 위해, 웹 탐색, 코드 작성, 프로그램 실행, 동료와의 소통 등 디지털 근로자와 유사한 방식으로 세계와 상호작용하는 AI 에이전트를 평가할 수 있는 확장 가능한 벤치마크인 TheAgentCompany를 제안합니다. 우리는 내부 웹사이트와 데이터를 포함한 자급자족 환경을 구축하여 소규모 소프트웨어 회사 환경을 모방하고, 해당 회사의 근로자가 수행할 수 있는 다양한 업무를 설계하였습니다. 폐쇄형 API 기반 및 오픈 웨이트 언어 모델(LM)을 탑재한 기본 에이전트를 테스트한 결과, 가장 경쟁력 있는 에이전트가 자율적으로 30%의 작업을 완료할 수 있음을 확인하였습니다. 이는 LM 에이전트를 활용한 작업 자동화에 대해 미묘한 시사점을 제공합니다. 실제 직장 환경을 시뮬레이션한 설정에서 단순한 작업의 상당 부분은 자율적으로 해결 가능하지만, 더 어렵고 장기적인 작업은 여전히 현 시스템의 범위를 벗어납니다. 코드, 데이터, 환경 및 실험 결과는 https://the-agent-company.com 에 공개합니다.\nWe interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.\n논문 링크\narXiv.org\nTheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks\nWe interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there...\n더 읽어보기\nthe-agent-company.com\nThe Agent Company\nBenchmarking LLM Agents on Consequential Real World Tasks\nDeepFleet: 대규모 모바일 로봇 군집을 위한 다중 에이전트 기반 파운데이션 모델 / DeepFleet: Multi-Agent Foundation Models for Mobile Robots\n논문 소개\nDeepFleet는 대규모 모바일 로봇 군집의 협력 및 계획 지원을 위해 설계된 파운데이션 모델 세트입니다. 아마존 창고에서 수집한 수십만 대 로봇의 위치, 목표, 상호작용 데이터를 기반으로 네 가지 아키텍처를 제안하며, 각각 로봇 중심, 로봇-바닥, 이미지-바닥, 그래프-바닥 모델로 구분됩니다. 평가 결과, 비동기적 로봇 상태 업데이트와 국소적 상호작용 구조를 반영한 로봇 중심 및 그래프-바닥 모델이 예측 성능에서 가장 우수한 것으로 나타났습니다. 또한 이 두 모델은 대규모 창고 운영 데이터셋을 활용할 때 확장성 측면에서도 효과적임을 확인하였습니다.\n논문 초록(Abstract)\n본 논문에서는 대규모 모바일 로봇 플릿의 조정 및 계획을 지원하기 위해 설계된 기초 모델군인 DeepFleet를 소개합니다. 이 모델들은 전 세계 아마존 물류창고에서 수십만 대의 로봇 위치, 목표, 상호작용을 포함한 플릿 이동 데이터로 학습되었습니다. DeepFleet는 각각 고유한 귀납적 편향(inductive bias)을 내포하고 다중 에이전트 기초 모델 설계 공간의 핵심 요소를 탐구하는 네 가지 아키텍처로 구성됩니다. 로봇 중심(RC) 모델은 개별 로봇의 이웃 영역에서 작동하는 자기회귀 결정 트랜스포머(autoregressive decision transformer)이며, 로봇-플로어(RF) 모델은 로봇과 물류창고 바닥 간 크로스 어텐션(cross-attention)을 활용하는 트랜스포머입니다. 이미지-플로어(IF) 모델은 전체 플릿의 다중 채널 이미지 표현에 합성곱 인코딩(convolutional encoding)을 적용하며, 그래프-플로어(GF) 모델은 시계열 어텐션(temporal attention)과 그래프 신경망(graph neural networks)을 결합하여 공간적 관계를 모델링합니다. 본 논문에서는 이들 모델을 상세히 설명하고, 설계 선택이 예측 과제 성능에 미치는 영향을 평가한 결과를 제시합니다. 비동기 로봇 상태 업데이트(asynchronous robot state updates)와 로봇 상호작용의 국소적 구조(localized structure)를 모두 반영한 로봇 중심 및 그래프-플로어 모델이 가장 유망한 성과를 보였습니다. 또한, 이 두 모델이 모델 규모 확장에 따라 더 큰 물류창고 운영 데이터셋을 효과적으로 활용할 수 있음을 입증하는 실험 결과도 함께 제시합니다.\nWe introduce DeepFleet, a suite of foundation models designed to support coordination and planning for large-scale mobile robot fleets. These models are trained on fleet movement data, including robot positions, goals, and interactions, from hundreds of thousands of robots in Amazon warehouses worldwide. DeepFleet consists of four architectures that each embody a distinct inductive bias and collectively explore key points in the design space for multi-agent foundation models: the robot-centric (RC) model is an autoregressive decision transformer operating on neighborhoods of individual robots; the robot-floor (RF) model uses a transformer with cross-attention between robots and the warehouse floor; the image-floor (IF) model applies convolutional encoding to a multi-channel image representation of the full fleet; and the graph-floor (GF) model combines temporal attention with graph neural networks for spatial relationships. In this paper, we describe these models and present our evaluation of the impact of these design choices on prediction task performance. We find that the robot-centric and graph-floor models, which both use asynchronous robot state updates and incorporate the localized structure of robot interactions, show the most promise. We also present experiments that show that these two models can make effective use of larger warehouses operation datasets as the models are scaled up.\n논문 링크\narXiv.org\nDeepFleet: Multi-Agent Foundation Models for Mobile Robots\nWe introduce DeepFleet, a suite of foundation models designed to support coordination and planning for large-scale mobile robot fleets. These models are trained on fleet movement data, including robot positions, goals, and interactions, from hundreds...\nVisionThink: 강화학습 기반 스마트하고 효율적인 비전-언어 모델 / VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning\n논문 소개\nVisionThink은 시각-언어 모델(VLM)의 시각 토큰 수를 동적으로 조절하여 효율성을 높이는 새로운 방법론입니다. 초기에는 저해상도 이미지를 사용하고, 문제 해결에 충분하지 않을 경우에만 고해상도 이미지를 요청하는 방식으로, 고정된 토큰 압축 비율 대신 샘플별로 최적의 압축을 수행합니다. 강화학습과 LLM-as-Judge 전략을 도입하여 일반적인 시각질문응답(VQA) 과제에 적용하였으며, 보상 함수와 페널티 메커니즘을 통해 안정적인 이미지 크기 조절을 달성하였습니다. 실험 결과, OCR 관련 과제에서는 세밀한 시각 이해 능력을 유지하면서도 단순 과제에서는 시각 토큰 수를 크게 절감하는 우수한 성능과 효율성을 입증하였습니다.\n논문 초록(Abstract)\n최근 비전-언어 모델(VLM)의 발전은 시각 토큰 수를 증가시켜 성능을 향상시켰으며, 시각 토큰은 텍스트 토큰보다 훨씬 긴 경우가 많습니다. 그러나 대부분의 실제 시나리오에서는 이처럼 많은 시각 토큰이 필요하지 않다는 점을 관찰하였습니다. OCR 관련 작업의 일부 소규모 집합에서는 성능이 크게 저하되지만, 대부분의 일반적인 VQA(Visual Question Answering) 작업에서는 1/4 해상도만으로도 정확한 성능을 유지합니다. 이에 본 논문에서는 서로 다른 샘플에 대해 동적으로 해상도를 조절하여 처리하는 새로운 시각 토큰 압축 패러다임인 VisionThink를 제안합니다. VisionThink는 저해상도 이미지를 시작점으로 하여 문제 해결에 충분한지 스마트하게 판단하며, 그렇지 않은 경우 더 높은 해상도의 이미지를 요청하는 특수 토큰을 출력할 수 있습니다. 기존의 고정된 가지치기 비율 또는 임계값을 사용하여 토큰을 압축하는 효율적 VLM 방법과 달리, VisionThink는 사례별로 토큰 압축 여부를 자율적으로 결정합니다. 그 결과 OCR 관련 작업에서 세밀한 시각 이해 능력을 발휘하는 동시에, 단순한 작업에서는 상당한 시각 토큰 절약 효과를 보입니다. 강화학습을 도입하고, LLM-as-Judge 전략을 제안하여 일반적인 VQA 작업에 성공적으로 RL을 적용하였습니다. 또한 안정적이고 합리적인 이미지 크기 조정 호출 비율을 달성하기 위해 보상 함수와 페널티 메커니즘을 정교하게 설계하였습니다. 광범위한 실험을 통해 본 방법의 우수성, 효율성 및 효과성을 입증하였습니다. 본 코드 저장소는 GitHub - dvlab-research/VisionThink: Efficient Reasoning Vision Language Models 에서 확인할 수 있습니다.\nRecent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at GitHub - dvlab-research/VisionThink: Efficient Reasoning Vision Language Models.\n논문 링크\narXiv.org\nVisionThink: Smart and Efficient Vision Language Model via Reinforcement...\nRecent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an...\n더 읽어보기\ngithub.com\nGitHub - dvlab-research/VisionThink: Efficient Reasoning Vision Language Models\nEfficient Reasoning Vision Language Models\nRAG-MCP: 검색-증강 생성을 통한 LLM 도구 선택 시 프롬프트 과부하 완화 방법 / RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation\n논문 소개\n대규모 언어 모델(LLM)이 Model Context Protocol(MCP)로 정의된 다양한 외부 도구를 활용할 때, 프롬프트 과부하와 선택 복잡성 문제를 겪는다는 점에 주목하였습니다. RAG-MCP는 검색 증강 생성(Retrieval-Augmented Generation) 방식을 도입하여, 외부 인덱스에서 쿼리와 가장 관련성 높은 MCP를 의미 기반 검색으로 선별함으로써 도구 탐색 부담을 줄입니다. 이로 인해 모델에 전달되는 프롬프트 크기가 크게 감소하고 도구 선택 정확도가 3배 이상 향상되는 효과를 보였습니다. 결과적으로 RAG-MCP는 LLM의 도구 통합을 보다 확장 가능하고 정확하게 만드는 방법을 제시합니다.\n논문 초록(Abstract)\n대형 언어 모델(LLM)은 Model Context Protocol(MCP)\\cite{IntroducingMCP}로 정의된 외부 도구의 수가 증가함에 따라 프롬프트 부피 증가와 선택 복잡성으로 인해 이를 효과적으로 활용하는 데 어려움을 겪고 있습니다. 본 논문에서는 도구 탐색 부담을 분산시켜 이러한 문제를 해결하는 검색-증강 생성(RAG) 프레임워크인 RAG-MCP를 제안합니다. RAG-MCP는 외부 인덱스에서 주어진 쿼리에 가장 적합한 MCP를 의미 기반 검색을 통해 식별한 후 LLM에 전달합니다. 모델에는 선택된 도구 설명만 전달되어 프롬프트 크기를 대폭 줄이고 의사결정을 단순화합니다. MCP 스트레스 테스트를 포함한 실험 결과, RAG-MCP는 프롬프트 토큰 수를 50% 이상 절감하고 벤치마크 과제에서 도구 선택 정확도를 13.62%의 기준선 대비 43.13%로 3배 이상 향상시킴을 보여줍니다. RAG-MCP는 LLM의 확장 가능하고 정확한 도구 통합을 가능하게 합니다.\nLarge language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to identify the most relevant MCP(s) for a given query from an external index before engaging the LLM. Only the selected tool descriptions are passed to the model, drastically reducing prompt size and simplifying decision-making. Experiments, including an MCP stress test, demonstrate RAG-MCP significantly cuts prompt tokens (e.g., by over 50%) and more than triples tool selection accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables scalable and accurate tool integration for LLMs.\n논문 링크\narXiv.org\nRAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via...\nLarge language models (LLMs) struggle to effectively utilize a growing number of external tools, such as those defined by the Model Context Protocol (MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We introduce RAG-MCP, a...\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 뉴스 발행에 힘이 됩니다~\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/2025-08-11-17-ai-ml/7500",
        "date": "2025-08-18T06:30:38.309000",
        "source": "pytorch_kr"
    },
    {
        "title": "RightNowAI의 rightnow‑cli: AI 기반의 CUDA 커널 자동 최적화 CLI 도구",
        "content": "rightnow‑cli 소개\nRightNow‑AI가 개발한 rightnow‑cli는 개발자가 GPU의 CUDA 커널을 손쉽게 개선할 수 있도록 돕는 명령줄(CLI) 도구입니다. 이 도구는 AI 기술을 활용하여 최소한의 수작업으로도 CUDA 커널의 성능을 비약적으로 향상시키도록 설계되었습니다. 특히 GPU 아키텍처에 대한 전문 지식이 부족한 사용자도 자연어 기반으로 최적화 요구를 전달하기만 하면 즉각적인 결과를 얻을 수 있는 것이 특징입니다.\nCUDA 최적화는 일반적으로 많은 시간과 노력이 필요한 고난도 작업입니다. 하지만 rightnow‑cli는 이를 기존 워크플로우에 자연스럽게 통합할 수 있는 방식으로 단순화하여, 개발자의 생산성과 코드 성능을 동시에 끌어올리는 접근을 제공합니다. 수작업 없이(no manual effort)도 코드 성능을 20~50배, 때로는 100배 이상까지 향상시킬 수 있다고 소개하고 있습니다 .\n이 CLI는 명령만 실행하면 AI가 CUDA 코드를 분석하고, 병목 현상을 찾아내 최적화된 코드를 생성해 줍니다. 즉, 사용자는 CUDA에 대한 깊은 전문 지식 없이도 자연어 기반 프롬프트로 고성능 커널을 얻을 수 있습니다. CUDA 최적화라는 복잡하고 시간이 많이 소요되는 작업을 AI가 자동으로 수행해 준다는 점에서, GPU 개발자에게 혁신적인 워크플로우를 제공한다고 볼 수 있습니다.\nrightnow-cli의 구조 및 특징\nAI 기반 자동 최적화 rightnow‑cli는 CUDA 커널 코드를 입력받고, AI가 이를 분석한 뒤 병목을 탐지하고 최적화된 코드로 리팩토링 합니다 . 사용자는 “optimize this kernel for A100”처럼 자연어 프롬프트를 작성하면 됩니다 .\n서버리스 GPU 프로파일링 로컬 GPU가 없어도, 서버 상의 GPU 환경에서 커널 실행 및 프로파일링이 가능해 성능 병목 현상을 확인할 수 있습니다 .\n주요 성능 향상 사례\n2–4배의 즉각적 개선\n일부 경우 20배 이상 향상 성능을 기록했다고 합니다 .\n지원 GPU 아키텍처 Ampere, Hopper, Ada Lovelace, Blackwell 등 최신 NVIDIA GPU 아키텍처 전반을 지원합니다 .\n성능 결과 지표 예시:\nMatrix Transpose: 104× 개선\nMonte Carlo Integration: 179× 개선\nDot Product: 102× 개선 등 .\n활용 이유 및 배경 개발자들이 수작업으로 최적화할 때 겪는 반복적인 시행착오와 시간 낭비를 대폭 줄이기 위해, RightNow‑AI 팀 자체의 경험에서 출발한 도구입니다 .\n한계점 및 향후 개선 방향\n신뢰성 및 안전성: AI 최적화 코드는 종종 예기치 못한 버그를 일으킬 수 있으므로, checkpointing 또는 테스트 자동화 기능의 보완이 필요할 수 있습니다.\n레이턴시 vs 성능 트레이드오프: 20–50x 성능 향상이 모든 상황에서 나타날지는 미지수이므로, 다양한 하드웨어와 워크로드에서의 반복 검증이 필요합니다.\n사용자 피드백 루프: 최적화 결과가 불만족스러울 때, 사용자가 직접 조정할 수 있는 피드백 메커니즘이 있으면 좋습니다.\n라이선스\nRightNowAI의 rightnow-cli 도구는 RightNow CLI License로 공개되어 있습니다. 개인적인 목적이나 교육/비상업적 목적으로만 소스 사용 및 수정이 가능하며, 상업적 목적으로는 별도의 라이선스 계약이 필요합니다.\n그 외, 수정 후 재배포 금지, 경쟁 서비스/제품 개발 금지 등의 조항이 있으니 사용 전 라이선스 전문을 읽고 숙지하셔야 합니다.\nrightnow-cli 프로젝트 GitHub 저장소\ngithub.com\nGitHub - RightNow-AI/rightnow-cli: Automatically optimize your CUDA kernels using...\nAutomatically optimize your CUDA kernels using cutting-edge AI. Get 20-50x performance improvements with zero manual effort.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/rightnowai-rightnow-cli-ai-cuda-cli/7498",
        "date": "2025-08-17T19:30:21.663000",
        "source": "pytorch_kr"
    },
    {
        "title": "[GN⁺] GPT-OSS vs. Qwen3 및 GPT-2 이후 LLM 아키텍처 발전 상세 비교",
        "content": "GPT-OSS vs. Qwen3 및 GPT-2 이후 LLM 아키텍처 발전 상세 비교 글 소개\nOpenAI가 gpt-oss-20b/120b 모델을 오픈 가중치로 공개함에 따라 2019년 GPT-2 이후 처음으로 OpenAI의 대형 공개 가중치 LLM이 등장함\ngpt-oss 모델은 GPT-2와 비교해 Dropout, Absolute Position Embedding, GELU 등을 효율적인 현대 기법인 RoPE, SwiGLU, RMSNorm 등으로 대체하며 발전함\nMixture-of-Experts(모듈형 전문가 구조), Sliding Window Attention, MXFP4 양자화 등의 적용으로 성능 효율뿐 아니라 단일 GPU 실행 환경을 크게 개선함\nQwen3와의 비교에서 아키텍처 깊이/넓이, 전문가 수, 주의 편향, 오픈소스 라이선스 등 다양한 차별점이 존재함을 확인함\ngpt-oss-20b는 최신 하드웨어에 맞춘 경량화와 reasoning effort 조정 기능으로 실제 활용성과 연구 확장성 모두 확보함\n개요 및 주요 혁신\nOpenAI는 gpt-oss-20b/120b를 2019년 GPT-2 이후 처음으로 오픈 가중치로 공개함\n일반 사용자 GPU(최대 16GB RAM)에서 20B, H100 80GB에서 120B를 실행 가능하게 함\nMXFP4 최적화로 단일 GPU 실행, 소비자 접근성 확대\nGPT-2 → gpt-oss 주요 아키텍처 변화\nDropout 제거\nGPT-2에는 Dropout이 포함됐으나 대량 데이터 단일 epoch 학습 환경에선 오히려 성능 저하가 확인됨\n최근 연구 결과에서도 Dropout 미적용이 LLM의 다운스트림 작업에서 더 뛰어난 성능을 보임\nRoPE(회전 위치 임베딩) 채택\n기존 절대 위치 임베딩 대신 RoPE(Rotary Position Embedding) 가 주류로 자리 잡음\nRoPE는 쿼리/키 벡터의 각도를 위치에 따라 회전시켜 더 유연하고 일반화된 위치 정보를 제공함\nSwiGLU 활성화 함수와 GLU 도입\nGEGLU/SwiGLU 등 GLU 방식 도입으로 기존 2-layer FFN보다 적은 파라미터로 더 우수한 표현 능력을 발휘함\nSwish는 연산적으로도 GELU 대비 효율적\nMixture-of-Experts(MoE) 적용\n단일 FFN 대신 다중 전문가(Expert) 네트워크를 활용해 매 토큰 생성 시 일부 전문가만 활성화\n모델 파라미터 수를 급격히 늘리면서도 추론 효율성(희소성) 유지, 학습 용량 증대\nGrouped Query Attention(GQA) 도입\n기존 Multi-Head Attention 대비 키/값 공유로 메모리 및 연산량 절감 효과\n성능 손실 없이 효율성 개선, 대규모 LLM에서 표준적 적용 추세\nSliding Window Attention 활용\n일부 레이어마다 전체 문맥 대신 최근 128토큰 한정 Sliding Window로 국소 주의 계산, 메모리 사용량 최소화\n성능 저하 없이 빠른 추론, 대규모 컨텍스트 지원용\nRMSNorm 채택\nLayerNorm 대신 RMSNorm 적용으로 연산 효율 증대\nLayerNorm의 평균/분산 계산 대신 RMS(평균제곱근)를 적용, GPU 연산 부담 감소\ngpt-oss와 Qwen3 비교\n규모/구조 차이\nQwen3은 더 깊은(48개 Transformer 블록) 구조이나, gpt-oss는 더 넓은(emb dimension, head 수 증가) 구조\n깊은 모델이 더 유연하지만 학습 어려움, 넓은 모델이 추론 병렬화에 유리(Gemma 2 논문, 9B 모델 기준 넓은 쪽이 소폭 우세)\nMoE 구조 차이\ngpt-oss-20b: 32명 대형 전문가, 4명만 활성화\nQwen3: 다수 소형 전문가, 8명 활성화\n최신 흐름은 더 많은 소형 전문가 구성이 효과적이라는 방향이나 gpt-oss는 대형-소수 구조 고수 (20B, 120B에서는 전문가 및 블록 수만 조정)\nAttention Bias와 Sinks\ngpt-oss는 attention에 bias 유닛 활용 (GPT-2 시절 이후 보기 드문 방식)\n하지만 key-proj에는 효과 미미함이 최근 연구에서 밝혀짐\n주의 sink는 시퀀스 시작위치에 항상 attend되는 특수 토큰 개념이나, gpt-oss에서는 입력 토큰에 변형 없이 Learned bias logit 형태로 각 head에 추가 적용\n라이선스 및 공개 범위\nApache 2.0 오픈소스 라이선스로 상업적 활용/파생 모델 구축 자유\n단, 진정한 의미의 오픈소스(학습 코드, 데이터 세트 공개)는 아님(‘open weight’ 모델임)\n기타 세부 사항 및 실제 운용\n훈련/최적화\ngpt-oss는 2.1M H100-hours 컴퓨팅 리소스로 훈련\n영어 중심, STEM과 코딩, 일반 지식 텍스트에 집중\n사전학습+지도 미세학습(Instruction), RL 기반 reasoning 단계 등 최신 기법 적용\nReasoning Effort 조절\nSystem prompt를 통해 reasoning effort(저/중/고)를 설정해 답변 길이·정확도를 자동 조정\n단순 작업은 저효율로 빠르게, 복잡한 reasoning이 필요하면 높게 설정 가능\nMXFP4 양자화로 단일 GPU 지원\nMXFP4 포맷 활용으로 20B도 16GB VRAM(최신 GPU 필수)에서 구동 가능\n120B는 H100 기준 80GB 메모리면 단일 GPU에서 실현 가능, 분산 처리 없고 구동 간편\n벤치마크 및 실 사용성\ngpt-oss는 학습 초점이 reasoning에 치중, 일부 범용 지식 질문에는 환각(hallucination) 경향\n사용성 면에서는 현존 오픈 모델 중 상위, tool integration과 조합 시 실용성 강화 예정\n실제 사용에서 정확도와 reasoning의 균형, 추후 타 오픈모델과의 비교 필요\nGPT-5와의 비교\ngpt-oss-120b는 OpenAI 상용 모델(GPT-5)과 벤치마크 기준 근접 성능을 보임\n현실 환경에서의 우위는 더 지켜봐야 하나, 오픈 가중치로 제공되는 최신 LLM 중 강력한 대안임\n벤치마크만으로 실전 경쟁력 완전히 설명하기엔 한계, 향후 외부 비교 및 연구에 큰 기회 제공\n요약\ngpt-oss 시리즈의 등장은 대형 오픈 가중치 LLM 분야의 새로운 기준 제시, 최신 LLM들이 도입한 혁신적 아키텍처들이 실제로 어떻게 구현·적용됐는지 상세히 비교, 분석됨\nQuen3, GPT-5 등 다른 최신 모델과의 차별점과 추세를 파악할 수 있어, 실제 적용/연구에 유용한 최신 동향 파악 가능\nHacker News 의견\nQwen3가 로컬 테스트에서 훨씬 뛰어남을 확인함. 32B 파라미터 버전에서는 프롬프트를 거의 완벽하게 지키며 결과가 자연스럽게 나옴. 반면 simplebench gpt-oss(120B)는 논리 퍼즐에서 좋지 않은 성능을 보임. 이런 차이는 트레이닝 방식, 모델 차원, 그리고 적은 수의 대형 전문가 vs 많은 수의 소형 전문가 등에서 비롯된다고 생각함\nQwen3 32B는 모든 파라미터를 항상 사용하는 덴스 모델임. GPT OSS 20B는 일부만 사용하는 스파스 MoE(Expert of Experts) 모델로, 한 번에 약 3.6B만 활용함. 이로 인해 덴스 20B 모델보다 빠르고, 3.6B 모델보다는 똑똑함. 공정한 비교라면 덴스 8B 모델과 비교해야 하고, Qwen Coder 30B A3B 같은 모델도 좋은 비교 지점임\n내 생각에 이런 차이는 모델 아키텍처보다는 데이터와 트레이닝 파이프라인 영향이 훨씬 크다고 봄. gpt-oss가 Phi 스타일의 합성 데이터셋만을 활용하고, 주로 벤치마크 게임에 집중했다는 이야기가 있는데, 그 증거가 충분히 설득력 있어 보임\nMoE의 기대 성능 공식은 sqrt(활성 헤드 수 * 전체 파라미터 수)임. 예를 들어 sqrt(120*5) ~= 24로, GPT-OSS 120B는 사실 24B 수준의 성능과 훨씬 작은 모델 수준의 속도를 제공함\nqwen3는 느린 편임. 직접 써보니 동작은 하는데 속도가 느리고, 기능이 부족한 느낌임\nSebastian Raschk의 블로그 글들이 보물 같은 정보임. get-oss와 qwen3 모델을 Ollama, LM Studio로 로컬에서 사용하고, 대형 모델은 상용 API를 씀. get-oss는 프롬프트에 많은 컨텍스트 정보를 넘기면 좋은 결과를 주고, qwen3는 그냥 훌륭함. 3년 전까지는 신경망, GAN, RNN, LSTM 등 머신러닝을 실제로 구현할 정도로 잘 이해했었는데, 요즘 LLM은 직접 개발할 정도로 쉽지 않아서 아쉬움. Sebastian Raschk의 책도 보고 있는데, 아마 끝까지 다 못 볼 듯함\n믿을 수 없을 정도로 빠르게 변화하는 분야에서 Sebastian Raschk가 항상 최신 정보를 간결하게 정리해줘서 정말 도움을 받고 있음\n로컬 3090 GPU에서 qwen3 coder instruct 30b-a3b exl3 q6 모델을 돌려서 샘플 페이지도 만들고, 서버 실행, 남아있는 서버 감지, 이를 직접 종료한 후(권한 요청까지 받음), 재실행 후 ip를 자동으로 찾아 브라우저에 띄우는 과정을 해봄. 이제는 더 이상 단순 데모가 아니라 주니어나 인턴에게도 실질적으로 유용한 수준의 도움임\n내 경험상 qwen3-coder가 월등히 뛰어남. gpt-oss:20b도 설치해봤지만, 코드 요약을 시키면 qwen3는 몇 초 만에 결과가 나오고 gpt-oss는 5분 넘게 아무 일도 하지 않아서 중단함. 그래서 그냥 qwen3만 씀. 만약 원하는 답을 못 받으면, 검색 엔진이나 Perplexity를 씀. 10GB 3080, Ryzen 3600x, 32GB RAM을 쓰고 있음. Qwen3-coder는 지금까지 써본 것 중 최고임\nQwen3 coder 480B는 Sonnet 4와 맞먹을 정도로 좋음. 이 덕분에 중국 모델이 미국 기반 모델을 조만간 앞지를 수도 있다는 실감을 처음 가짐(특히 코딩 분야에서)\ngpt-oss 20B는 10GB에 올라가지 않아서 생긴 문제일 가능성이 있음\n나도 gpt-oss-20b를 간단하게 쓰는데, 짧은 프롬프트(단문)에는 무한 반복에 빠질 때가 있음. llama.cpp로 돌릴 때 반복 패널티 값을 작게 잡으니 그런 문제가 없었음(주로 diff 분석에 하루 몇 번 정도 사용함). 단, 내가 운이 좋은 걸 수도 있음\n혹시 agentic 방식(여러 번의 질문과 답변을 주고받는 자동화)으로 쓰고 있는지, 아니면 복사해서 “이 코드 짜줘” 식의 단일 입력/출력으로만 쓰는지 궁금함. 최신 공개 모델이 agentic한 코딩에서 얼마나 상용 모델을 따라잡았는지 알고 싶음\n요즘 오픈 웨이트 LLM들은 아키텍처가 너무 비슷하고, 혁신이 데이터나 RL 쪽에서만 일어나고 있는 점이 흥미로움. 예전 대형 ML 조직에서는 아키텍처 튜닝이 가장 중요했는데 현실은 달라 보임\nLLM 규모에서는 하이퍼파라미터 튜닝 자체가 불가능하다고 봄. 비용이 너무 커서 여러 아키텍처를 기본 테스트만 하고, 하나를 골라 데이터와 RL로 최적화하는 식임\n좋은 지적임. LLM 덕분에 리소스만 충분하면 누구나 도전할 수 있게 되었음. 아키텍처가 꽤 조정에 강하고, 충분한 컴퓨트와 데이터를 넣으면 확장 법칙(scaling law)를 어겨도 괜찮은 모델을 만들 수 있음(Llama 3가 과거에 보여줬던 것처럼)\nQwen3 4B 모델을 로컬에서 정말 잘 사용 중임. 온라인 모델은 거의 안 쓰고, 웹 검색도 훨씬 타깃팅이 잘 됨. 완전히 신뢰하지는 않지만 전반적으로 괜찮음. 이런 오픈소스 모델이 로컬 지식 자동화의 판도를 바꿀 거라고 확신함\nQwen이 직접 더 나은 검색 파라미터를 안내해주는 것인지, 아니면 Qwen이 실제 웹 검색까지 해주는 것인지 궁금함\nLM Arena에서 순수 Transformer 기반이 아닌 모델 중 가장 성능이 좋은 모델은 Jamba임(Transformers와 state space 모델의 하이브리드 구조, 96위). Tencent의 hunyuan-turbos도 역시 하이브리드로, 22위임. arxiv 논문 참고\nLLM은 보통 아주 거대한 데이터셋을 딱 한 번(단일 에폭)만 학습함. 이는 여러 번 반복 학습(수백 에폭) 전제를 깔고 있던 Dropout 방식과는 다른 환경임\n이건 잘 알려진 사실임. GPT-3 논문의 Table 2.2를 참고하면 됨\n대형 연구실에서 공개하는 모델들이 추가적인 학습을 더 하면 얼마나 발전할 수 있을지 궁금함. 예를 들어 GPT-OSS가 210만 시간 학습했다면, 그걸 두 배로 늘리면 얼마나 개선될 수 있을지 알고 싶음\nGPT-4.5는 사실 더 큰 GPT-5로 기획되어 더 많은 데이터를 학습했을 수도 있음. 하지만 너무 비싸서 대규모 상용화는 못 했고, RL 적용 버전도 못 보게 된 아쉬움 있음\nGPT-5에서 활용된 RL 기반 트레이닝 첨단 기법도 무한정 확장되진 않는다는 점이 이미 드러남\n사이트에 접속하면 \"연결이 안전하지 않습니다\"라는 오류 메시지를 받음. \"magazine.sebastianraschka.com 웹사이트가 HSTS를 사용 중이라 지금 방문할 수 없습니다\"라고 나옴. 크롬 최신 버전, Ubuntu 환경임\n원문\nmagazine.sebastianraschka.com\nFrom GPT-2 to gpt-oss: Analyzing the Architectural Advances\nAnd How They Stack Up Against Qwen3\n출처 / GeekNews\nGeekNews – 11 Aug 25\nGPT-OSS vs. Qwen3 및 GPT-2 이후 LLM 아키텍처 발전 상세 비교 | GeekNews\nOpenAI가 gpt-oss-20b/120b 모델을 오픈 가중치로 공개함에 따라 2019년 GPT-2 이후 처음으로 OpenAI의 대형 공개 가중치 LLM이 등장함gpt-oss 모델은 GPT-2와 비교해 Dropout, Absolute Position Embedding, GELU 등을 효율적인 현대 기법인 RoPE, SwiGLU, RMSNorm 등으로 대체\n알려드립니다\n이 글은 국내외 IT 소식들을 공유하는 GeekNews의 운영자이신 xguru님께 허락을 받아 GeekNews에 게제된 AI 관련된 소식을 공유한 것입니다.\n출처의 GeekNews 링크를 방문하시면 이 글과 관련한 추가적인 의견들을 보시거나 공유하실 수 있습니다!\n아래쪽에 좋아요를 눌러주시면 새로운 소식을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/gn-gpt-oss-vs-qwen3-gpt-2-llm/7497",
        "date": "2025-08-17T23:39:22.597000",
        "source": "pytorch_kr"
    },
    {
        "title": "all-smi: 현존하는 대부분의 GPU 및 NPU의 하드웨어 모니터링 CLI 도구",
        "content": "all-smi 소개\n최근 AI·머신러닝 워크로드와 고성능 컴퓨팅(HPC) 환경이 확산되면서 GPU와 NPU의 실시간 상태 추적은 필수적인 운영 요소가 되고 있습니다. all-smi는 이러한 요구에 대응하기 위해 플랫폼 간 호환성과 확장성을 극대화하였고, 설치와 사용이 간단하며 다양한 배포 방법(Homebrew, PPA, .deb, Cargo, 바이너리 다운로드 등)을 지원하는 명령줄 기반(CLI)의 도구입니다.\nall-smi는 다양한 하드웨어 플랫폼에서 GPU와 NPU 자원을 실시간으로 모니터링할 수 있는 명령줄 기반 유틸리티입니다. 또한, 터미널 기반 UI를 통해 가속기 사용량, 메모리, 온도, 전력 소비량 등을 직관적으로 시각화하며, 색상 코드와 인터랙티브 정렬 기능을 제공해 대규모 모니터링 환경에서도 효율적으로 활용할 수 있습니다.\n전통적으로 NVIDIA GPU 모니터링에 사용되는 nvidia-smi를 대체할 수 있도록 설계되었으며, NVIDIA GPU뿐 아니라 NVIDIA Jetson, Apple Silicon GPU, Tenstorrent NPU, Rebellions NPU, Furiosa NPU 등 여러 가속기 하드웨어를 지원합니다. 특히 단일 PC뿐 아니라 클러스터 환경에서도 여러 노드를 동시에 모니터링할 수 있으며, 로컬·원격 모드와 Prometheus API 통합 기능을 제공합니다. Prometheus와 연동을 하게되면 Grafana 같은 시각화 대시보드로 확장 가능하며, 내장 모의(Mock) 서버로 개발·테스트 환경 구축도 손쉽게 할 수 있어 개발자와 운영자 모두에게 유용합니다.\nnvidia-smi와의 비교\n기능 nvidia-smi all-smi\n지원 하드웨어 NVIDIA GPU 전용 NVIDIA, Apple Silicon, Jetson, Tenstorrent, Rebellions, Furiosa\n운영체제 지원 Linux, Windows (일부 macOS 제한) Linux, macOS\nGPU 외 자원 모니터링 제한적 (GPU 중심) GPU, NPU, CPU, 메모리, 디스크\n클러스터 원격 모니터링 불가능 가능 (256+ 노드 지원)\nPrometheus API 연동 불가능 가능\nUI 인터랙션 기본 텍스트 출력 (정렬/컬러 없음) 컬러 코드, 컬럼별 정렬, 마우스 클릭 지원\n설치 방식 다양성 드라이버 설치 시 포함 Homebrew, PPA, Debian 패키지, Cargo, 바이너리 다운로드, 소스 빌드\n기존의 nvidia-smi는 NVIDIA GPU에 특화되어 있으며, 다른 하드웨어 플랫폼에서는 사용할 수 없는 한계가 있습니다. 반면, all-smi는 다양한 하드웨어 가속기(GPU·NPU)를 단일 인터페이스에서 관리할 수 있고, NVIDIA 외의 플랫폼에서도 동일한 명령과 UI를 제공합니다.\n또한, all-smi는 CPU·메모리·디스크 모니터링까지 통합 제공하며, 클러스터 단위의 원격 모니터링과 Prometheus API 연동 기능을 내장해 확장성이 뛰어납니다. UI 측면에서도 색상 코드, 탭 전환, 컬럼별 정렬, 마우스 클릭 정렬 등 상호작용성이 강화되어 있습니다.\n즉, nvidia-smi가 NVIDIA 전용 도구라면, all-smi는 \"플랫폼 불문 통합 하드웨어 모니터링 도구\"라고 요약할 수 있습니다.\nall-smi의 주요 기능\nGPU 및 NPU 모니터링: all-smi는 GPU 이름·드라이버 버전·사용량·메모리 상태·온도·클럭 속도·전력 소비 등 세부 지표를 실시간으로 표시합니다. NVIDIA, Apple Silicon, Jetson, Tenstorrent, Rebellions, Furiosa 등 각 플랫폼별 특화 지표도 지원하며, 다중 GPU 환경에서도 개별 장치별 모니터링이 가능합니다.\nCPU 및 메모리 모니터링: CPU는 소켓별 사용량, 클럭 속도, 온도, 전력 소비량을 표시하며, Apple Silicon의 경우 P코어·E코어별 사용량과 주파수까지 세분화 지원합니다. 메모리는 총량·사용량·가용량·스왑 영역까지 시각화하며, Linux에서는 버퍼·캐시 메모리 정보까지 제공합니다.\n프로세스·클러스터 관리: GPU 메모리를 사용하는 프로세스 목록, PID, CPU 사용률, 실행 사용자 등을 보여주며 컬럼별 색상 표시와 정렬 기능을 지원합니다. 클러스터 관리 모드에서는 전체 노드와 GPU 상태를 집계해 평균 사용량, 온도, 전력 통계를 실시간으로 제공합니다.\n원격 모니터링 및 API 연동: 256개 이상의 원격 노드를 동시에 모니터링할 수 있으며, Prometheus API 형식으로 메트릭을 노출해 Grafana 등 외부 모니터링 시스템과 연동할 수 있습니다.\nall-smi 설치 방법\nHomebrew (macOS/Linux)\nbrew tap lablup/tap\nbrew install all-smi\nUbuntu PPA\nsudo add-apt-repository ppa:lablup/backend-ai\nsudo apt update\nsudo apt install all-smi\nDebian 패키지: Releases 페이지에서 .deb 다운로드 후 설치\nCargo 설치: cargo install all-smi\n바이너리 다운로드: GitHub Releases에서 직접 내려받아 $PATH에 추가\n소스 빌드: 개발자 문서 참고\n사용 예시\n# 로컬 모니터링 (기본)\nall-smi\nsudo all-smi local --interval 5\n\n# 원격 노드 모니터링\nall-smi view --hosts http://node1:9090 http://node2:9090\nall-smi view --hostfile hosts.csv\n\n# Prometheus API 서버 실행\nall-smi api --port 9090 --processes\n라이선스\nall-smi 프로젝트는 Apache License 2.0으로 공개 및 배포되고 있습니다. 상업적 이용에 제한이 없습니다.\nall-smi GitHub 저장소\ngithub.com\nGitHub - inureyes/all-smi: Command-line utility for monitoring GPU hardware.\nCommand-line utility for monitoring GPU hardware.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/all-smi-gpu-npu-cli/7496",
        "date": "2025-08-16T18:00:38.441000",
        "source": "pytorch_kr"
    },
    {
        "title": "DINOv3: 자기 지도 학습(SSL)을 활용한 초대규모의 범용 비전 백본(Vision Backbone) 모델(feat. Meta AI)",
        "content": "DINOv3 소개\nDINOv3는 Meta AI가 개발한 차세대 자기지도학습(Self-Supervised Learning, SSL) 기반 비전 모델로, 대규모 이미지 데이터에서 범용적으로 활용 가능한 고성능 시각 백본(vision backbone)을 제공합니다. 기존의 강력한 이미지 인코딩 모델들이 웹 캡션 같은 사람 손으로 작성된 메타데이터에 의존했던 것과 달리, DINOv3는 전혀 라벨이 없는 상태에서 이미지 데이터로부터 의미있는 시각 표현을 스스로 학습하는 자기 지도 학습(Self-Supervised Learning, SSL) 기반의 비전 모델입니다.이를 통해 라벨 수집이 어렵거나 불가능한 분야에서도 대규모 학습이 가능해졌습니다.\n특히, DINOv3는 다양한 이미지 도메인에 걸쳐 범용적으로 사용 가능한 백본을 제공함으로써, 웹 이미지뿐만 아니라 위성, 의료, 산업용 이미지를 포함한 다양한 도메인에서 활용할 수 있으며, 이미지 분류, 객체 탐지, 시맨틱 분할, 비디오 객체 추적 등 폭넓은 비전 작업에서 최첨단 성능을 기록합니다. 17억 장의 이미지와 최대 70억 개의 파라미터로 학습하여 단일 백본으로도 밀집 예측(dense prediction) 작업에서 전문 모델을 초월하는 결과를 보여줍니다.\nDINO 계열 모델과 자기 지도 학습(SSL)의 흐름\nDINO는 2021년 공개된 첫 버전부터 Vision Transformer(ViT)를 기반으로, 학습자(학생)-참조자(교사)의 지식 증류(distillation) 메커니즘을 활용해 자기 지도 학습을 구현해왔습니다. DINOv2에서는 이미지 표현 학습을 보다 정제된 이미지 셋에서 수행하면서 범용성과 정확도를 개선했고, DINOv3는 이를 계승하여 더욱 큰 스케일과 강력한 백본으로 성능을 극대화한 버전입니다.\nDINOv2도 자기지도학습 기반으로 높은 성능을 보여줬지만, DINOv3는 이를 한층 발전시켜 더 큰 모델 규모(7B 파라미터)와 12배 더 큰 학습 데이터셋을 사용했습니다. 성능 면에서도, 특히 시맨틱 분할, 객체 탐지, 깊이 추정 같은 밀집 예측 작업에서 DINOv2 대비 월등히 향상되었습니다.\n또한 DINOv3는 CLIP 기반 모델(SigLIP 2, Perception Encoder 등)과 비교 시 다양한 이미지 분류 벤치마크에서 동등하거나 그 이상을 기록하며, 밀집 예측에서는 압도적인 우위를 보입니다. DINOv3는 라벨 없는 상태에서 학습했음에도, 라벨이 있는 약지도(weakly supervised) 모델보다 성능이 더 높다는 점이 특징입니다.\nDINO 계열 모델들의 핵심은, 텍스트나 외부 라벨 없이 이미지 자체만으로도 강력한 표현을 학습할 수 있다는 점입니다. CLIP과 같은 텍스트-이미지 쌍 기반 모델과는 달리, DINO는 오직 이미지 간 유사도와 구조만으로 시각적 의미를 파악하고 학습합니다. 이를 위해서는 모델 아키텍처와 학습 기법 모두에서 섬세한 설계가 요구됩니다.\nDINOv3 아키텍처 구성\nDINOv3는 Vision Transformer(ViT)와 ConvNeXt 두 가지 백본 구조를 지원하며, 그 중에서도 ViT 기반이 주류입니다. 특히 ViT-g/14 모델은 약 70억 개의 파라미터를 가지며, 이는 ViT-H/14보다도 크고 CLIP ViT 모델을 능가하는 스케일입니다.\n교사-학생 구조\n학생(Student) 네트워크는 인코딩하고자 하는 입력 이미지를 다양한 증강 방식으로 변형된 뷰(views)로 처리합니다.\n교사(Teacher) 네트워크는 EMA(Exponential Moving Average)를 통해 학생 네트워크의 가중치로부터 업데이트되며, 직접 학습되지 않습니다.\nLoss는 서로 다른 뷰들 간의 예측 일관성을 최대화하도록 설계됩니다. 이는 서로 다른 증강 이미지를 같은 것으로 인식하도록 모델이 스스로 학습하게 합니다.\n입력 증강 및 멀티크롭 전략\n입력 이미지는 다양한 해상도와 위치로 잘라진 여러 crop(예: 224×224, 96×96)으로 생성됩니다.\n교사는 대형 crop만 사용하고, 학생은 대형 및 소형 crop을 함께 사용하여 더욱 강건한 표현을 유도합니다.\n이러한 멀티크롭(multicropping)은 모델이 다양한 공간 및 시각 스케일에서 일관된 표현을 학습하도록 도와줍니다.\nMLP Head\n각 백본 출력 후에는 MLP 헤드가 붙어, 최종적으로 표현(feature embedding)을 생성합니다.\n이 표현은 downstream task (분류, 분할 등)에 그대로 활용될 수 있습니다.\nMLP 헤드는 교사-학생 간 출력을 정렬시키는 핵심 위치입니다.\n학습 방식과 최적화 전략\n대규모 비지도 학습\nDINOv3는 17억 개의 웹 이미지로 학습되었습니다. 이 이미지들은 라벨이 없으며, CLIP의 사전학습 이미지보다 4배 많고 품질도 뛰어납니다. 또한 단일 GPU로는 학습이 불가능하며, 대규모 클러스터에서 수 주간 학습됩니다.\n성능을 끌어올린 최적화 기법\nSharpness-Aware Minimization(SAM) 기법이 적용되어, 일반적인 SGD나 Adam보다 일반화 성능이 뛰어납니다.\nEMA decay 파라미터 조정으로 학생-교사 간 동기화를 세밀히 조절합니다.\nCosine similarity loss 및 centering 기법을 통해 출력의 다양성과 안정성을 동시에 확보합니다.\nDINOv3의 주요 특징\n자기지도학습(SSL)과 대규모 학습: DINOv3의 핵심은 대규모 라벨 없는 데이터로부터 범용 시각 표현을 학습하는 능력입니다. 이를 통해 위성 이미지, 의료 영상, 산업용 데이터 등 라벨링이 어려운 데이터셋에서도 고성능 모델을 만들 수 있습니다. 예를 들어, 위성 이미지 기반 숲 캐노피 높이 추정에서 DINOv2 대비 오차를 4.1m → 1.2m로 줄였습니다.\n파인튜닝 없이도 최첨단 성능: DINOv3는 백본 가중치를 고정(frozen)한 상태에서도 객체 탐지, 시맨틱 분할, 깊이 추정 같은 핵심 비전 작업에서 최첨단 성능을 기록합니다. 이는 다양한 애플리케이션에서 동일한 백본을 공유할 수 있어, 엣지 디바이스나 멀티태스크 환경에서 연산 효율성을 극대화합니다.\n다양한 모델 크기 제공: Meta는 7B 모델뿐만 아니라, 경량화된 ViT-B, ViT-L, ConvNeXt(T/S/B/L) 구조로도 DINOv3를 distillation하여 공개했습니다. 이를 통해 모바일·임베디드 환경 등 자원 제약이 있는 환경에서도 활용할 수 있습니다.\n실제 활용 사례\n세계자원연구소(WRI): 위성 이미지로 산림 훼손과 토지 이용 변화를 감지, 탄소 복원 프로젝트의 검증 및 자금 지원을 효율화합니다. (더 자세히 보기)\nNASA JPL: DINO 백본을 활용한 화성 탐사 로봇의 멀티태스크 비전 인식에 활용합니다. (더 자세히 보기)\nOrakl Oncology: DINO를 장기유사체 이미지로 사전 훈련시켜, 암 치료에 대한 환자 반응 예측을 지원하는 기반 모델을 생성합니다. (더 자세히 보기)\n라이선스\nDINOv3 모델은 Commercial License로 공개 및 배포되고 있습니다. 상업적 사용이 가능하지만, 세부 조건은 라이선스 문서를 반드시 확인해야 합니다.\nDINOv3 공식 홈페이지\nAI at Meta\nDINOv3\nDINOv3 scales self-supervised learning (SSL) for images to produce our strongest universal vision backbones, enabling breakthrough performance across diverse domains.\nMeta AI의 DINOv3 공개 블로그\nMeta AI\nDINOv3: Self-supervised learning for vision at unprecedented scale\nDINOv3 scales self-supervised learning for images to create universal vision backbones that achieve absolute state-of-the-art performance across diverse domains, including web and satellite imagery.\nDINOv3 논문\narXiv.org\nDINOv3\nSelf-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training...\nDINOv3 GitHub 저장소\ngithub.com\nGitHub - facebookresearch/dinov3: Reference PyTorch implementation and models for...\nReference PyTorch implementation and models for DINOv3\nDINOv3 모델 다운로드\nhuggingface.co\nDINOv3 - a facebook Collection\nDINOv3: foundation models producing excellent dense features, outperforming SotA w/o fine-tuning - https://arxiv.org/abs/2508.10104\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/dinov3-ssl-vision-backbone-feat-meta-ai/7495",
        "date": "2025-08-16T11:50:36.168000",
        "source": "pytorch_kr"
    },
    {
        "title": "ECA(Editor Code Assistant): 다양한 코드 편집기를 지원하는 오픈소스 AI Pair Programming 도구",
        "content": "ECA 소개\nECA(Editor Code Assistant)는 다양한 코드 편집기와 대형 언어 모델(LLM)을 연결해주는 오픈소스 툴로, AI 기반 페어 프로그래밍을 보다 간편하게 경험할 수 있도록 설계되었습니다. 이 프로젝트의 가장 큰 특징은 에디터에 종속되지 않는 통합 프로토콜을 제공한다는 점입니다. 이를 통해 Emacs, VSCode, Vim 등 다양한 편집기에서 동일한 환경과 기능을 구현할 수 있습니다. ECA는 LSP(Language Server Protocol)에서 영감을 받아 설계되었으며, 서버-클라이언트 구조를 기반으로 합니다.\nLLM 기술 발전이 가속화되면서 모델 간 성능 격차는 점차 줄어드는 반면, 개발자가 편리하게 코드를 작성하고 변경 사항을 계획할 수 있는 **사용자 경험(UX)**의 중요성은 더욱 커지고 있습니다. ECA는 바로 이 UX 향상을 위해, 편집기 개발자가 모델 연동과 같은 복잡한 구현 대신 UI/UX에 집중할 수 있는 환경을 제공합니다.\n또한, 단일 설정 파일을 통해 전역 혹은 로컬에서 동일한 환경을 구성할 수 있으며, OpenAI, Anthropic, Ollama와 같은 다양한 모델을 동시에 사용할 수 있습니다. 이를 통해 사용자는 프로젝트 요구사항과 개인 선호에 맞춰 유연하게 모델을 선택하고 조합할 수 있습니다.\nECA는 GitHub Copilot이나 Cursor 등 특정 에디터에 종속된 AI 코드 도우미와 달리, **편집기 비종속성(editor-agnostic)**을 핵심 가치로 삼습니다. 예를 들어, GitHub Copilot은 VSCode, JetBrains IDE 중심으로 동작하지만, ECA는 Emacs, VSCode, Vim뿐 아니라 앞으로 Intellij 등 다른 에디터로도 확장될 계획입니다. 또한, LSP처럼 표준화된 통신 프로토콜을 적용하여 신규 에디터 통합 시 최소한의 개발로 빠르게 연동할 수 있습니다.\n또한 ECA는 단순히 코드 자동완성에 그치지 않고, 채팅 기반 상호작용, 도구 호출 관리, 다중 모델 지원, 맥락 정보 제공 등 LLM 활용의 확장성을 극대화할 수 있는 기능을 제공합니다. 이 점에서 단일 모델 기반 도구보다 훨씬 유연하고 확장성 있는 아키텍처를 갖추고 있습니다.\nECA의 주요 기능\n편집기에 종속되지 않는 프로토콜(Editor-agnostic Protocol): ECA는 모든 편집기에서 동일한 UX를 제공하기 위해 자체 정의한 통신 프로토콜을 사용합니다. 이는 LSP와 유사하게 표준화된 방식으로 서버와 편집기가 stdin/stdout을 통해 데이터를 교환하도록 설계되었습니다. 덕분에 새로운 에디터가 추가되더라도 복잡한 수정 없이 쉽게 통합할 수 있습니다.\n단일 설정 관리(Single configuration): 사용자는 .eca/config.json 파일에 API 키와 모델 정보를 입력하여 환경을 설정할 수 있습니다. 이 파일은 프로젝트 루트나 전역 설정 경로에 위치할 수 있으며, OpenAI, Anthropic, Ollama, 그리고 사용자 정의 모델까지 모두 지원합니다. 설정은 다음과 같은 방식으로 구성됩니다:\n{\n  \"openaiApiKey\": \"your-openai-api-key-here\",\n  \"anthropicApiKey\": \"your-anthropic-api-key-here\"\n}\n채팅 기반 상호작용: ECA는 코드 작성 중 질문, 코드 리뷰, 리팩토링 제안 등을 채팅 형태로 지원합니다. 사용자는 마치 동료 개발자와 협업하듯 LLM과 대화하며 개발을 진행할 수 있습니다.\n다중 모델 및 컨텍스트 지원: ECA는 하나의 세션에서 여러 LLM 모델을 사용할 수 있으며, 프로젝트 코드나 MCP(Model Context Protocol) 리소스, 사용자 정의 프롬프트 등을 포함해 풍부한 컨텍스트를 LLM에 전달할 수 있습니다. 이를 통해 답변 품질과 코드 추천의 정확도를 높일 수 있습니다.\nECA 설치 및 시작\n편집기용 플러그인을 설치하면 ECA 서버가 자동으로 다운로드 및 실행됩니다.\nEmacs 플러그인\nVSCode 확장\nVim 플러그인\nIntellij 지원 예정\n.eca/config.json에 모델 설정을 추가합니다.\n편집기 내 채팅 인터페이스를 통해 바로 AI 코딩 지원을 시작할 수 있습니다.\n라이선스\nECA 프로젝트는 Apache License 2.0으로 공개 및 배포되고 있습니다. 상업적 사용에 제한이 없습니다.\nECA 프로젝트 공식 홈페이지\neca.dev\nOverview - ECA - Editor Code Assistant\nECA - AI pair programming capabilities in any editor\nECA 프로젝트 GitHub 저장소\ngithub.com\nGitHub - editor-code-assistant/eca: Editor Code Assistant (ECA) - AI pair programming...\nEditor Code Assistant (ECA) - AI pair programming capabilities agnostic of editor\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/eca-editor-code-assistant-ai-pair-programming/7491",
        "date": "2025-08-15T18:30:45.632000",
        "source": "pytorch_kr"
    },
    {
        "title": "Anthropic의 Claude 4 Sonnet 모델이 1백만 토큰(1M tokens)의 컨텍스트 지원",
        "content": "Claude Sonnet 4의 1M Context Window 소개\nAnthropic이 자사의 언어 모델 Claude Sonnet 4에 1백만 토큰 규모의 콘텍스트 윈도우를 도입했습니다. 이는 이전 대비 5배 이상 향상된 수치로, 이제 개발자나 연구자들은 수십 개의 논문, 대규모 코드베이스 전체, 방대한 문서 집합을 한 번의 요청으로 처리할 수 있습니다. 이러한 기능 확장은 자연어 처리(NLP) 및 AI 모델 활용에 있어 매우 실질적인 전환점을 의미합니다.\n기존의 대부분 언어 모델은 수만 개 수준의 토큰 콘텍스트만 지원했기 때문에, 대규모 데이터를 다루는 데 있어 제한적이었습니다. 특히 긴 코드를 다루거나 문서 간의 상호 연관성을 파악해야 할 때, 콘텍스트 제한으로 인한 정보 손실이나 문맥 부재 문제가 자주 발생했습니다. 하지만 1백만 토큰 지원은 이러한 한계를 근본적으로 해소하며, AI 모델이 전반적인 시스템 구조를 보다 깊이 이해하도록 돕습니다.\n이번 발표는 Anthropic API 및 Amazon Bedrock을 통해 퍼블릭 베타로 제공되고 있으며, 곧 Google Cloud의 Vertex AI에서도 사용할 수 있도록 확장될 예정입니다. 모델의 콘텍스트 한계가 늘어나면서, 보다 현실적인 엔지니어링 워크플로우가 가능해졌고, 이를 활용한 다양한 사례도 함께 소개되었습니다.\n확장된 Context Window로는 무엇을 할 수 있나요?\n대규모 코드 분석: 1백만 토큰 지원은 전체 소스코드, 테스트 파일, 문서 등을 한 번에 로딩하고 분석하는 데 유리합니다. Claude는 코드 간의 의존 관계를 파악하고 프로젝트 전체의 아키텍처를 이해한 상태에서 개선 방향을 제안할 수 있습니다. 예전에는 파일별로 분할하여 처리하고, 모델에게 반복적으로 맥락을 제공해야 했다면 이제는 이 과정을 대폭 줄일 수 있습니다.\n문서 통합 및 요약: 수백 개에 달하는 법률 문서, 연구 논문, 기술 명세서도 한 번의 요청으로 통합 분석이 가능합니다. Claude는 이들 문서 간의 연관성과 흐름을 유지한 채, 보다 일관된 해석과 분석을 제공할 수 있습니다. 복잡한 문서 간의 연관성 분석이 필요한 분야에서 매우 유용하게 사용될 수 있습니다.\n콘텍스트를 유지하는 에이전트 구축: 에이전트 기반 시스템에서는 수많은 도구(tool) 호출과 멀티스텝 워크플로우가 필요한데, 이때 이전 대화 내역과 API 문서, 도구 정의 등을 모두 기억하는 것이 중요합니다. 1백만 토큰 콘텍스트는 이 모든 정보를 한 번에 포함하여, 더욱 일관되고 지능적인 대화를 유지할 수 있게 해줍니다.\n가격 정책 및 최적화 팁\n1백만 토큰 처리에는 많은 계산 자원이 필요하기 때문에, 200K 토큰을 초과한 요청부터는 가격이 상승합니다. 예를 들어, 200K 이하 요청은 입력 기준 1백만 토큰(1MTok)당 $3, 출력을 기준으로는 $15이며, 200K 초과 시에는 각각 $6, $22.5로 책정됩니다.\n하지만 프롬프트 캐싱이나 배치 처리를 활용하면 비용과 지연시간을 절감할 수 있습니다. 특히 배치 처리 시 최대 50%까지 절감 효과를 얻을 수 있어, 대규모 작업에서 효율적인 전략이 될 수 있습니다.\n고객 사례 소개\nBolt.new: 웹 기반 개발 플랫폼 Bolt.new는 Claude Sonnet 4를 코드 생성 워크플로우에 적극 활용하고 있습니다. CEO인 Eric Simons는 “1M 콘텍스트 윈도우 덕분에 훨씬 더 큰 프로젝트를 정확하게 처리할 수 있게 되었다”며 실사용 성과를 강조합니다. Claude는 생산 환경에서 여타 모델보다 높은 정확도를 보이고 있다고 합니다.\niGent AI: 런던의 iGent AI는 ‘Maestro’라는 AI 코딩 파트너를 통해 실제 대규모 코드베이스를 며칠간 지속적으로 다루는 기능을 구현했습니다. 이들은 Claude Sonnet 4가 “자율적인 소프트웨어 엔지니어링의 새로운 패러다임”을 가능하게 한다고 평가하고 있습니다.\nClaude Sonnet 4 1M 콘텍스트 발표 블로그\nanthropic.com\nClaude Sonnet 4 now supports 1M tokens of context\nClaude Sonnet 4 now supports up to 1 million tokens of context on the Anthropic API—a 5x increase.\n\n\n이 글은 GPT 모델로 정리한 글을 바탕으로 한 것으로, 원문의 내용 또는 의도와 다르게 정리된 내용이 있을 수 있습니다. 관심있는 내용이시라면 원문도 함께 참고해주세요! 읽으시면서 어색하거나 잘못된 내용을 발견하시면 덧글로 알려주시기를 부탁드립니다.\n파이토치 한국 사용자 모임이 정리한 이 글이 유용하셨나요? 회원으로 가입하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 Daily로 변경도 가능합니다.)\n아래쪽에 좋아요를 눌러주시면 새로운 소식들을 정리하고 공유하는데 힘이 됩니다~",
        "author": "9bow",
        "url": "https://discuss.pytorch.kr/t/anthropic-claude-4-sonnet-1-1m-tokens/7489",
        "date": "2025-08-15T15:55:59.860000",
        "source": "pytorch_kr"
    },
    {
        "title": "GPT Fallback 검색 결과: AI research trends 2024",
        "content": "1. 현재 주요 AI 연구 분야\n2024년 현재, AI 연구의 주요 분야는 다음과 같습니다.\n\n- 자연어 처리(NLP): GPT-3와 같은 대형 언어 모델의 등장 이후, 더욱 세밀하고 효율적인 언어 이해 및 생성 모델에 대한 연구가 활발하게 진행되고 있습니다.\n- 강화학습: AlphaGo와 같은 성공 사례 이후, 실제 세계 문제에 강화학습을 적용하는 연구가 확산되고 있습니다. 특히, 자율주행, 에너지 최적화, 로봇공학 등 다양한 분야에서의 활용 가능성을 탐구하고 있습니다.\n- 페더레이티드 러닝: 개인정보 보호와 데이터 보안에 대한 우려로, 중앙 집중식 학습에서 벗어나 각기 다른 기기에서 모델을 학습하고, 그 결과를 집계하는 페더레이티드 러닝에 대한 연구가 활발히 진행되고 있습니다.\n- 인공지능 윤리: AI의 판단에 대한 투명성과 공정성, 그리고 AI가 인간 사회에 미치는 영향에 대한 연구가 확산되고 있습니다.\n\n2. 최근 발표된 중요한 논문이나 기술\n- \"Language Models are Few-Shot Learners\": OpenAI가 발표한 이 논문은 GPT-3 모델을 통해 소량의 예시만으로도 다양한 언어 작업을 수행할 수 있음을 보여줍니다.\n- \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\": DeepMind의 논문으로, MuZero라는 모델을 통해 환경에 대한 사전 지식 없이도 강화학습을 통해 복잡한 게임을 마스터할 수 있음을 보여줍니다.\n\n3. 산업계 동향\n- 기업들은 AI를 활용한 제품과 서비스를 개발하고 있습니다. 특히, 고객 서비스, 제품 추천, 공급망 최적화 등 다양한 분야에서 AI를 활용하고 있습니다.\n- 기업들은 또한 AI 윤리에 대한 고려를 강화하고 있습니다. AI의 판단 과정에 대한 투명성을 높이고, 편향을 줄이는 방법에 대한 연구를 진행하고 있습니다.\n\n4. 향후 전망\n- AI는 계속해서 발전할 것으로 예상되며, 이는 더욱 정교하고 효율적인 모델의 등장을 의미합니다. 특히, 강화학습과 페더레이티드 러닝 등의 분야에서 중요한 발전이 예상됩니다.\n- AI 윤리에 대한 고려는 더욱 중요해질 것으로 예상됩니다. AI의 판단 과정을 이해하고, 그 결과를 공정하게 만드는 방법에 대한 연구가 확산될 것입니다.\n- AI는 산업계에서 더욱 활용될 것으로 예상됩니다. 특히, 공급망 최적화, 에너지 관리, 자율",
        "author": "OpenAI GPT-4",
        "url": "https://openai.com",
        "date": "2025-08-22T05:34:54.220998",
        "source": "gpt_fallback"
    }
]